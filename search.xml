<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Python类和对象</title>
    <url>/2019/11/01/Python/4.python%E7%B1%BB%E5%92%8C%E5%AF%B9%E8%B1%A1/</url>
    <content><![CDATA[<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1.引言"></a>1.引言</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;本部分主要介绍一些Python的面向对象编程。在Python中一切的数据类型都是面向对象的。主要分三块内容进行讲述：</p>
<ul>
<li>类成员和定义</li>
<li>面向对象的三个基本特征</li>
<li>Python中的基本类</li>
</ul>
<a id="more"></a>
<hr>
<h2 id="2-类成员与定义"><a href="#2-类成员与定义" class="headerlink" title="2.类成员与定义"></a>2.类成员与定义</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;类主要是封装了一类对象的数据和操作。一个类，它具有以下属性：</p>
<ul>
<li>成员变量（attribute)<ul>
<li>实例变量</li>
<li>类变量</li>
<li>私有变量</li>
</ul>
</li>
<li>属性(property)</li>
<li>成员方法<ul>
<li>实例方法</li>
<li>类方法</li>
<li>静态方法</li>
<li>私有方法</li>
</ul>
</li>
</ul>
<h3 id="2-1-类的定义与创建对象"><a href="#2-1-类的定义与创建对象" class="headerlink" title="2.1 类的定义与创建对象"></a>2.1 类的定义与创建对象</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;类的首先包括类名和类的内容，使用关键字class,语法格式:</p>
<pre><code>class 类名(父类)：
    类的内容

class Person(object):
    #类的内容部分
    pass

A = Person()
print(A)   #&lt;__main__.Person object at 0x0000021D5842A470&gt;
B = Person()
print(B)   #&lt;__main__.Person object at 0x00000233D15BA4E0&gt;
</code></pre><p>上部分定义了一个Person类，继承自Object类，这个是Python的基本类，后面做详细的介绍。类的内容用了pass占位。那么在后续利用这个类定义了一个对象A和对象B，A和B对象的输出是在main这个module中的Person对象，因为是不同的对象，所以内存地址不同。而为什么通过print对象会输出这个字符串呢。因为在Object类中定义了<strong>str()</strong>函数，这个函数会返回有关这个对象的描述信息。当然我们可以重写<strong>str()</strong>函数来输出我们想要的东西。<strong>一般这种__双划线结尾的是Python原来保留着的特殊含义的方法</strong></p>
<h3 id="2-2-变量"><a href="#2-2-变量" class="headerlink" title="2.2 变量"></a>2.2 变量</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;Python中的实例变量就是某个对象特有的变量，比如一个人的名字，身高，体重等都是每个人特有的。而类变量是所有对象共有的，比如鼻孔个数，耳朵个数等，这跟具体的对象没有关系。在其他的语言中可能还有私有变量，但是由于Python的动态语言特性，因此它没有私有变量这个概念，但是可以通过变量名的应用来写私有变量。<br>接下来通过一个例子来看看变量的定义以及调用问题。<br>输入：</p>
<pre><code>class Person(object):
    ear_num = 2
    def __init__(self, name, age, weight):
        self.name = name
        self.age = age
        self.__weight = weight

A = Person(&quot;javierhuang&quot;, 24, 120)
print(Person.ear_num)
print(A.name)
print(A.weight)

A.sex = 1
print(A.ear_num)
print(A.__dict__)
</code></pre><p>输出：</p>
<pre><code>2
javierhuang
AttributeError: &#39;Person&#39; object has no attribute &#39;weight&#39;
{&#39;name&#39;: &#39;javierhuang&#39;, &#39;age&#39;: 24, &#39;_Person__weight&#39;: 120, &#39;sex&#39;: 1}
</code></pre><p>&nbsp;&nbsp;&nbsp;&nbsp;<strong>类变量</strong>：通过上述代码可以看出，定义在Person这个类内的ear_num就是一个类变量，它是所有Person共有的，采用类名.类变量方式调用。<br>&nbsp;&nbsp;&nbsp;&nbsp;<strong>实例变量</strong>：通过init构造函数构造带有self.的参数都是实例变量了，因为每创建一个实例就会有对应的参数改变。实例变量的调用采用实例.实例变量调用。但是尽量不要对实例变量在外面进行赋值的时候，比如上图创建了一个sex,原本是这个类中没有的，但是当我们在外面创建了之后，那么通过A的变量字典我们看到sex已经在了，只是内部的方法不能应用这个变量而已。<br>&nbsp;&nbsp;&nbsp;&nbsp;<strong>私有变量</strong>：这里还有一个特殊的变量，self.<strong>weight,这是一个私有变量，但是Python原本是没有私有变量这个说法的，事实上通过这样写之后只是在名字上做了限定，实际上如果真要访问，如上图A的dict中显示，变量名是’A._Person</strong>weight’就可以访问了。建议一些私有变量，还是采用私有变量这种写法。</p>
<h3 id="2-3-方法"><a href="#2-3-方法" class="headerlink" title="2.3 方法"></a>2.3 方法</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;一个对象除了对象的变量之外，那么剩下最重要的就是方法了，方法主要分为构造方法，实例方法，类方法和静态方法，他们各有其用处以及定义，除了了解上述方法的建立之外，我们更需要了解下何时才采用这些方法。首先通过一个例子来了解这些方法。</p>
<pre><code>class Person(object):
    ear_num = 2
    def __init__(self, name, age, weight):
        self.name = name
        self.age = age
        self.__weight = weight

    def run(self,distance):
        self.__weight-= 1 if distance&gt;10 else 0
        print(&quot;running&quot;)

    @classmethod
    def add_eye(cls,num):
        cls.eye_num = num
        print(&quot;add eye&quot;)

    @staticmethod
    def add_mouse():
        print(&quot;add_nouse&quot;) 

    def __sleep(self):
        print(&quot;sleeping&quot;)

A = Person(&quot;javierhuang&quot;, 24, 120)
A.run(100)
Person.add_eye(2)
Person.add_mouse()
print(Person.__dict__)
A.sleep()
</code></pre><p>输出：</p>
<pre><code>running
add eye
add_nouse
{&#39;__module__&#39;: &#39;__main__&#39;, &#39;ear_num&#39;: 2, &#39;__init__&#39;: &lt;function Person.__init__ at 0x000001B57F721F28&gt;, &#39;run&#39;: &lt;function Person.run at 0x000001B57F722048&gt;, &#39;add_eye&#39;: &lt;classmethod object at 0x000001B57F72D7B8&gt;, &#39;add_mouse&#39;: &lt;staticmethod object at 0x000001B57F72D7F0&gt;, &#39;_Person__sleep&#39;: &lt;function Person.__sleep at 0x000001B57F7221E0&gt;, &#39;__dict__&#39;: &lt;attribute &#39;__dict__&#39; of &#39;Person&#39; objects&gt;, &#39;__weakref__&#39;: &lt;attribute &#39;__weakref__&#39; of &#39;Person&#39; objects&gt;, &#39;__doc__&#39;: None, &#39;eye_num&#39;: 2}
AttributeError: &#39;Person&#39; object has no attribute &#39;sleep&#39;
</code></pre><p>&nbsp;&nbsp;&nbsp;&nbsp;<strong>构造方法</strong>：在类的定义的时候，一般都铁定出现构造方法<strong>init</strong>(),这个是用来构建创建和初始化实例变量的，第一个参数是self,后续其他是实例变量需要的参数，调用的时候不需要用self。<br>&nbsp;&nbsp;&nbsp;&nbsp;<strong>实例方法</strong>：实例方法的定义如run()函数，首先第一个是self,后续是实例方法的参数，实例方法不需要传入self。实例方法和实例变量相同，都是某个实例特有的方法，因此调用的时候是实例对象.实例方法<br>&nbsp;&nbsp;&nbsp;&nbsp;<strong>类方法</strong>：类方法定义如add_eye()的定义，第一个参数不是self,而是cls,是一个type型的实例，而后续是传入的参数。同时，要定义类方法要使用装饰器@classmethod。具体装饰器的理解可参考之前的文章<a href="https://jaryhuang.github.io/2019/10/30/Python/3.python%E5%87%BD%E6%95%B0/#more" target="_blank" rel="noopener">Python函数与装饰器</a>。类方法的调用是类名.类方法。<br>&nbsp;&nbsp;&nbsp;&nbsp;<strong>静态方法</strong>：如果想要定义的方法既不想和实例绑定也不想和类绑定，那么可以定义静态方法。定义静态方法的例子如上述代码所示，第一使用@staticmethod装饰器。第二其不用声明self或者cls,直接用。<br>&nbsp;&nbsp;&nbsp;&nbsp;那么通过上述的介绍，我们知道一个类定义一般构造函数是都是哟的，然后实例方法也是常常包含在内，供一些实例的调用。那么这三种方法有什么区别呢？<br><strong><em>实例方法</em></strong> 由对象调用，一个self参数指向当前的实例对象，所以只要该方法有self参数,在调用此方法的时候会先把这个对象的内存空间加载进来,以便在方法中使用该实例对象的属性或者方法。<strong><em>类方法</em></strong> 是由类调用，一个cls参数，装饰器@classmethod修饰cls指向当前的类对象,当调用一个含有@classmethod装饰的方法,则会先加载这个类的内存空间。<br><strong><em>静态方法</em></strong> 由类调用，不需要参数，不要使用实例对象的属性方法也不需要使用类对象的属性，所以静态方法的调用不会加载类或者对象到内存空爱你,节省资源。</p>
<h2 id="3-面向对象的三个基本特性"><a href="#3-面向对象的三个基本特性" class="headerlink" title="3. 面向对象的三个基本特性"></a>3. 面向对象的三个基本特性</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;面向对象思想中有三个基本特性，它们分别是：封装性，继承性和多态性。</p>
<h3 id="3-1-封装性"><a href="#3-1-封装性" class="headerlink" title="3.1 封装性"></a>3.1 封装性</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;由于Python语言的动态特性，因此没有关于封装性的关键字，主要通过特定的命名方式对对象和方法进行了封装。在前面讲私有变量和私有方法中已经提到了如何创建和调用他们。这里特别的提一下属性的定义，这也是为封装性而作的一个功能。面向对象的特性理论上应该是没有公有的属性，应该所有都设计成私有的，然后通过公有的属性方法来进行调用。<br>例子：</p>
<pre><code>class Person(object):
    def __init__(self, name, age, weight):
        self.name = name
        self.age = age
        self.__weight = weight

    @property
    def weight(self):
        return self.__weight

    @weight.setter
    def weight(self,weight):
        self.__weight = weight

A = Person(&quot;javierhuang&quot;, 24, 120)
print(A.weight)
A.weight = 118
print(A.weight)
</code></pre><p>输出：</p>
<pre><code>120
118
</code></pre><p>&nbsp;&nbsp;&nbsp;&nbsp;上述中通过@property装饰器相当于定义了一个getter的访问器，直接可以通过属性名来取值了。而通过@weight.setter直接定义了setter访问器，这样就能直接设定值了。如上述代码所示通过A.weight之后能直接访问，就相当于把私有变量公有化了。</p>
<h3 id="3-2-继承性"><a href="#3-2-继承性" class="headerlink" title="3.2 继承性"></a>3.2 继承性</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;类的一个很重要的特性就是继承性，比如你有一个类为Person,然后未来你要建造一个类是Student类，那么这两个类的Student类肯定有一部分是和Person类相同的，为了避免重复写所有的东西，因此可以采用继承的方式。<br>测试：</p>
<pre><code>class Person(object):
    def __init__(self, name, age, weight):
        self.name = name
        self.age = age
        self.__weight = weight

    def run(self,distance):
        self.__weight-= 1 if distance&gt;10 else 0
        print(&quot;running&quot;)

class Student(Person):
    def __init__(self, name, age, weight, school):
        super().__init__(name, age, weight)
        self.school = school

    def run(self):
        print(&quot;I&#39;m running&quot;)
</code></pre><p>&nbsp;&nbsp;&nbsp;&nbsp;<strong>集成的构造</strong>上述Student类通过继承了Person类，采用super().<strong>init</strong>()的方式调用了父类的构造方法，super()函数返回父类的引用，通过它可以调用父类中的实例变量和方法。<br>&nbsp;&nbsp;&nbsp;&nbsp;<strong>重写方法</strong>除了继承方法，新的类还可以重写方法，上述Student类的run和父类的方法同名，因此相当于对方法进行了重写，Student调用run时调用的是自己的run。<br>&nbsp;&nbsp;&nbsp;&nbsp;<strong>多继承</strong>多继承的含义就是一个子类有多个父类，大多数语言不支持多集成，主要是由于多继承发生的方法冲突。例如公共汽车是汽车也是交通工具，它有两个父类分别是汽车和交通工具，如果两个父类都定义了stop功能，那么该继承哪个方法呢。在Python中是支持多继承，它解决一个冲突的思路是：子类调用方法，首先从子类中找，若子类中没有，按照子类声明的父类顺序从左到右从父类中找，如果没找到再找父类的父类。</p>
<h3 id="3-3-多态性"><a href="#3-3-多态性" class="headerlink" title="3.3 多态性"></a>3.3 多态性</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;Python发生多态性的前提是1.继承，多态一定发生在父类和子类之间。2.重写，子类重写了父类的方法。</p>
<pre><code>class Figure:
    def draw(self):
        print(&quot;draw Figure&quot;)

class Ellipse(Figure):
    def draw(self):
        print(&#39;draw Ellipse&#39;)

class Triangle(Figure):
    def draw(self):
        print(&quot;draw Triangle&quot;)

f1 = Figure()
f1.draw()

f2 = Ellipse()
f2.draw()

f3 = Triangle()
f3.draw()

print(isinstance(f1,Figure))
print(isinstance(f2,Ellipse))
print(isinstance(f3,Ellipse))

print(issubclass(Triangle,Figure))
print(issubclass(Ellipse,Triangle))
</code></pre><p>输出：</p>
<pre><code>draw Figure
draw Ellipse
draw Triangle
True
True
False
True
False
</code></pre><p>上述代码中可以看到f2和f3符合多态的要求，他会去指定调用实例的方法。通过isinstance函数我们可以看到，f2因为继承了Figure,所以它也是Figure类型的。而f3和f2是一个平行关系，所以两者不相同。通过issubclass这个子类也可以看得出来其特点。</p>
<h2 id="4-Python的基本类"><a href="#4-Python的基本类" class="headerlink" title="4. Python的基本类"></a>4. Python的基本类</h2><h3 id="4-1-根类——object"><a href="#4-1-根类——object" class="headerlink" title="4.1 根类——object"></a>4.1 根类——object</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;Python的所有类都是直接或者间接的继承object类，是所有类的“祖先”，Object类有很多方法，可以通过print(object.__dict__)来进行查看，这里重点介绍介绍一下<br>__str<strong>():返回该对象的字符串<br>\</strong>eq__(other):和other对象是否相等</p>
<pre><code>class Person(object):
    def __init__(self, name, age, weight):
        self.name = name
        self.age = age
        self.__weight = weight

    def __str__(self):
        return &quot;this is Person class&quot;

    def __eq__(self,other):
        if self.name == other.name and self.age == other.age:
            return True
        else:
            return False

A = Person(&quot;javierhuang&quot;, 24, 120)
print(A)

B = Person(&quot;javierhuang&quot;, 24, 120)
print(A == B)

#没有重写__str__()时候返回的是：&lt;__main__.Person object at 0x00000229E2FBB588&gt;
#重写了__str__()之后返回的是：this is Person class

#重写了__eq__()之后是True,因为定义了名字和年龄相同就是对的
#没重写就是False
</code></pre><p>所以上述当我们定义好了一个类之后，如果对于一些特殊的功能需要重新定义或者重新输出信息，可以采用上述的方法进行重写，输出我们想要的有用信息。</p>
<h3 id="4-2-枚举类——enum"><a href="#4-2-枚举类——enum" class="headerlink" title="4.2 枚举类——enum"></a>4.2 枚举类——enum</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;枚举的目的是将我们用的一些变量变的更加的有意义，并且同一类的全部都存储到一起。Python中枚举主要通过enmu这个类来实现。我们通过一段代码来看看这个类的建立和使用。</p>
<pre><code>import enum

@enum.unique
class WeekDays(enum.Enum): #enum.IntEnum
    MONDAY = 1
    TUESDAY = 2
    WEDNESDAY = 3
    THURSDAY = 4
    FRIDAY = 5

day = WeekDays.WEDNESDAY
print(day)
print(day.name,day.value)
</code></pre><p>&nbsp;&nbsp;&nbsp;&nbsp;枚举类的定义如上述所示，也是采取class 枚举类名（enum.Enum）：的方式进行。枚举类定义好后，day是进行实例化对象，其像字典一样有name和value。为了防止枚举成员值重复，可以加一个装饰器@enum.unique，如果想要限定枚举类的类型，比如可以继承enum.IntEnum。</p>
<h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;本篇第2部分主要介绍了Python中类的定义，变量和方法。第3部分主要针对类的封装性、继承性、多态性三个方面来对类的一些特性做了介绍，第4部分着重介绍了Python中的基本类以及方法的重写。希望通过这部分能对Python的类有一个全面的了解，为后续代码的书写增添一些内容。</p>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python基础</tag>
      </tags>
  </entry>
  <entry>
    <title>Python函数与装饰器</title>
    <url>/2019/10/30/Python/3.python%E5%87%BD%E6%95%B0/</url>
    <content><![CDATA[<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1.引言"></a>1.引言</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;本部分主要介绍一些Python的函数式编程。Python中的函数是定义在模块之中的。如果函数定义在类中，那么称之为类的方法，如果定义在别的函数中，那么就是嵌套函数。要了解一个语言函数的写法，那么必须要了解的几个函数的基本要素：1）如何定义。2）函数参数。3）返回值。本部分除了介绍函数重要的三部分之外，还会介绍一下生成器、三大基础函数：filter()、map()和reduce()和装饰器。</p>
<a id="more"></a>
<hr>
<h2 id="2-函数的基本要素"><a href="#2-函数的基本要素" class="headerlink" title="2.函数的基本要素"></a>2.函数的基本要素</h2><h3 id="2-1-函数的定义"><a href="#2-1-函数的定义" class="headerlink" title="2.1 函数的定义"></a>2.1 函数的定义</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;Python中函数的定义非常简单，用的是def关键字。</p>
<pre><code>def 函数名(参数):
    函数功能主体
    return 返回值
</code></pre><p>但是其中一个注意事项是Python中函数必须是先定义后调用，否则就发生错误。</p>
<h3 id="2-2-函数的参数"><a href="#2-2-函数的参数" class="headerlink" title="2.2 函数的参数"></a>2.2 函数的参数</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;Python中函数的参数传递比较灵活，主要有：</p>
<ul>
<li>关键字参数</li>
<li>参数默认值</li>
<li>可变参数<br>接下来我们通过一个函数来了解下这几个函数参数的意义。</li>
</ul>
<p>输入</p>
<pre><code>def test(name,age,height=170,*kwage,**info):
    print(name,age,height)
    for value in kwage:
        print(value)
    for key,value in info.items():
        print(key,value)
nums_kwage = [1,2,3,4,5]
nums_info = {&quot;001&quot;:&quot;aaa&quot;,&quot;002&quot;:&quot;bbb&quot;,&quot;003&quot;:&quot;ccc&quot;}
test(&quot;javierhuang&quot;, 24,175, *nums_kwage,**nums_info)
</code></pre><p>输出：</p>
<pre><code>javierhuang 24 175
1
2
3
4
5
001 aaa
002 bbb
003 ccc
</code></pre><p>&nbsp;&nbsp;&nbsp;&nbsp;通过上述的代码我们来分析一下，name和age就是属于关键字参数。我们在调用的时候可以直接填入数字，按顺序对应，当然也可以用name=”javierhuang”这样来进行调用。但是如果第一个用了关键字调用，后续的所有参数都必须要用关键字调用，否则会报错。height是一个默认参数，默认是170，当然在调用的时候也可以对其进行修改。这里重点介绍一下<em>和**表示的可变参数。前者表示的是将后续的所有参数都用列表输入。后者表示了后续的所有参数用字典储存。在调用过程中，实际上</em>nums_kwage是将列表解包，然后变为[1,2,3,4,5].实际在函数中相当于输入了1,2,3,4,5个参数。同理后面的nums_info也是。所以这样就达到了一个可变参数输入的目的。在程序设计的时候可以多留个接口出来。</p>
<h3 id="2-3-函数返回值"><a href="#2-3-函数返回值" class="headerlink" title="2.3 函数返回值"></a>2.3 函数返回值</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;Python中函数不需要指明返回类型，只需要return参数就行。如果没有返回值可以不写返回值或者return None。如果有一个返回参数，那就直接return 返回值。如果有多个返回值，那么可以用tuple将他们括号起来，直接return (A,B,C)。然后调用函数的时候，比如res = test(),res[0]就代表了A,res[1]就代表了B。</p>
<h2 id="3-生成器与基础函数"><a href="#3-生成器与基础函数" class="headerlink" title="3. 生成器与基础函数"></a>3. 生成器与基础函数</h2><h3 id="3-1-生成器"><a href="#3-1-生成器" class="headerlink" title="3.1 生成器"></a>3.1 生成器</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;Python的生成器实际上在<a href="https://jaryhuang.github.io/2019/10/23/Python/1.python%E8%AF%AD%E6%B3%95%E5%9F%BA%E7%A1%80/" target="_blank" rel="noopener">Python语法基础</a>中的2.9 yield关键字中做了一个说明。这里再对这个生成器方法做一个介绍。生成器，返回的是一个generate对象，它存在的意义是，比如我们要返回一个1到100万的数进行遍历。如果通过列表产生这个数，那么就占用了100万个单元了。但是如果用生成器呢，相当于在每次调用的时候仅仅迭代该值，并将该值填到内存单元，只占用一个单元，非常适合大序列的处理。<br>输入</p>
<pre><code>def fib(num):
    for i in range(num):
        yield i**i
gefib = fib(5)
print(type(gefib))
for value in gefib:
    print(value)

res = fib(3)
print(res.__next__())
</code></pre><p>输出：</p>
<pre><code>&lt;class &#39;generator&#39;&gt;
1
1
4
27
256

1
</code></pre><p>&nbsp;&nbsp;&nbsp;&nbsp;上述试验中定义了fib这样一个生成器，我们可以看到得到的类型是generator生成器，我们可以通过遍历来进行访问，在每次遍历的时候，都会使其运行.<strong>next</strong>()函数来调用生成下一个数。这是其generator的内置功能函数。因此合理的运用生成器会使得程序内存消耗更少。</p>
<h3 id="3-2-三大基础函数"><a href="#3-2-三大基础函数" class="headerlink" title="3.2 三大基础函数"></a>3.2 三大基础函数</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;函数式编程的思想主要是通过函数来对数据进行处理。对数据处理的主要方式是：</p>
<ul>
<li>过滤：filter()</li>
<li>映射：map()</li>
<li>聚合：reduce()</li>
</ul>
<h4 id="3-2-1-过滤filter"><a href="#3-2-1-过滤filter" class="headerlink" title="3.2.1 过滤filter()"></a>3.2.1 过滤filter()</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;过滤可以对可迭代的对象进行滤波，语法主要是filter(function, iterable)。其中function是一个功能函数，参数iterable是一个可迭代对象。在调用filter的时候，可迭代对象iterable的元素会被逐一传入到function函数中，function函数编写的是过滤的条件，返回布尔值，如果是True的会被保留，如果是False的就会被过滤掉。<br>测试：</p>
<pre><code>num = range(1,20)
num_filter = filter(lambda v:v%3 == 0,num)
print(list(num_filter)) 
</code></pre><p>输出：</p>
<pre><code>[3, 6, 9, 12, 15, 18]
</code></pre><h4 id="3-2-2-映射map"><a href="#3-2-2-映射map" class="headerlink" title="3.2.2 映射map()"></a>3.2.2 映射map()</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;映射是对可迭代的对象进行变换，利用map函数来进行，主要语法如下map(function,iterable),其中function是一个功能函数，iterble是一个可迭代的对象，在调用map时，可迭代对象的每一个元素都被逐一的传入到function中，在function中进行元素的变换。<br>测试：</p>
<pre><code>users = [&#39;Tony&#39;, &#39;Tom&#39;, &#39;Ben&#39;, &#39;Alex&#39;]
users_map = map(lambda u:u.lower(), users)
print(list(users_map))
</code></pre><p>输出：</p>
<pre><code>[&#39;tony&#39;, &#39;tom&#39;, &#39;ben&#39;, &#39;alex&#39;]
</code></pre><p>上述代码主要是将所有对象的大写字母变为小写字母。</p>
<h4 id="3-2-3-聚合reduce"><a href="#3-2-3-聚合reduce" class="headerlink" title="3.2.3 聚合reduce()"></a>3.2.3 聚合reduce()</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;聚合操作是将多个数据聚合到一起输出多个数据，聚合操作最基础的函数就是归纳函数reduce(),语法如下reduce(function, iterable,initializer)<br>输入：</p>
<pre><code>from functools import reduce
a = [1,2,3,4]
a_reduce = reduce(lambda acc,i: acc+i,a,2)
print(a_reduce)
</code></pre><p>输出：</p>
<pre><code>12
</code></pre><p>此函数的作用是将a进行累加，然后2是一个初始值也进行累加，最终有一个结果。</p>
<h2 id="4-装饰器"><a href="#4-装饰器" class="headerlink" title="4. 装饰器"></a>4. 装饰器</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;装饰器（Decorator）的作用主要是传入一个函数，返回一个函数。而在这个装饰器中加入了我们的一些公用的代码。这里借用一篇文章的内容<a href="https://lotabout.me/2017/Python-Decorator/" target="_blank" rel="noopener">Python Decorator(装饰器)</a>来进行说明，这篇文章写的很不错，清晰明廖的讲明白了用装饰器的目的。按照这个思路我们来进行讲解。</p>
<p>首先假设我们写了一个函数并测试功能：</p>
<pre><code>def add(a,y=10):
    return x+y
print(add(10))
print(add(20))
</code></pre><p>现在，我们想看看这个函数的运行时间，于是我们利用了time库来进行</p>
<pre><code>from time import time

before = time()
print(add(10))
after = time()
print(&quot;time taken: {}&quot;.format(after - before))

before = time()
print(add(20))
after = time()
print(&quot;time taken: {}&quot;.format(after - before))
</code></pre><p>那么这个代码就变得复杂了起来，如果我们有很多个add,那么所有都需要复制一遍了。因此是否有简单的办法呢，我们将这个改掉，将我们的计算时间功能直接嵌入到函数之中。</p>
<pre><code>from time import time

def add(x, y=10):
    before = time()
    result = x + y
    after = time()
    print(&#39;time taken: {}&#39;.fomat(after - before))
    return result

print(add(10))
print(add(20))
</code></pre><p>代码变得简单了很多，但是如果我们又写了一个函数，那么如果我们想同样测试这个函数的性能，那么我们只能再加个上述步骤复制一遍。</p>
<pre><code>def sub(x, y=10):
    before = time()
    result = x - y
    after = time()
    print(&#39;time taken: {}&#39;.fomat(after - before))
    return result
</code></pre><p>这感觉又带来了麻烦，因为这串代码又反复的出现了，那么我们是否能将这块代码抽象出来了。于是我们用函数来表示，并将另外一个函数作为传入参数。</p>
<pre><code>from time import time

def timer(func, x, y = 10):
    before = time()
    result = func(x, y)
    after = time()
    print(&#39;time taken: {}&#39;.fomat(after - before))
    return result

def add(x, y = 10):
    return x + y

def sub(x, y = 10):
    return x - y

print(timer(add, 10))
print(timer(add, 20, 30))
</code></pre><p>但是这样还是带来了一个麻烦，每次我们建立一个新函数，相当于用一个timer函数来封装这个函数，那么是否有直接的函数呢。那么再改进下，直接返回函数就好。</p>
<pre><code>def timer(func):
    def wraper(x, y=10):
        before = time()
        result = func(x, y)
        after = time()
        print(&#39;time taken: {}&#39;.fomat(after - before))
        return result
    return wraper

def add(x, y = 10):
    return x + y
add = timer(add)

def sub(x, y = 10):
    return x - y
sub = timer(sub)

print(add(10))
print(add(20, 30))
</code></pre><p>这样之后，我们相当于用一个timer函数把add给包起来了，而且重新返回了这个add函数，调用的时候相当于没有区别。这里的timer就是一个“装饰器”，它接受了一个函数，又返回了一个新的函数。在内部对原来的函数进行了装饰。</p>
<pre><code>def add(x, y = 10):
    return x + y
add = timer(add)

def sub(x, y = 10):
    return x - y
sub = timer(sub)
</code></pre><p>这种方式写的话，如果我们的add函数名字做了修改，那么我们下面的几个名字都需要修改，那有没有更好的办法来进行替换呢。</p>
<pre><code>@timer
def add(x, y=10):
    return x + y
</code></pre><p>这就是我们最常见的装饰器了。</p>
<h3 id="4-1-带参数的装饰器"><a href="#4-1-带参数的装饰器" class="headerlink" title="4.1 带参数的装饰器"></a>4.1 <strong>带参数的装饰器</strong></h3><p>在我们对装饰器的使用是会看到装饰器带一些参数，举一个例子：</p>
<pre><code>def use_logging(level):
    def decorator(func):
        def wrapper(*args, **kwargs):
            if level == &quot;warn&quot;:
                logging.warn(&quot;%s is running&quot; % func.__name__)
            elif level == &quot;info&quot;:
                logging.info(&quot;%s is running&quot; % func.__name__)
            return func(*args)
        return wrapper

    return decorator

@use_logging(level=&quot;warn&quot;)
def foo(name=&#39;foo&#39;):
    print(&quot;i am %s&quot; % name)
</code></pre><p>看了这段代码也就理解了，首先use_logging(level = “warn”)调用返回了decorator函数，调用decorator(foo),返回了wrapper调用wrapper后犯规foo(*args)。</p>
<h3 id="4-2-类作为装饰器"><a href="#4-2-类作为装饰器" class="headerlink" title="4.2 类作为装饰器"></a>4.2 类作为装饰器</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;如果说 Python 里一切都是对象的话，那函数怎么表示成对象呢？其实只需要一个类实现 <strong>call</strong> 方法即可。</p>
<pre><code>class Timer:
    def __init__(self, func):
        self._func = func
    def __call__(self, *args, **kwargs):
        before = time()
        result = self._func(*args, **kwargs)
        after = time()
        print(&quot;elapsed: &quot;, after - before)
        return result

@Timer
def add(x, y=10):
    return x + y
</code></pre><p>把类的构造函数当成了一个装饰器，它接受一个函数作为参数，并返回了一个对象，而由于对象实现了 <strong>call</strong> 方法，因此返回的对象相当于返回了一个函数。因此该类的构造函数就是一个装饰器。</p>
<h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;本篇第2部分主要介绍了Python中函数的写法，从函数的三要素出发进行讲解。第3部分主要讲解了Python中有一个特别的函数叫生成器，它不适用return,适用yeild,用于解决大量数据占内存等问题。而Python主要的数据处理基础函数是滤波（filter)、映射（map)和聚合（reduce）。在了解了这些之后，第四部分还特别介绍了Python的一个很好用的叫装饰器。顾名思义，是用来封装函数的装饰器，其输入是一个函数，返回的也是一个函数，可以使得我们对一些重复运用的模块进行封装使用。</p>
<h2 id="6-参考"><a href="#6-参考" class="headerlink" title="6. 参考"></a>6. 参考</h2><p><a href="https://lotabout.me/2017/Python-Decorator/" target="_blank" rel="noopener">Python Decorator装饰器</a></p>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python基础</tag>
      </tags>
  </entry>
  <entry>
    <title>Python数据类型与结构</title>
    <url>/2019/10/25/Python/2.python%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/</url>
    <content><![CDATA[<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1.引言"></a>1.引言</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;本部分主要介绍一些Python的数据类型，数据结构，数据间的转换等。主要包括</p>
<ul>
<li><p>数据类型</p>
<ul>
<li>数字</li>
<li>字符串</li>
</ul>
</li>
<li><p>数据结构</p>
<ul>
<li>元组</li>
<li>列表</li>
<li>集合</li>
<li>字典</li>
</ul>
</li>
</ul>
<a id="more"></a>
<hr>
<h2 id="2-数据类型"><a href="#2-数据类型" class="headerlink" title="2.数据类型"></a>2.数据类型</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;Python数据类型的话主要就是数字和字符串。本部分主要按照数字和整数来了解下性质以及互相转换的方式。</p>
<h3 id="2-1-数字类型和相互转换"><a href="#2-1-数字类型和相互转换" class="headerlink" title="2.1 数字类型和相互转换"></a>2.1 数字类型和相互转换</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;Python中数字类型有四种：</p>
<ul>
<li><p><strong>整数</strong>：int，Python3都是长整数。模型情况下一个数字就表示10进制。如果要表示其他进制的话，得加上其他的符号。</p>
<pre><code>  12          #整形
  0b1100      #二进制
  0O12        #八进制
  0X0a        #十六进制
</code></pre></li>
<li><p><strong>浮点</strong>：float,主要是用于存小数，科学计数法表示的数据是浮点型。</p>
<pre><code>  1.0         #浮点型
  2.25e2      #浮点型225.0
  1.34e-2     #浮点型0.0134
</code></pre></li>
<li><p><strong>复数</strong>：complex,Python支持复数类型</p>
<pre><code>  2+3j            #复数
  (2+3j)+(3+2j)   #复数
</code></pre></li>
<li><p><strong>布尔</strong>：bool,只有两个值True和False。只有空数据或者数字0会转换为False</p>
<pre><code>  bool(0)         #False
  bool(2)         #True
  bool(&#39;&#39;)        #False
  bool(&#39; &#39;)       #True
  bool([])        #False
  bool({})        #False
  bool(())        #False
</code></pre></li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;上述4中数据类型，除了复数之外，其余三种可以互相转换。首先，我们了解下在计算的时候，如果整数和浮点数相加得到的是什么型呢。</p>
<p>输入：</p>
<pre><code>print(type(1+1.0))
print(type(1+True))
print(type(1.0+False))
</code></pre><p>输出：</p>
<pre><code>&lt;class &#39;float&#39;&gt;
&lt;class &#39;int&#39;&gt;
&lt;class &#39;float&#39;&gt;
</code></pre><p>通过上述的试验我们可以知道，在计算的时候，Python内部的数据类型转换为float&gt;int&gt;bool，最终的结果类型以出现类型的最高级别排序如“浮点+整型=浮点”。<br>除了Python内部计算时有一个数据转换，当我们得到一个数据之后，是否能主动将这个数据进行类型转换呢？这就得用到自己的转换函数了，分别为float(),int(),bool()</p>
<p>测试：</p>
<pre><code>print(type(float(True)))
print(type(int(1.3)))
print(type(bool(3)))
</code></pre><p>输出：</p>
<pre><code>&lt;class &#39;float&#39;&gt;
&lt;class &#39;int&#39;&gt;
&lt;class &#39;bool&#39;&gt;
</code></pre><h3 id="2-2-字符串类型"><a href="#2-2-字符串类型" class="headerlink" title="2.2 字符串类型"></a>2.2 字符串类型</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;Python中字符串类型是str,是由一系列字符组成的。从左到右按顺序排序。</p>
<h4 id="2-2-1-字符串的表示与转义字符串"><a href="#2-2-1-字符串的表示与转义字符串" class="headerlink" title="2.2.1 字符串的表示与转义字符串"></a>2.2.1 字符串的表示与转义字符串</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;字符串采用’xxxx’或者”xxxx”表示，有一些特殊的字符比如换行等，需要用到转义字符。</p>
<ul>
<li>\t：tab</li>
<li>\n: 换行</li>
<li>\r: 回车</li>
<li>\\”：双引号</li>
<li>\\’：单引号</li>
<li>\\\：反斜杆</li>
</ul>
<p>测试：</p>
<pre><code>a = &quot;1024 haha&quot;
b = &#39;today\thaha&#39;
c = &#39;today\nhuhu&#39;
d = r&quot;today\tyiyi&quot;
print(a)
print(b)
print(c)
print(d)
</code></pre><p>输出：</p>
<pre><code>1024 haha
today    haha
today
huhu
today\tyiyi
</code></pre><h4 id="2-2-2-字符串格式化"><a href="#2-2-2-字符串格式化" class="headerlink" title="2.2.2 字符串格式化"></a>2.2.2 字符串格式化</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;字符串的格式化主要是利用”{0:5d}{1:10.2f}{2:s}{3:e}{4:o}{6:x}”.format(A,B,C,D,E,F)形式来处理，其中{}表示占位符，站位符中的：前的数字代表这对应后面的哪个数。默认的形式是0,1,2,3…顺序排列，：后面的代表的是转换的数据类型。d，f，s比较数据，分别代表的是整数，浮点数，字符串。e，o，x代表的是科学家计数法，8进制和16进制。5d代表的是保留5个数字的长度。</p>
<p>测试：</p>
<pre><code>name = &quot;javierhuang&quot;
age = 24
weight=123.21

print(&quot;姓名：{}，年龄：{}，体重：{}。&quot;.format(name,age,weight))
print(&quot;姓名：{2}，年龄：{0}，体重：{1}。&quot;.format(age,weight,name))
print(&quot;姓名：{0:3s}，年龄：{1:5d}，体重：{2:10.2f}。&quot;.format(name,age,weight))
print(&quot;姓名：{0:3s}，年龄：{1:5d}，体重：{2:e}。&quot;.format(name,age,weight))
print(&quot;年龄：{0:d}，年龄：{1:o}，年龄：{2:x}。&quot;.format(age,age,age))
</code></pre><p>输出:</p>
<pre><code>姓名：javierhuang，年龄：24，体重：123.21。
姓名：javierhuang，年龄：24，体重：123.21。
姓名：javierhuang，年龄：   24，体重：    123.21。
姓名：javierhuang，年龄：   24，体重：1.232100e+02。
年龄：24，年龄：30，年龄：18。
</code></pre><h4 id="2-2-3-字符串查找与类型转换"><a href="#2-2-3-字符串查找与类型转换" class="headerlink" title="2.2.3 字符串查找与类型转换"></a>2.2.3 字符串查找与类型转换</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;字符串的查找主要是通过find(sub,start,end)和rfind(sub,start,end)方法，这两个都是字符串的功能函数。前者是从左向右查找，后者是从右向左查找。start和end是查找的起始位置和终点位置，默认是从最左到最右。</p>
<p>测试：</p>
<pre><code>introduction = &quot;javierhuang zju tencent&quot;

print(introduction.find(&quot;e&quot;))
print(introduction.rfind(&quot;e&quot;))
</code></pre><p>输出:</p>
<pre><code>4
20
</code></pre><p>上述find或者rfind函数一个左边找，一个右边找，而且找到一个符合就不找了。那如果要找到所有位置呢？那么这个不愁，只需要在每次找到位置之后，在以这个位置为起始位置继续查找即可。结束的条件是找到的位置值为-1（查找不到，返回的是-1）。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;讲完了字符串的查找，那么继续看一下字符串的格式转换。Python数据类型有数字有字符串，他们之间的转换通过函数int(),float()和str()来实现。但是字符串要转为数字，首要前提是能转。比如”abcd”这种就会报错了。</p>
<p>输入：</p>
<pre><code>print(int(&quot;abcd&quot;))
print(int(&#39;2.5&#39;))
print(float(2.5))
print(str(3.24))  
print(&#39;{:.1f}&#39;.format(3.24)) #str不能格式化，采用format形式
</code></pre><p>输出：</p>
<pre><code>ValueError: invalid literal for int() with base 10: &#39;abcd&#39;
ValueError: invalid literal for int() with base 10: &#39;2.5&#39;
2.5
3.24
3.2
</code></pre><h2 id="3-数据结构"><a href="#3-数据结构" class="headerlink" title="3. 数据结构"></a>3. 数据结构</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;Python的数据结构主要有三种：序列、集合和字典。Python中没有数组结构，因为Python动态语言的特性，它不限定数据类型，而数组这个数据结构是要求统一的数据类型的。如果要用数组这个数据类型，那么可以导入numpy库。</p>
<h3 id="3-1-序列"><a href="#3-1-序列" class="headerlink" title="3.1 序列"></a>3.1 序列</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;序列这种数据结构，它具有可包含重复元素、元素有序、可以通过索引访问、可迭代（返回对象，具有方法）等特点。Python中序列的数据结构主要有元组(tuple)，列表(list)和字符串(str)。接下来简单介绍一些序列这种数据结构一些公共的特点。</p>
<ul>
<li><strong>索引操作</strong>  ：可通过序列的index来进行访问，比如(1,2,3),a[0]访问的是第一个元素。a[-1]访问的是最后一个元素。</li>
<li><strong>加和乘</strong>   ：序列的加乘操作都是追加的效果，如a=”abc”,a+a=”abcabc”，a*3=”abcabcabc”。</li>
<li><strong>切片</strong>    ：序列的切片操作是可以获得内部的sub子块的,形式为 <em>[start:end:step]</em>。主要通过索引,比如a=”abcdefghijklmn”。a[0:2]=”ab”，a[-2:]=”mn”，a[0:7:2]=”aceg”。</li>
</ul>
<h4 id="3-1-1-元组"><a href="#3-1-1-元组" class="headerlink" title="3.1.1 元组"></a>3.1.1 元组</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;元组是一种不可变序列，一旦创建就不能修改，和字符串一样。元组的创建方式可以直接使用(1,2,3)或者tuple([1,2,3])等方式创建。但是注意，如果是单元素的元组的话(1,)一定得加后面的逗号，否则就变为int数据了。<br>&nbsp;&nbsp;&nbsp;&nbsp;元组是序列，因此它具有切片、加乘和索引等操作。对于元组来讲，可以对元组进行解包和遍历。</p>
<p>测试：</p>
<pre><code>#解包
a=(1,2,3,4,5)
num1,num2,num3,num4,num5 = a
print(num1,num2,num3,num4,num5)

num1,num2,*num3 = a
print(num1,num2,num3)

num1, _, num3, *_= a
print(num1,num3)
#遍历
for item in a:
    print(item)
</code></pre><p>输出：</p>
<pre><code>1 2 3 4 5
1 2 [3, 4, 5]
1 3
1
2
3
4
5
</code></pre><h4 id="3-1-2-列表"><a href="#3-1-2-列表" class="headerlink" title="3.1.2 列表"></a>3.1.2 列表</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;列表的创建采用list((1,2,3))或者[1,2,3]方式。其和元组的一个区别就是列表具有可变性，可以对列表进行修改,主要是增删改插操作。</p>
<ul>
<li>增加：list.append(x)是增加一个元素。list.extend(X)是增加一个列表</li>
<li>删除：list.remove(a)是删除一个匹配元素输入的是值。list.pop(index)删除指定的inde下的元素，并将元素返回。</li>
<li>修改：这个操作不许要对应的函数，直接根据索引值来修改，比如a[0]=5就直接将a[0]修改为5了。</li>
<li>插入：list.insert(index,value)插入一个数据，输入插入的index和插入的值。</li>
</ul>
<p>测试：</p>
<pre><code>test=[5,8,9,12]
x=6
X=[1.3,2,6]

#增
test.append(x)
print(test)         #[5, 8, 9, 12, 6]
test.extend(X)
print(test)         #[5, 8, 9, 12, 6, 1.3, 2, 6]

#删
test.remove(8)
print(test)         #[5, 9, 12, 6, 1.3, 2, 6]
print(test.pop())   # 6
print(test)         #[5, 9, 12, 6, 1.3, 2]

#改
test[1]=100
print(test)         #[5, 100, 12, 6, 1.3, 2]

#插
test.insert(2,1000)
print(test)         #[5, 100, 1000, 12, 6, 1.3, 2]
</code></pre><p>&nbsp;&nbsp;&nbsp;&nbsp;列表中除了上述的增删改插操作外，还有一些其他常见的操作。</p>
<ul>
<li>list.reverse():倒置列表</li>
<li>list.copy():复制列表</li>
<li>list.clear():清除列表中所有元素</li>
<li>list.index(value,start,end):查找value在list中的位置，start是起始，end是终止。</li>
<li>list.count(value)：返回value在列表中出现的次数。</li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;产生一个列表，我们可以通过上述的[1,2,3]来写，但是如果我们要差生1000个数字，总不能通过写1000个数字来吧。因此这里介绍一种方法叫列表推导式。它是将一个数据结构作为数据，经过过滤和处理得到一个列表。</p>
<p>输入：</p>
<pre><code>a = [x**2 for x in range(20) if x%2==0]
print(a)
b = [x*y for x in range(5) for y in range(6) if (x+y)%3==0]
print(b)
</code></pre><p>输出：</p>
<pre><code>[0, 4, 16, 36, 64, 100, 144, 196, 256, 324]
[0, 0, 2, 5, 2, 8, 0, 9, 8, 20]
</code></pre><p><strong>列表和元组的相互转化采用list()和tuple()的方式。list((1,2,3))=[1,2,3],tuple([1,2,3])=(1,2,3)</strong></p>
<h3 id="3-2-集合"><a href="#3-2-集合" class="headerlink" title="3.2 集合"></a>3.2 集合</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;集合是一种可迭代的、无序、不能含有重复元素的数据结构。集合分为可变集合（set）和不可变结合（frozenset）<br>测试：</p>
<h4 id="3-2-1-可变集合（set）"><a href="#3-2-1-可变集合（set）" class="headerlink" title="3.2.1 可变集合（set）"></a>3.2.1 可变集合（set）</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;可变集合的创建采取set({a,b,c})等形式来进行创建，创建的时候会剔除掉重复元素。可变集合是可以修改的，因此可变集合具有一些操作方法，增删等。</p>
<ul>
<li><strong>添加元素</strong>：add(elem)，如果元素存在，则不能添加</li>
<li><strong>删除元素</strong>: remove(elem)，如果元素不存在会报错。discard(elem)，如果元素不存在不会报错。pop()删除返回集合中任一一个元素。</li>
<li><strong>清除元素</strong>：clear(),清除集合。</li>
</ul>
<p>集合是无序的，因此无法通过索引来访问结合，但是对于集合的遍历，可以取出集合的每一个元素，但是取的顺序是不一定的。</p>
<p>输入：</p>
<pre><code>data_set = {&#39;12&#39;,&#39;34&#39;,&#39;56&#39;}
print(type(data_set))
for i,data in enumerate(data_set):
print(i,data)
</code></pre><p>输出：</p>
<pre><code>&lt;class &#39;set&#39;&gt;
0 56
1 34
2 12
</code></pre><p>集合和列表一样具有推导式，除了[]改为{}之外，其他操作都一样，唯一一个区别就是集合推导式会将重复的元素直接过滤掉。</p>
<h4 id="3-2-2-不可变集合（frozenset"><a href="#3-2-2-不可变集合（frozenset" class="headerlink" title="3.2.2 不可变集合（frozenset)"></a>3.2.2 不可变集合（frozenset)</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;不可变结合的创建直接采用frozenset({“12”,”34”,”56”}),因为其是不可变，因此无法对其进行修改。</p>
<h3 id="3-3-字典"><a href="#3-3-字典" class="headerlink" title="3.3 字典"></a>3.3 字典</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;字典是一种可迭代的、可变的数据结构。键（key)不能有重复，而值（value)能够重复出现。一个键对应一个值，通过键来访问值。字典的数据类型是（dict），那么对于字典的创建采用dict()函数或者用{键1：值1，键2：值2，键3：值3，…}将“键：值”对括号起来。</p>
<p>输入：</p>
<pre><code>dict1={1:&quot;haha&quot;,2:&quot;hehe&quot;,3:&quot;heihei&quot;,4:&quot;xixi&quot;}
print(type(dict1))

dict2=dict([(1,&#39;haha&#39;),(2,&#39;haha&#39;),(3,&#39;heihei&#39;),(4,&#39;xixi&#39;)])
print(dict2)

dict3 = dict(zip([1,2],[&#39;haha&#39;,&#39;haha&#39;])) 
print(dict3)
</code></pre><p>输出：</p>
<pre><code>&lt;class &#39;dict&#39;&gt;
{1: &#39;haha&#39;, 2: &#39;haha&#39;, 3: &#39;heihei&#39;, 4: &#39;xixi&#39;}
{1: &#39;haha&#39;, 2: &#39;haha&#39;}
</code></pre><h4 id="3-3-1-字典的修改"><a href="#3-3-1-字典的修改" class="headerlink" title="3.3.1 字典的修改"></a>3.3.1 字典的修改</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;字典可以被修改，主要针对键和值同时操作。字典的修改操作主要包括增、删除、修改。</p>
<p>输入：</p>
<pre><code>dict1={1:&quot;haha&quot;,2:&quot;hehe&quot;,3:&quot;heihei&quot;,4:&quot;xixi&quot;}
dict1[5]=&quot;huhu&quot;                         #增加
print(dict1)

del dict1[2]                            #删除
print(dict1)
print(dict1.popitem())                  #删除

dict1[1]=&#39;lala&#39;                         #修改
print(dict1)
</code></pre><p>输出：</p>
<pre><code>{1: &#39;haha&#39;, 2: &#39;hehe&#39;, 3: &#39;heihei&#39;, 4: &#39;xixi&#39;, 5: &#39;huhu&#39;}
{1: &#39;haha&#39;, 3: &#39;heihei&#39;, 4: &#39;xixi&#39;, 5: &#39;huhu&#39;}
(5, &#39;huhu&#39;)
{1: &#39;lala&#39;, 3: &#39;heihei&#39;, 4: &#39;xixi&#39;}
</code></pre><h4 id="3-3-2-字典的访问与遍历"><a href="#3-3-2-字典的访问与遍历" class="headerlink" title="3.3.2 字典的访问与遍历"></a>3.3.2 字典的访问与遍历</h4><ul>
<li>get(key):通过键返回值</li>
<li>items():返回字典的所有键值对</li>
<li>keys():返回字典的所有键</li>
<li>values():返回字典的所有值</li>
</ul>
<p>那么可以根据上述的访问方式来遍历字典。</p>
<p>输入：</p>
<pre><code>dict1={1:&quot;haha&quot;,2:&quot;hehe&quot;,3:&quot;heihei&quot;,4:&quot;xixi&quot;}

#遍历键
for k in dict1.keys():
    print(k)

#遍历值
for v in dict1.values():
    print(v)

#遍历键和值
for k,v in dict1.items():
    print(k,v)
</code></pre><p>输出：</p>
<pre><code>1
2
3
4
haha
hehe
heihei
xixi
1 haha
2 hehe
3 heihei
4 xixi
</code></pre><p><strong>字典的推导式：</strong></p>
<pre><code>dict1={1:&quot;haha&quot;,2:&quot;hehe&quot;,3:&quot;heihei&quot;,4:&quot;xixi&quot;}
out_dict = {k:v for k,v in dict1.items() if k%2==0}
print(out_dict)          #{2: &#39;hehe&#39;, 4: &#39;xixi&#39;}
</code></pre><h2 id="4-总结"><a href="#4-总结" class="headerlink" title="4.总结"></a>4.总结</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;第二部分主要介绍了Python的数据类型包括数字类型和字符类型。数字类型中重点介绍了4中不同的数字类型，以及他们的转换关系。在字符类型中主要讲了字符的创建、查找和格式化。第三部分主要讲了Python中的数据结构。主要为列表、集合和字典。列表中分为元组和列表，前者不可变，后者可变。重点介绍了列表的增删改插以及列表推导式。在集合中主要讲解了可变集合和不可变结合。介绍了可变集合的增删改方法和遍历。在字典这部分主要讲解了字典的创建、访问与遍历，以及字典推导式。希望这块的学习能更加理解Python的数据类型和存储方式。</p>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python基础</tag>
      </tags>
  </entry>
  <entry>
    <title>Python语法基础(关键字、标识符、模块等)</title>
    <url>/2019/10/23/Python/1.python%E8%AF%AD%E6%B3%95%E5%9F%BA%E7%A1%80/</url>
    <content><![CDATA[<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1.引言"></a>1.引言</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;本部分主要介绍一些Python的基础语法，主要包括</p>
<ul>
<li><p>关键字</p>
</li>
<li><p>常量和变量</p>
</li>
<li><p>模块与包</p>
</li>
</ul>
<a id="more"></a>
<hr>
<h2 id="2-关键字"><a href="#2-关键字" class="headerlink" title="2.关键字"></a>2.关键字</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;Python的关键字有33个，False,None和True是大写的，其余都是小写的。这里介绍一下几个不常见的关键字。</p>
<h3 id="2-1-as"><a href="#2-1-as" class="headerlink" title="2.1 as"></a>2.1 <strong>as</strong></h3><p>&nbsp;&nbsp;&nbsp;&nbsp;as关键字不单独用，一般是跟着with A as B这样的结构。其主要一个要求是A这个对象有<strong>enter</strong>()和<strong>exit()</strong>方法。紧接着A对象的<strong>enter</strong>()方法被调用，将返回值赋值给B。当with A as B后面的代码被全部调用完了之后，会返回执行<strong>exit()</strong>方法。我们最常用的就是with open(‘A.txt’,’r’) as f来进行操作。</p>
<p>测试：</p>
<pre><code>class Test:
    def __enter__(self):
        print(&quot;__enter__&quot;)
        return &quot;Test Result&quot;
    def __exit__(self,type,value,trace):
        print(&quot;__exit__&quot;)

def get_test():
    return Test()

with get_test() as f:
    print(f)
</code></pre><p>输出：</p>
<pre><code>__enter__
Test Result
__exit__
</code></pre><h3 id="2-2-except"><a href="#2-2-except" class="headerlink" title="2.2 except"></a>2.2 <strong>except</strong></h3><p>&nbsp;&nbsp;&nbsp;&nbsp;except是和try一起组合用于捕获异常的。语句形式为try…except….,具体可参考异常捕获那一章节。<br>测试：</p>
<pre><code>try:
    value=5/0
except ZeroDivisionError:
    print(&quot;Zero error&quot;)
</code></pre><p>输出：</p>
<pre><code>Zero error
</code></pre><h3 id="2-3-is"><a href="#2-3-is" class="headerlink" title="2.3 is"></a>2.3 <strong>is</strong></h3><p>&nbsp;&nbsp;&nbsp;&nbsp;提到Python中的is关键字，那么这里要和==一起来比较做一个对比。<br>测试：</p>
<pre><code>a=(1,2,3)
b=(1,2,3)
print(a==b)
print(a is b)
print(id(a),id(b))
</code></pre><p>输出：</p>
<pre><code>True
False
2117640755312 2117640755384
</code></pre><p>所以从以上的测试也可以看出，== 比较的是两个值，而is比较的是内存地址。</p>
<h3 id="2-4-lambda"><a href="#2-4-lambda" class="headerlink" title="2.4 lambda"></a>2.4 <strong>lambda</strong></h3><p>&nbsp;&nbsp;&nbsp;&nbsp;lambda语句作为一个表达式，定义了一个匿名函数。它没有其他的作用，主要的就是简化程序中函数的定义，比如一些简单的函数就可以用lambda表达式来写，不需要定义一个功能函数。<br>测试：</p>
<pre><code>g = lambda x:x**2

def f(val):
    return val**2
print(g(2))
print(f(2))
</code></pre><p>输出:</p>
<pre><code>4
4
</code></pre><h3 id="2-5-raise"><a href="#2-5-raise" class="headerlink" title="2.5 raise"></a>2.5 <strong>raise</strong></h3><p>&nbsp;&nbsp;&nbsp;&nbsp;刚刚2.2提到except可以捕获异常，那么利用raise语句，我们可以自己触发异常。语法格式主要通过raise Exception(“Invalid…”,*kwag)<br>测试：</p>
<pre><code>def get_error( level ):
    if level &lt; 1:
        raise Exception(&quot;Invalid level!&quot;, level)

for i in range(5):
    try:
        get_error(i)               
    except Exception:
        print(i,&quot;error&quot;)
</code></pre><p>输出：</p>
<pre><code>0 error
</code></pre><p>通过raise Exception触发异常后，后面的代码就不会再执行。上述for循环执行到第一步就没继续执行了。raise Exception最大的一个作用就是用于自定义的一些Error的捕获。这个在异常捕获部分细细的进行讲解。</p>
<h3 id="2-6-assert"><a href="#2-6-assert" class="headerlink" title="2.6 assert"></a>2.6 <strong>assert</strong></h3><p>&nbsp;&nbsp;&nbsp;&nbsp;Python中assert（断言）语句用于判断一个表达式是否为True。在表达式为False的时候，触发异常。<br>测试：</p>
<pre><code>try:
    assert 1==2
except Exception:
    print(&quot;First assertException&quot;)

try:
    assert 1==1
except Exception:
    print(&quot;Second assertException&quot;)

assert 1==1    # 条件为 true 正常执行
assert 1==2    # 条件为 false 触发异常
</code></pre><p>输出：</p>
<pre><code>First assertException

Traceback (most recent call last):
File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
AssertionError
</code></pre><h3 id="2-7-global"><a href="#2-7-global" class="headerlink" title="2.7 global"></a>2.7 <strong>global</strong></h3><p>&nbsp;&nbsp;&nbsp;&nbsp;global关键字定义的是全局变量。在Python程序中，在函数内部定义了局部变量，当要在函数外使用这个变量的时候，就需要在这个变量前加global关键字，将局部变量变为全局变量。<br>测试：</p>
<pre><code>T = 100 
def fun():
    a = 10  
    return a+100

def fun2():
    global b
    b=10
    return b+100
print(T)    
print(b)
print(a)    
</code></pre><p>输出：</p>
<pre><code>100
10
Exception has occurred: NameError
name &#39;a&#39; is not defined
File &quot;C:\Users\Administrator\Desktop\test.py&quot;, line 777, in &lt;module&gt;
    print(a)
</code></pre><p>以上可以看出，定义在外的T是一个全局变量，在函数内的a因为是局部变量，外部访问不了。但是b加了关键字global之后就变为全局变量了，外部可以访问。</p>
<h3 id="2-8-nonlocal"><a href="#2-8-nonlocal" class="headerlink" title="2.8 nonlocal"></a>2.8 <strong>nonlocal</strong></h3><p>&nbsp;&nbsp;&nbsp;&nbsp;nonlocal的用法主要是如果在一个函数内定义了局部变量，但是在这个函数内部又有一个函数，这个函数如果想要和外面的这个函数都用同一个变量，那么在内部函数定义时要加一个nonlocal,我们通过代码再来细细的分析下这个。<br>测试：</p>
<pre><code>count = 1

def first_function():
    count = &#39;first_function函数里面&#39;
    def second_function():
        nonlocal count
        print(count)
        count = &#39;second_function函数里面&#39;
    second_function()
    print(count)

first_function()
print(count)
</code></pre><p>输出：</p>
<pre><code>first_function函数里面
second_function函数里面
1
</code></pre><p>从上述的试验可以看出。当我们执行first_function函数的时候，我们在里面是定义了count的。在second_function中我们用了nonlocal关键字之后，接下来second_function中的count就和first_function中的count是同一个变量了。因此打印出来上述结果。但是对于外面的全局变量count,局部变量count再怎么改都不影响这个结果。所以nonlocal相当于局部函数内部的”global”。</p>
<h3 id="2-9-yield"><a href="#2-9-yield" class="headerlink" title="2.9 yield"></a>2.9 <strong>yield</strong></h3><p>&nbsp;&nbsp;&nbsp;&nbsp;yield关键字在函数中的做用是将一个函数修改为生成器。生成器内部的代码执行到yield会返回，返回的内容是yield后面的表达式。等这个值返回了之后，下次再执行这个生成器的内部代码时将从上次的状态继续开始。这样当我们遍历一个列表的时候，就不会因为比如列表的长度过大而占用过多的内存，每次只是生成器生成一个返回。<br>测试：</p>
<pre><code>g = (i**i for i in range(1,5)) 
print(type(g))

def get_generate():
    for i in range(1,5):
        yield i**i

g = get_generate()
print(type(g))
</code></pre><p>输出：</p>
<pre><code>&lt;class &#39;generator&#39;&gt;
&lt;class &#39;generator&#39;&gt;
</code></pre><p>上述是创建生成器的两种方式，两者的功能是相同的，只是一个是生成表达式，一个是生成器函数。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;以上主要讲了Python中的9个比较不常用或者大家初学的时候不大了解的几个关键字。对于常见的if，while，pass等等就不做过多的介绍了。</p>
<h2 id="3-常量和变量"><a href="#3-常量和变量" class="headerlink" title="3 常量和变量"></a>3 常量和变量</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;Python中的变量不需要像C语言那样要进行数据类型确认，它是一种动态语言，只需要给标识符一个变量，Python内部自动会有定义数据类型。我们常说的常量是一些值定义之后就不能被修改，但是因为Python动态语言的特性，使得只能变量作为常量使用，只要不修改它。只能通过自律或者一些技术手段，不如定义个Const模块来将常量变为字段，如果修改就报错等方式来进行。</p>
<h2 id="4-模块与包"><a href="#4-模块与包" class="headerlink" title="4.模块与包"></a>4.模块与包</h2><h3 id="4-1-模块"><a href="#4-1-模块" class="headerlink" title="4.1 模块"></a>4.1 模块</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;在Python中一个模块就是一个文件。在模块中可以有变量、常量、函数、类等程序元素。只要将其导入，一个模块就可以使用另外一个模块的程序元素。<br>导入模块有两种形式。</p>
<pre><code>import module
from module import 元素
</code></pre><p>这是在同一个目录下的模块导入。如果是不同目录下的导入，或者两个module的名字相同的情况下，那么建议采用包的形式。</p>
<h3 id="4-2-包"><a href="#4-2-包" class="headerlink" title="4.2 包"></a>4.2 包</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;创建包的方式是通过在每个文件夹中，比如pkg1和pkg2，建立<strong>init</strong>.py文件，以及module.py。其中<strong>init</strong>.py文件夹中可以导入包内部module的一些元素等。接下来导入文件，主要是通过包名+module名的方式。</p>
<pre><code>import pkg1.module as module1
from pkg2 import module
</code></pre><h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5.总结"></a>5.总结</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;本篇主要介绍了Python的基础语法知识，包括一些稍微不常用一点的关键字，常量变量，模块等有一些基本的了解，为后续的Python学习奠定点基础。</p>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python基础</tag>
      </tags>
  </entry>
  <entry>
    <title>LeetCode括号类问题：20,22,32</title>
    <url>/2019/05/05/Leetcode/leetcode_%E6%8B%AC%E5%8F%B7%E7%B1%BB%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1.引言"></a>1.引言</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;进行实习面试，刷了很多code,这次要进行系统的整理一下，同时督促自己继续努力的刷题，要坚持住，code才是王道！这次主要整理下做的leetCode中关于括号类的问题，其中涉及到 <strong>验证匹配</strong>，<strong>排列组合</strong>，<strong>最长有效</strong>的问题，不同类型的题目都有不同的解题方式。<br><a id="more"></a></p>
<h2 id="2-验证括号"><a href="#2-验证括号" class="headerlink" title="2.验证括号"></a>2.验证括号</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://leetcode.com/problems/valid-parentheses/" target="_blank" rel="noopener">Valid Parentheses</a>验证括号的有效性，这种题目有一个特点：<strong>碰到这种成对的，有顺序的</strong>我们首先可以想到用栈的先后顺序来进行求解，之前大家应该都做过一道 <em>验证某一个序列是不是栈的压出序列</em>，或者是 <em>用栈来实现四则运算</em>,这两道题分别是用到了栈的顺序和括号的成对。<br>思路：<br>1)如果是空栈就append进来<br>2)如果是左括号和右括号匹配，那就将最顶端的左括号弹出，如果不匹配，那就继续append进来。<br>判别：如果是空栈，那就是匹配的，如果不是空栈，那就肯定存在不匹配的。<br>时间复杂度：O(n),空间复杂度：O(n)</p>
<pre><code>class Solution:
    def isValid(self, s: str) -&gt; bool:
        str_len = len(s)
        #如果长度不是偶数，那么肯定不匹配
        if str_len % 2 !=0:
            return False
        else:
            stack = []
            for index in range(str_len):
                #如果是空栈，那就增加进来
                if len(stack)==0:
                    stack.append(s[index])
                #如果不匹配，那也加入到栈
                elif self.judge(stack[-1],s[index]) == False:
                    stack.append(s[index])
                #既不是空栈同时也匹配，那就弹出
                else:
                    stack.pop()
            return True if len(stack)==0 else False
    #判断左右括号是否匹配，这里有三种括号，所以用了三个if               
    def judge(self,left,right):
        if left == &#39;(&#39; and right == &#39;)&#39;:
            return True

        if left == &#39;[&#39; and right == &#39;]&#39;:
            return True

        if left == &#39;{&#39; and right == &#39;}&#39;:
            return True

        return False
</code></pre><h2 id="3-产生成对的括号"><a href="#3-产生成对的括号" class="headerlink" title="3.产生成对的括号"></a>3.产生成对的括号</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://leetcode.com/problems/generate-parentheses/" target="_blank" rel="noopener">Generate Parentheses</a>产生成对的括号,这一题虽然用括号来进行表示，但是实际上它是一类 <strong>排列组合问题</strong>。碰到这类问题，每一个位置上都可以有多种选择的，一定要想到 <strong>回溯法（Backtracking）</strong>。思考递归题时，只要明确三点就行：<strong>选择 (Options)，限制 (Restraints)，结束条件 (Termination)。</strong>即<a href="https://www.1point3acres.com/bbs/thread-172641-1-1.html" target="_blank" rel="noopener">“ORT原则”</a><br>在这道题目中：<br><strong>选择</strong>：每个位置上放左括号或者右括号。<br><strong>限制</strong>：1.左括号用完了，就不能放左括号了。2.如果出现的右括号和左括号一样多了，就不能放右括号了，否则就肯定不是正确解。<br><strong>结束条件</strong>：左括号和右括号都用完了。<br><strong>结束后的正确性</strong>：左右括号用完以后，一定是正确解。因为1. 左右括号一样多，2. 每个右括号都一定有与之配对的左括号。因此一旦结束就可以加入解集（有时也可能出现结束以后不一定是正确解的情况，这时要多一步判断）。<br><strong>递归函数传入参数</strong>：限制和结束条件中有“用完”和“一样多”字样，因此你需要知道左右括号的数目。当然你还需要知道当前局面sublist和解集res。</p>
<pre><code>class Solution(object):
    def generateParenthesis(self, n):
        result = []
        sub = &quot;&quot;
        num = n
        self.dfs(0,0,num,sub,result)
        return result
    &#39;&#39;&#39;
    left,right：左右括号的用过的数量
    num：括号的总对数，也就是左右括号的总量
    sub:储存子序列
    resutl:储存最终结果
    &#39;&#39;&#39;
    def dfs(self,left,right,num,sub,result):
        #结束条件，左右括号都大于等于num
        if left&gt;=num and right&gt;=num:
            result.append(sub)
            return
        #如果左括号小于总数，那就放小括号
        if left&lt;num:
            self.dfs(left+1,right,num,sub+&#39;(&#39;,result)
        #如果右括号小于左括号那就可以放右括号
        if right&lt;left:
            self.dfs(left,right+1,num,sub+&#39;)&#39;,result)
</code></pre><h2 id="4-最长有效括号对"><a href="#4-最长有效括号对" class="headerlink" title="4.最长有效括号对"></a>4.最长有效括号对</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://leetcode.com/problems/longest-valid-parentheses/" target="_blank" rel="noopener">Longest Valid Parentheses</a>最长有效括号对，碰到这种 <strong>最长、最短、时间最少</strong>等要特别想起 <strong>动态规划DP</strong>。在处理动归问题的时候，要考虑它的子结构和自身结构的一个关系。本题因为要找最长有效括号对，而右括号明确了位置，是可以推算左括号位置的，因此这里用<strong>一维DP</strong>：dp[i]为到i处有效括号，<br>1）如果s[i]为左括号，则dp[i]为0，因为若字符串是以左括号结束，则不可能为有效的；<br>2）若是为右括号，有两种情况：<br>    (1)其前者s[i-1]为左括号，所以dp[i]=dp[i-2]+2;<br>    (2)s[i-1]为右括号且s[i-dp[i-1]-1]为左括号，所以 dp[i] = dp[i-1] + 2 + dp[i-dp[i-1]-2]，其中i-dp[i-1]-1对应最长括号的起始点</p>
<pre><code>class Solution:
    def longestValidParentheses(self, s: str) -&gt; int:
        s_len = len(s)
        if s_len&lt;2:
            return 0
        max_len = 0
        #dp[i]表示刚好在s[i]以前（包括s[i]在内)的最长括号长度
        #如果s[i] = &#39;(&#39;, dp[i] = 0
        dp = [0 for _ in range(s_len)]
        for i in range(1,s_len):
            #当前i的对称点的索引
            pos = i-1-dp[i-1]
            if s[i] == &#39;)&#39; and s[i-1] == &#39;(&#39; and i-2&gt;0:#i-2为前前项
                 #等于前前项的最长长度+2
                dp[i] = dp[i-2]+2
            elif s[i] == &#39;)&#39; and pos &gt;= 0 and s[pos] ==&#39;(&#39;:
                 #等于上个对称点+2
                dp[i] = dp[i-1]+2
                if pos-1&gt;=0:
                     #如果对称点前还有长度，则加上那段长度
                    dp[i]+=dp[pos-1]
        return max(dp)
</code></pre><h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5.总结"></a>5.总结</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;这里总结了leetcode的三道题目，感觉跟考研一样要看题眼，什么类型要想到什么方法，哈哈！继续努力，为秋招而奋斗！</p>
]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title>pytorch函数操作Tensor</title>
    <url>/2019/05/01/DeepLearning/Pytorch/pytorch%20operation%20in%20image/</url>
    <content><![CDATA[<h2 id="1-创建torch"><a href="#1-创建torch" class="headerlink" title="1.创建torch"></a>1.创建torch</h2><h3 id="1-1直接初始化"><a href="#1-1直接初始化" class="headerlink" title="1.1直接初始化"></a>1.1直接初始化</h3><pre><code>    #直接输入数据,用小写的tensor
    torch.tensor([2.,3.2])--------tensor([1.0000, 3.2000]) 

    #大写的Tensor,输入为shape,输出为随机的初始化结果，默认为double
    torch.Tensor(2,3)--------tensor([[1.4622e+19, 4.5682e-41, 1.4622e+19],
                                     [4.5682e-41, 6.6830e+22, 2.8231e+23]])

    #empty,随机初始化,和Tensor一样，但是容易出现和上述Tensor一样值非常大
    torch.empty(2,3)--------tensor([[-2.2838e+04,  4.5897e-41,-2.2838e+04],
                                [ 4.5897e-41, -2.8084e+17,  2.7149e+17]])

    #可以设置默认的数据类型IntTensor,FloatTensor,DoubleTensor等
    torch.set_default_tensor_type(torch.FloatTensor)

    #填充初始化,第一个参数为shape,第二个参数为填充的值
    torch.full([2,3],7)--------tensor([[7., 7., 7.],
                                    [7., 7., 7.]])

    #arange初始化,给定起始值，终值以及步长。
    torch.arange(0,10,2)--------tensor([0, 2, 4, 6, 8])

    #linspace和logspace,给定起始值，终值以及生成的个数。
    torch.linspace(0,10,8)--------tensor([0.0000,1.4286,2.8571,4.2857,
                                            5.7143,7.1429,8.5714,10.0000])
    torch.logspace(0,10,8)--------tensor([1.0000e+00,2.6827e+01,
                                        7.1969e+02,1.9307e+04,5.1795e+05,
                                        1.3895e+07,3.7276e+08,1.0000e+10])

    #ones,zeros,eye等都输入shape
    torch.ones(3,3)--------tensor([[1., 1., 1.],
                                [1., 1., 1.],
                                [1., 1., 1.]])
</code></pre><h3 id="1-2随机初始化"><a href="#1-2随机初始化" class="headerlink" title="1.2随机初始化"></a>1.2随机初始化</h3><pre><code>    #rand(),随机初始化,输入shape,生成均匀的0,1分布的值。
    torch.rand(3,3)--------tensor([[0.7518, 0.7011, 0.8187],
                            [0.8910, 0.9224, 0.4854],
                            [0.9668, 0.0404, 0.9374]])

    #输入为最小值,最大值以及shape
    torch.randint(1,10,[3,3])--------tensor([[2., 4., 4.],
                                            [9., 3., 2.],
                                            [4., 6., 1.]])

    #randn输入为shape,主要N(0,1)的正态分布
    torch.randn(3,3)--------tensor([[ 0.1118, -0.4003, -0.9782],
                                    [ 0.7846,  0.7151,  0.8911],
                                    [-2.1824,  0.6975,  1.1570]])

    #randperm 输入一个整数，随机的index分布,可以用来进行随机打乱数据。
    torch.randperm(10)--------tensor([5, 4, 8, 1, 0, 2, 3, 6, 9, 7])
</code></pre><a id="more"></a>
<h2 id="2-切片操作"><a href="#2-切片操作" class="headerlink" title="2.切片操作"></a>2.切片操作</h2><p>numpy中有切片操作,那么tensor也是，其余的操作跟numpy操作基本一样，这里介绍切片的几个特殊的函数。</p>
<pre><code>    #当我们切片的时候，想要任选某几个
    a = torch.Tensor(8,3,32,32)
    b = a[[0,1],:,:,:]
    c = a.index_select(0,torch.tensor([0,1]))

    #b和c等价，但是要注意.index_select中第一个输入是维度，第二个是一个tensor,即
    #要选择的列,当然b后面的三个：：：可以简写成
    b = a[[0,1],...]

    #还有一种，比如在对象检测中，我们得到了IOU阈值,我们要获得那些大于0.5的阈值怎么#办 d,e两者等价
    maks = a.ge(0.5)
    d = a[mask]
    e = a.mask_select(mask)
</code></pre><h2 id="3-Tensor的维度变换"><a href="#3-Tensor的维度变换" class="headerlink" title="3.Tensor的维度变换"></a>3.Tensor的维度变换</h2><h3 id="3-1-view和reshape"><a href="#3-1-view和reshape" class="headerlink" title="3.1 view和reshape"></a>3.1 view和reshape</h3><pre><code>    #比如将一张图片拉长，这适用于全连接层
    a = torch.rand(8,3,32,32)
    a.view(8,3*32*32)  
    a.reshape(8,3*32*32)    
</code></pre><h3 id="3-2-squeeze和unsqueeze"><a href="#3-2-squeeze和unsqueeze" class="headerlink" title="3.2 squeeze和unsqueeze"></a>3.2 squeeze和unsqueeze</h3><p>挤压和增加维度也很常见,不管是挤压还是增加维度，都是对增加一个1维度。举个例子，当我们在卷积过程中要加入偏置b,那么这里就需要用到unsqueeze了。</p>
<pre><code>    #挤压和增加维度也很常见   
    a = torch.rand(8,3,32,32)
    a.unsqueeze(0)--------(1,8,3,32,32) 正的数是从前面插入
    a.unsqueeze(-1)--------(8,3,32,32,1)负的数是从后面插入

    b = torch.rand(3)
    f = torch.rand(8,3,32,32)
    #if use b+f first and want the b add in channel,you must
    b = b.unsuqeeze(1).unsqueeze(2).unqueeze(0)--------[1,3,1,1]        


    #如果b为[1,3,1,1]
    b.squeeze()--------[3]将所有为1的维度都消掉
    b.squeeze(0)--------[3,1,1]只消掉指定的且为1的维度
</code></pre><h3 id="3-3-Expand和repeat"><a href="#3-3-Expand和repeat" class="headerlink" title="3.3 Expand和repeat"></a>3.3 Expand和repeat</h3><pre><code>    #维度扩展，如上述b维度变为了[1,3,1,1]之后，要想和f相加，必须进行维度扩展
    #输入的是shape,且只有1--M这种形式，3---N不行
    b.expand(8,3,32,32)
    #如果b.expand(8,6,32,32)则会出现match at non-singletion diemnsion


    #repeat的应用，输入的不是shape，而是每个维度上重复的倍数。
    b.repeat(8,1,32,32)
</code></pre><h3 id="3-4-transpose和permute"><a href="#3-4-transpose和permute" class="headerlink" title="3.4 transpose和permute"></a>3.4 transpose和permute</h3><pre><code>    #转置也是比较常见的操作，比如矩阵的相乘等等
    a = torch.Tensor(8,3,32,32)
    #transpose两两维度交换
    b = a.transpose(1,2)--------(8,32,3,32)


    #permute多个维度一次性交换,这在我们图像中换通道什么的操作最多了
    c = a.permute(0,2,3,1)--------(8,32,32,3)
</code></pre><h2 id="4-Tensor的拼接和拆分"><a href="#4-Tensor的拼接和拆分" class="headerlink" title="4.Tensor的拼接和拆分"></a>4.Tensor的拼接和拆分</h2><h3 id="4-1-cat和stack"><a href="#4-1-cat和stack" class="headerlink" title="4.1 cat和stack"></a>4.1 cat和stack</h3><pre><code>    #拼接在神经网络中也比较常见,Inception块最后都是concatente
    a = torch.rand(4,32,32,32)
    b = torch.rand(4,64,32,32)
    c = torch.cat([a,b],dim = 1)--------(4,96,32,32)
    #上述过程相当于是将两个feature map concatente了

    #stack操作是增加一个新的维度来表征到底是哪一块的内容,a和b的维度必须一样
    a = torch.rand(32,8)
    b = torch.rand(32,8)
    c = torch.stack([a,b],dim = 0)--------(2,32,8)
</code></pre><h3 id="4-2-split和chunk"><a href="#4-2-split和chunk" class="headerlink" title="4.2 split和chunk"></a>4.2 split和chunk</h3><pre><code>    #split是按照长度来拆分，第一个输入的是分割的各部分长度，第二个是维度，这个可以#进行深度学习的一些channle分组操作
    a = torch.Tensor(5,3,32,32)
    aa,bb = a.split([2,3],dim = 0)----aa:(2,3,32,32) bb:(3, 3, 32, 32)

    #chunk，输入的为分为几个组以及维度，这个可以用来进行channel的分组操作
    a = torch.Tensor(8,64,32,32)
    torch.chunk(a,4,dim=1)--------tuple(4) 每一个的shape为(5,16,32,32)
</code></pre><h2 id="5-Tensor的一些基本运算"><a href="#5-Tensor的一些基本运算" class="headerlink" title="5. Tensor的一些基本运算"></a>5. Tensor的一些基本运算</h2><h3 id="5-1-add-sub-mul-div"><a href="#5-1-add-sub-mul-div" class="headerlink" title="5.1 add/sub/mul/div"></a>5.1 add/sub/mul/div</h3><pre><code>    普通的加减乘除,也可以用+-*/来代替，但是//代表整除
</code></pre><h3 id="5-2-matmul"><a href="#5-2-matmul" class="headerlink" title="5.2 matmul"></a>5.2 matmul</h3><pre><code>    #Tensor的乘法
    a = torch.Tensor(5,3,32,64)
    b = torch.Tensor(5,3,32,64)
    c = torch.matmul(a,b.permute(0,1,3,2))--------(5,3,32,32)
</code></pre><h3 id="5-3-pow-exp-log"><a href="#5-3-pow-exp-log" class="headerlink" title="5.3 pow/exp/log"></a>5.3 pow/exp/log</h3><pre><code>    #Tensor的次方，指数以及对数
    a = torch.full([2,2],3)-------[[3,3],[3,3]]
    torch.pow(a,2)-------[[9,9],[9,9]]
    torch.exp(a)--------[[20.08,20.08],[20.08,20.08]]
    torch.log(a)--------[[1.09,1.09],[1.09,1.09]]
</code></pre><h3 id="5-4-floor-ceil-trunc-frac-round"><a href="#5-4-floor-ceil-trunc-frac-round" class="headerlink" title="5.4 floor/ceil/trunc/frac/round"></a>5.4 floor/ceil/trunc/frac/round</h3><pre><code>    #Tensor的取整和四舍五入等
    a = torch.tensor([[3.14,2.86],[5.55,6.876]])
    a.floor()--------[[3,2],[5,6]]
    a.ceil()-------[[4, 3],[6, 7]]
    a.trunc()--------[[3,2],[5,6]]
    a.frac()--------[[0.14, 0.86],[0.55, 0.876]]
    a.round()--------[[3,3],[6,7]]
</code></pre><h3 id="5-5-clamp"><a href="#5-5-clamp" class="headerlink" title="5.5 clamp"></a>5.5 clamp</h3><pre><code>    #clamp是用来限制大小的，输入第一个参数为最小值，第二个参数为最大值,这个可以应
    #用到神经网络中去限制梯度的大小
    a = torch.tensor([11,9],[-2,5])
    a.clamp(0,10)--------[[10,9],[0,5]]
</code></pre><h2 id="6参考"><a href="#6参考" class="headerlink" title="6参考"></a>6参考</h2><p>当然函数还有很多，举例子是举不完的，了解完这几个参数之后对于读代码的话应该问题会少很多，但是如果遇到具体的问题的话还是建议去看一下官方的参考文档</p>
]]></content>
      <categories>
        <category>DeepingLearning</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>DenseNet</title>
    <url>/2019/04/09/DeepLearning/Classficaition/DenseNet/</url>
    <content><![CDATA[<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1.引言"></a>1.引言</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;ResNet等网络的成功应用表明,在输入层和输出层中加入短连接(skip-connect)能够更加准确有效的训练网络。基于上述启发,作者提出了DenseNet(密集连接网络)。每一层接收来自前面多层的feature map形成密集连接。DenseNet优点主要在以下几个方面:</p>
<ul>
<li>缓解梯度消失问题(同ResNet)</li>
<li><strong>加强特征传播,鼓励特征重用</strong></li>
<li>大幅度减少参数量。</li>
</ul>
<a id="more"></a>
<h2 id="2-DenseNet"><a href="#2-DenseNet" class="headerlink" title="2.DenseNet"></a>2.DenseNet</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;在介绍具体的网络之前,先来了解下DenseNet的整体结构:<br><img src="/../../../images/DeepLearning/Classfication/DenseNet/DenseNet.png" alt="DenseNet"><center>图1.DenseNet结构图</center><br>如图1所示,DenseNet由基础卷积、池化以及DenseBlock组成。不同大小的网络如DenseNet-121,169,201等差异主要就在DenseBlock上,接下来重点来介绍下DenseBlock。</p>
<h2 id="2-1-DenseBlock"><a href="#2-1-DenseBlock" class="headerlink" title="2.1 DenseBlock"></a>2.1 DenseBlock</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;DenseBlock的结构如下图所示:<br><img src="/../../../images/DeepLearning/Classfication/DenseNet/DenseBlock.png" alt="DenseBlock"><center>图2.DenseBlock</center></p>
<ul>
<li><strong>Dense Connect</strong><br>&nbsp;&nbsp;&nbsp;&nbsp;DenseNet和ResNet两个网络有一个共同点就是连接(connect),但是两者的连接方式有很大的差异。ResNet采用的是加法(summation),DenseNet采用的是拼接(concatenate)。假设经过一系列的卷积和非线性作用的函数为$H_l$,各个层分别表示为$x_0,x_1,…x_{l-1},x_{l}$:</li>
</ul>
<p>ResNet中:</p>
<script type="math/tex; mode=display">x_l = H_l(x_{l-1})+x_l</script><p>DenseNet中:</p>
<script type="math/tex; mode=display">x_l = H_l([x_0,x_1,...x_{l-1}])</script><ul>
<li><strong>Growth rate</strong></li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;讲完上述的连接之后,接下来还有一个问题,假设每一层产生k个feature map, 那么最后一层得到的特征图就是$k*l$个。原论文中因为定义了第一层是$k_0$,公式不一样,实际上同理。作者将这个k值定为一个参数,试验了k=12,24,40等,说明了小k也有很好的效果,小k也大大的减少了参数量。</p>
<ul>
<li>$H_l$<strong>的定义</strong></li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;$H_l$是由卷积(Conv)、激活函数(ReLU)和批归一化(BN)组成。具体形式:</p>
<script type="math/tex; mode=display">BN-ReLU-Conv(1×1)-BN-ReLU-Conv(3×3)</script><p>其中1x1卷积用于降维。</p>
<h2 id="2-2-Transition-Layer"><a href="#2-2-Transition-Layer" class="headerlink" title="2.2 Transition Layer"></a>2.2 Transition Layer</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;从图1的总体结构中了解,Dense Block之间的采用卷积和池化连接,将其称之为Transition Layer:</p>
<p><img src="/../../../images/DeepLearning/Classfication/DenseNet/Transition_Layer.png" alt="称之为Transition_Layer:"></p>
<p><center>图3.Transition Layer</center><br>&nbsp;&nbsp;&nbsp;&nbsp;这一层的结构没什么特殊的改进,主要从上述Dense连接方式可知Dense Block的输出特征图很大,所以在这个层中用了1x1卷积核来进行降维,利用一个参数$\theta$来表征降维的幅度,作者默认用0.5。</p>
<h2 id="3-网络的理论解释"><a href="#3-网络的理论解释" class="headerlink" title="3.网络的理论解释"></a>3.网络的理论解释</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;深度学习虽然是一个黑箱,但是渐渐的大家根据网络的效果总结出一些准则来指导网络的设计以及解释。主要有以下3种(这几种解释,其本质表达的是一个意思):</p>
<ol>
<li><p><strong>多层语义信息</strong>：这种解释主要应用在语义分割中。在网络前馈过程中,每层产生的feature map都包含了语义信息,但是在前进过程中不管是激活函数还是池化操作等,都有可能会丢失重要的语义信息。因此,ResNet和DenseNet等都通过connection的方式尽可能的使得重要语义信息保留和传递。</p>
</li>
<li><p><strong>感受野</strong>：这个概念出现在检测任务中比较多,它与生理视觉相对应。人眼对在一定范围内具有感受野,太远的东西就看不到了。在神经网络的设计中,不同的网络结构具有不同的感受野,具体参照<a href="https://jaryhuang.github.io/2019/03/12/Convlution/#more" target="_blank" rel="noopener">Convolution and Receptive Field</a>。DenseNet等网络中,通过connection feature map,相当于叠加了不同大小感受野,就好像看一张图片咱们放大看了又放小看,做出综合评价,这样多种尺度会看的更加准。</p>
</li>
<li><p><strong>数据的冗余</strong>：这个是从信息论的角度出发来解释。一张图片,存在很多的信息(有我们的关注信息和无关信息)。要做到正确的分类就要抑制掉干扰量,保留关注的一些特征。因此,对于冗余的信息应该抑制,对于重点特征的要进行传递。那么connection为什么有效呢？因为抑制(卷积+激活函数),保留(connect),两者都做到了。</p>
</li>
</ol>
<h2 id="4-参考"><a href="#4-参考" class="headerlink" title="4.参考"></a>4.参考</h2><p>1.<a href="https://arxiv.org/pdf/1608.06993.pdf" target="_blank" rel="noopener">DenseNet</a><br>2.<a href="https://mp.weixin.qq.com/s/2cxEIqGhhQ-AgrEC1-a3gA" target="_blank" rel="noopener">对ResNet本质的一些思考</a> </p>
]]></content>
      <categories>
        <category>DeepingLearning</category>
      </categories>
      <tags>
        <tag>Classfication</tag>
      </tags>
  </entry>
  <entry>
    <title>Learning Deep Features for Discriminative Localization</title>
    <url>/2019/04/02/DeepLearning/Visualization/Class%20Activation%20Mapping/</url>
    <content><![CDATA[<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1.引言"></a>1.引言</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;这篇文章是2016年发表在CVPR中的,它阐述了在神经网络在图像分类(图像级标签上训练)具有显著的定位判别能力,这个定位到的就是网络学习关注的重点。我们利用此技术可以本地可视化跟踪看看神经网络在分类过程中对于每个类别的关注点在哪里,这能给在之后的网络优化过程中提供一点点思考。<br><a id="more"></a></p>
<h2 id="2-Class-Activation-Mapping-CAM"><a href="#2-Class-Activation-Mapping-CAM" class="headerlink" title="2.Class Activation Mapping(CAM)"></a>2.Class Activation Mapping(CAM)</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;产生的CAM图主要是通过global average pooling(GAP)作用。产生CAM图的总体流程如论文中的图1所示：</p>
<p><img src="/../../../images/DeepLearning/Visualization/CAM/cam.png" alt="CAM"></p>
<center>图1.CAM产生示意图</center>

<p>&nbsp;&nbsp;&nbsp;&nbsp;从上述可看出,网络首先通过卷积层特征进行提取,通过最后一个卷积层之后,提取的feature map经过一个GAP层变为单维特征,再经过一个全连接层后输出的值与我们类别数相同,最后再经过softmax转化即可。那么产生CAM,关注的重点就在最后一层卷积的feature map 到全连接层这里,如图2所示：</p>
<p><img src="/../../../images/DeepLearning/Visualization/CAM/cam细节图.png" alt="CAM"></p>
<center>图2.CAM产生过程图</center>

<p>$f_n$代表的是GAP或者Global Max Pooling(GMP),最后一层卷积出来的feature map(总共k个chanel)经过$f_n$之后为:</p>
<script type="math/tex; mode=display">Fn = \sum_{x,y}f_n(x,y)                           (1)</script><script type="math/tex; mode=display">S_c = \sum_{i=0}^n w^c_i\cdot Fi                       (2)</script><p>通过(1)和(2)可得</p>
<script type="math/tex; mode=display">S_c = \sum_{i=0}^n w^c_i \cdot \sum_{x,y}f_n(x,y) = \sum_{x,y}\sum_{i=0}^n w^c_i \cdot f_n(x,y) = \sum_{x,y}M_c(x,y)</script><p>从上述推理可以看出,输出的结果$S_c$是在feature map上的所有点的$M_c(x,y)$累加而成,因此他也表征了在feature map上每个点(x,y)的重要性。那么这种feature map重要性特征图已经得到,原图是通过卷积核下采样变成现在的feature map的,因此我们进行上采样就能恢复出原图上的一些重点关注区域。因此这就完成了从原图上可视化哪些重点区域是影响后面的第c类的分类的。</p>
<h2 id="3-CAM代码实现"><a href="#3-CAM代码实现" class="headerlink" title="3.CAM代码实现"></a>3.CAM代码实现</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;CAM的代码实现比较简单,输入为最后一个卷积层的特征(就是GAP之前的feature map比如为[512,8,8]),输入weight_softmax为(比如二分类为[512,2]),class_idx来控制到底取哪个类的权重。代码如下：</p>
<pre><code>def returnCAM(feature_conv, weight_softmax, class_idx):
    # generate the class activation maps upsample to 256x256
    size_upsample = (opt.pic_size, opt.pic_size)
    bz, nc, h, w = feature_conv.shape
    output_cam = []
    for idx in class_idx:
        cam = weight_softmax[idx].dot(feature_conv.reshape((nc, h*w)))
        cam = cam.reshape(h, w)
        cam = cam - np.min(cam)
        cam_img = cam / np.max(cam)
        cam_img = np.uint8(255 * cam_img)
        output_cam.append(cv2.resize(cam_img, size_upsample))
    return output_cam
</code></pre><p>&nbsp;&nbsp;&nbsp;&nbsp;首先设定上采样的图片大小(和输入图片相同),然后得到feature map的大小,接下来循环提取输入的类,计算cam,接下来通过归一化后再乘以255变为颜色,最后进行上采样,然后返回。</p>
<h2 id="4-效果"><a href="#4-效果" class="headerlink" title="4.效果"></a>4.效果</h2><div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 50%;"><img src="/../../../images/DeepLearning/Visualization/CAM/test0.jpg" width="300"></div><div class="group-picture-column" style="width: 50%;"><img src="/../../../images/DeepLearning/Visualization/CAM/CAM0.jpg" width="300"></div></div><div class="group-picture-row"><div class="group-picture-column" style="width: 50%;"><img src="/../../../images/DeepLearning/Visualization/CAM/test5.jpg" width="300"></div><div class="group-picture-column" style="width: 50%;"><img src="/../../../images/DeepLearning/Visualization/CAM/CAM5.jpg" width="300"></div></div></div></div>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>1.<a href="https://arxiv.org/pdf/1512.04150.pdf" target="_blank" rel="noopener">Learning Deep Features for Discriminative Localization</a></p>
]]></content>
      <categories>
        <category>DeepingLearning</category>
      </categories>
      <tags>
        <tag>Visualization</tag>
      </tags>
  </entry>
  <entry>
    <title>Activation Function</title>
    <url>/2019/04/01/DeepLearning/Basic/Activation/</url>
    <content><![CDATA[<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1.引言"></a>1.引言</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;深度学习是一种表示学习方法,其强大的表征能力得益于非线性函数。如果各层都采用线性函数来激活，那么最终只能进行线性拟合，就会散失其强大的表征能力。近年来激活函数有了很大的发展，特别是新型的ReLU克服了梯度消失问题，使得模型直接训练成为了可能。以下将对比较常见的激活函数及它们的变体进行一个简单的介绍。<br><a id="more"></a></p>
<h2 id="2-Activation思维导图"><a href="#2-Activation思维导图" class="headerlink" title="2.Activation思维导图"></a>2.Activation思维导图</h2><p><img src="/../../../images/DeepLearning/Basic/activation_function/激活函数.png" alt="Activation思维导图"></p>
<center>图1.激活函数思维导图</center>

<h2 id="3-激活函数及其倒数"><a href="#3-激活函数及其倒数" class="headerlink" title="3.激活函数及其倒数"></a>3.激活函数及其倒数</h2><h3 id="3-1-Sigmoid"><a href="#3-1-Sigmoid" class="headerlink" title="3.1 Sigmoid"></a>3.1 Sigmoid</h3><p><img src="/../../../images/DeepLearning/Basic/activation_function/sigmoid.jpg" alt="Sigmoid"></p>
<center>图2.Sigmoid函数及其倒数</center>

<h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><p>⑴平滑，便于求导<br>⑵压缩数据，使得数据幅值不会太大</p>
<h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><p>①出现梯度消失(gradient  vanishing)<br>②非0均值(zero-centered)<br>③幂计算相对比较耗时</p>
<h3 id="3-2-Tanh"><a href="#3-2-Tanh" class="headerlink" title="3.2 Tanh"></a>3.2 Tanh</h3><p><img src="/../../../images/DeepLearning/Basic/activation_function/tanh.jpg" alt="Tanh"></p>
<center>图3.Tanh函数及其倒数</center>

<h3 id="优点-1"><a href="#优点-1" class="headerlink" title="优点"></a>优点</h3><ul>
<li>值的范围是[-1,1],其解决了Sigmoid的非0均值问题,但是其他的问题依然存在</li>
</ul>
<h3 id="3-3-ReLU"><a href="#3-3-ReLU" class="headerlink" title="3.3 ReLU"></a>3.3 ReLU</h3><p><img src="/../../../images/DeepLearning/Basic/activation_function/relu.jpg" alt="ReLU"></p>
<center>图4.ReLU函数及其倒数</center>

<h3 id="优点-2"><a href="#优点-2" class="headerlink" title="优点"></a>优点</h3><p>⑴梯度不会饱和，缓解了梯度消失问题<br>⑵计算复杂度第，导数就是0和1</p>
<h3 id="缺点-1"><a href="#缺点-1" class="headerlink" title="缺点"></a>缺点</h3><p>①非0均值(zero-centered)<br>②神经元坏死(Dead  ReLU  Problem)，既是优点也是缺点<br>③不会做数据幅值压缩</p>
<h3 id="3-4-Leakly-ReLU"><a href="#3-4-Leakly-ReLU" class="headerlink" title="3.4 Leakly ReLU"></a>3.4 Leakly ReLU</h3><p><img src="/../../../images/DeepLearning/Basic/activation_function/leakly_relu.jpg" alt="Leakly ReLU"></p>
<center>图5.Leakly ReLU函数及其倒数</center>
- 解决神经元坏死问题，但是效果不一定好。

### 3.5 ELU

![ELU](/../../../images/DeepLearning/Basic/activation_function/elu.jpg)
<center>图6.ELU函数及其倒数</center>

<h3 id="优点-3"><a href="#优点-3" class="headerlink" title="优点"></a>优点</h3><ul>
<li>右侧融合了RELU的优点</li>
<li>左侧使得其加入了sigmoid的优点,负半段为a*(exp(x)-1)</li>
<li>输出均值接近于0，收敛速度更快</li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>1.<a href="https://zhuanlan.zhihu.com/p/22142013" target="_blank" rel="noopener">深度学习中的激活函数导引</a><br>2.<a href="https://blog.csdn.net/NOT_GUY/article/details/78749509" target="_blank" rel="noopener">深度学习中激活函数的优缺点</a></p>
]]></content>
      <categories>
        <category>DeepingLearning</category>
      </categories>
      <tags>
        <tag>BasicConcept</tag>
      </tags>
  </entry>
  <entry>
    <title>NMS,Soft NMS and Softer NMS</title>
    <url>/2019/03/31/DeepLearning/ObjectDetection/NMS/</url>
    <content><![CDATA[<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1.引言"></a>1.引言</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;在目标检测任务中,网络会产生许许多多的定位框,很多定位框确定的都是同一对象。为了减少一些重复检测的框,在检测任务中会引入非极大值抑制(NMS)算法,主要统计与指定类别最高置信度box的重叠度(IoU),高于阈值,则置信度置为0,低于阈值,则不变。这是传统的NMS算法,但是存在一定的缺点,后续的研究者针对缺点对NMS进行了改进,提出了Soft NMS和Softer NMS。<br><a id="more"></a></p>
<h2 id="2-NMS系列思维导图"><a href="#2-NMS系列思维导图" class="headerlink" title="2.NMS系列思维导图"></a>2.NMS系列思维导图</h2><p><img src="/../../../images/DeepLearning/ObjectDetection/NMS/NMS系列.png" alt="NMS系列"></p>
<h3 id="2-1-Soft-NMS"><a href="#2-1-Soft-NMS" class="headerlink" title="2.1 Soft NMS"></a>2.1 Soft NMS</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;对于NMS,在引言中已经介绍了,这里就不着重说明,我们直接对于soft nms进行介绍,如图所示：</p>
<p><img src="/../../../images/DeepLearning/ObjectDetection/NMS/soft_nms.png" alt="Soft NMS">&nbsp;&nbsp;&nbsp;&nbsp;从图中可以看出,soft nms主要改进是对IOU大于阈值的对象的处理上。其引入了$s_i:=s_i\cdot f(iou(M,b_i))$使得对于IOU大于阈值的框不会直接删除,而是将其置信度按照f函数规则进行减小。这个的好处就在于在未来这个框还是有可能保留的。它的一个很大的作用在思维导图中也已经提到了,合理的设置阈值可以保留重叠的对象。<br>这里的f是一个随着IOU递减的函数,可以是<script type="math/tex">s_i=\left\{
\begin{array}{rcl}
s_i      &      & {iou(M,b_i)<N_t}\\
s_i\cdot (1-iou(M,b_i))     &      & {iou(M,b_i)\geq N_t}\\
\end{array} \right.</script>或者</p>
<script type="math/tex; mode=display">s_i:=s_i\cdot e^{-\frac{iou(M,b_i)^2}{\sigma}}</script><h3 id="2-2-Softer-NMS"><a href="#2-2-Softer-NMS" class="headerlink" title="2.2 Softer NMS"></a>2.2 Softer NMS</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;Softer NMS解决的是定位框不准的问题。其引入了一个置信度参数$\sigma$对于每一个预测框都有${\sigma_{x1},\sigma_{y1},\sigma_{x2},\sigma_{y2}}$,分别代表的是四个点的置信度。$x_e$是预测的box分布,那么我们利用一个高斯分布来衡量x与$x_e$之间的置信度<script type="math/tex">P_{\theta}(x) =\frac{1}{2\pi \sigma^2}e^{-\frac{(x-x_e)^2}{2\sigma^2}}</script>,$\sigma$的大小表征了这个假设的不确定度,如果$\sigma$无穷小,那么高斯分布就是一条尖脉冲,代表很确定。$x_g$是真实标注的box的分布<script type="math/tex">P_D(x) = \delta(x-x_g)</script>这是一个$\sigma$非常小的高斯分布,几乎是一条直线。</p>
<p><img src="/../../../images/DeepLearning/ObjectDetection/NMS/confidence.png" alt="Confidence">&nbsp;&nbsp;&nbsp;&nbsp;我们的目的是想让预测的高斯分布和真实标注的分布相同,因此这里利用了KLLoss来对两个分布进行衡量。而$\sigma$越大,代表分布越不同,那么这个框越不准。因此在后续对框进行处理的时$\sigma$越大的占的比重越小:</p>
<script type="math/tex; mode=display">X1_i:=\frac{\sum_j{\frac{X1_j}{\sigma^2_{x1,j}}}}{\sum_j{\frac{1}{\sigma^2_{x1,j}}}}</script><script type="math/tex; mode=display">subject\quad to\quad IOU(x1_j,x1_i) > N_t</script><p>完整的算法如下图所示：</p>
<p><img src="/../../../images/DeepLearning/ObjectDetection/NMS/softer_nms.png" alt="Softer NMS"></p>
<h2 id="3参考资料"><a href="#3参考资料" class="headerlink" title="3参考资料"></a>3参考资料</h2><p>1.<a href="https://www.merl.com/publications/docs/TR2016-144.pdf" target="_blank" rel="noopener">R-CNN</a><br>2.<a href="https://arxiv.org/pdf/1704.04503.pdf" target="_blank" rel="noopener">Improving Object Detection With One Line of Code</a><br>3.<a href="https://arxiv.org/pdf/1809.08545.pdf" target="_blank" rel="noopener">Softer-NMS: Rethinking Bounding Box Regression for Accurate Object Detection</a></p>
]]></content>
      <categories>
        <category>DeepingLearning</category>
      </categories>
      <tags>
        <tag>ObjectDetection</tag>
      </tags>
  </entry>
  <entry>
    <title>Faster RCNN</title>
    <url>/2019/03/15/DeepLearning/ObjectDetection/Faster%20RCNN/</url>
    <content><![CDATA[<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1.引言"></a>1.引言</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;Fast RCNN改进了RCNN,使得网络有了很大的提升，然而他还存在着一个问题是它候选框的产生方式还是采用的常规SS等方式。那是否可以让网络来学习到这些候选框呢？基于这个思路，Faster RCNN应运而生，其最重要的是引入了RPN网络(全卷积网络)来产生候选框。同时，RPN网络中引入的Anchor，不仅提高了检测的精度，对后续一些检测网络的影响非常的大。本专题通过Faster RCNN的网络、训练细节和实验验证三个方面来进行介绍。<br><a id="more"></a></p>
<h2 id="2-Faster-RCNN思维导图"><a href="#2-Faster-RCNN思维导图" class="headerlink" title="2.Faster RCNN思维导图"></a>2.Faster RCNN思维导图</h2><p><img src="/../../../images/DeepLearning/ObjectDetection/Faster_RCNN/Faster-RCNN.png" alt="Fast RCNN"></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>1.<a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf" target="_blank" rel="noopener">Fast R-CNN</a><br>2.<a href="https://www.merl.com/publications/docs/TR2016-144.pdf" target="_blank" rel="noopener">R-CNN</a><br>3.<a href="https://arxiv.org/pdf/1406.4729.pdf" target="_blank" rel="noopener">SPP-Net</a></p>
]]></content>
      <categories>
        <category>DeepingLearning</category>
      </categories>
      <tags>
        <tag>ObjectDetection</tag>
      </tags>
  </entry>
  <entry>
    <title>Convolution and Receptive Field</title>
    <url>/2019/03/12/DeepLearning/Basic/Convlution/</url>
    <content><![CDATA[<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1.引言"></a>1.引言</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;神经网络在图像领域发展得益于卷积神经网络的提出。卷积的局部连接、全局共享以及形变不变性使得网络在表征能力和参数数量上有一个很好的平衡。伴随着卷积神经网络的改进，在标准卷积的基础上，许多人做不同的方面做了改进，其中主要在两个方面：通道和卷积核。同时,由于各种各样卷积的提出，那么对于卷积之后的feature map大小的计算等也是一个要点。在本文的最后还特别介绍了重要概念感受野、有效是感受野的以及他们的计算。<br><a id="more"></a></p>
<h2 id="2-Conv思维导图"><a href="#2-Conv思维导图" class="headerlink" title="2.Conv思维导图"></a>2.Conv思维导图</h2><p><img src="/../../../images/DeepLearning/Basic/Convolution/Convolution.png" alt="Conv思维导图"></p>
<center>图1.卷积思维导图</center>

<h2 id="3-feature-map的计算"><a href="#3-feature-map的计算" class="headerlink" title="3.feature map的计算"></a>3.feature map的计算</h2><h3 id="3-1-标准卷积"><a href="#3-1-标准卷积" class="headerlink" title="3.1 标准卷积"></a>3.1 标准卷积</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;卷积有多个参数，主要的是：核的大小(kernel_size),填充(padding)和step(步长)。那么这三个参数是如何影响其feature map的。这里有计算公式:<script type="math/tex">f_{out} =\frac{(f_{in}+2*padding-kernel_{size})}{stride}+1</script>。以32x32输入的图片为例，经过conv(kernel_size = 5,stride = 1,padding=0)之后为28x28。<strong>这里多说一句话，池化层的feature map计算和卷积层一样。</strong></p>
<h3 id="3-2空洞卷积"><a href="#3-2空洞卷积" class="headerlink" title="3.2空洞卷积"></a>3.2空洞卷积</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;前面第二点中的思维导图谈到了空洞卷积，其就是在卷积核中填充(d-1)个空格。他的输出feature map计算方式和标准卷积相同，只是卷积kernel_size变为$kernel_{size}+(kernel_{size}-1) \cdot (d-1)$。例如，原本为3x3卷积的话，如果d=2,那么就变为5x5了。</p>
<h3 id="3-3转置卷积"><a href="#3-3转置卷积" class="headerlink" title="3.3转置卷积"></a>3.3转置卷积</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;转置卷积就是将feature map从小变为大，他的推导按照标准卷积的方式反向计算即可。比如输入为2x2，卷积核为4x4，步长为3.那么将上述公式带入标准卷积公式，求$f_{in}$,所以容易计算出结果为7x7。</p>
<h2 id="4-感受野"><a href="#4-感受野" class="headerlink" title="4.感受野"></a>4.感受野</h2><h3 id="4-1-理论感受野-RF-的计算"><a href="#4-1-理论感受野-RF-的计算" class="headerlink" title="4.1 理论感受野(RF)的计算"></a>4.1 理论感受野(RF)的计算</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;感受野是深度学习中很重要的概念,<strong>是指某一层feature map中的某个位置，由前面某一层固定区域计算得出的，那这个区域大小就是这个位置的感受野，常用的是看这一层在输入层的感受野。</strong>感受野的大小在目标检测中对于Anchor具有重要的指导意义。如何进行感受野的计算是重中之重。这里我们可以根据公式进行计算：</p>
<script type="math/tex; mode=display">L_{k}=L_{k-1}+(f_{k}-1) \cdot \prod_{i=1}^{k-1}{S_i}</script><p>&nbsp;&nbsp;&nbsp;&nbsp;公式表述了$L_k$的感受野与$L_{k-1}$层的关系，$f_{k}$为计算层的卷积核(或者池化操作)的$kernel_{size}$，$S_i$为每一层的stride。<br><strong>这里有几项通用的规则：</strong></p>
<blockquote>
<p>1.按照上述公式计算的时候,第一层RF=kernel_size。<br>2.经过多分支路径时,按照感受野最大之路计算,这个可用于Inception等网络的理论感受野计算。<br>3.shortcut不会影响理论感受野。<br>4.RELU，BN和Dropout等操作不会影响理论感受野的范围。</p>
</blockquote>
<p>国外的研究者开发了这个感受野计算器的软件，大家可以根据上述的理论,通过自己手动计算来进行对比验证。<a href="https://fomoro.com/projects/project/receptive-field-calculator" target="_blank" rel="noopener">Receptive Field Calculator</a></p>
<h3 id="4-2-有效感受野-ERF"><a href="#4-2-有效感受野-ERF" class="headerlink" title="4.2 有效感受野(ERF)"></a>4.2 有效感受野(ERF)</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;2017年的文章<a href="https://arxiv.org/pdf/1701.04128.pdf" target="_blank" rel="noopener">Understanding the Effective Receptive Field in Deep Convolutional Neural Networks</a>中提出了有效感受野这个概念,它比<strong>理论感受野</strong>要小的多。作者主要从文中提到了以下几个点：</p>
<p><img src="/../../../images/DeepLearning/Basic/Convolution/ERF_Gaussian.png" alt="ERF_Gaussian"><center>图2.ERF的高斯分布</center>1.图2白色部分是有效感受野，而理论感受野分别是11,21,41,81。从上述可以得出,有效感受野的<strong>是高斯分布(Gaussian)</strong>，具体理论的计算可以参考文献。</p>
<p><img src="/../../../images/DeepLearning/Basic/Convolution/ERF_Nonlinear.png" alt="ERF_Nonlinear"><center>图3.非线性层对ERF的影响</center>2.<strong>不同的非线性函数的感受野也略有不同</strong>。图3中Tanh和Sigmoid非常的相似，但是RELU略有不同，感受野范围内存在一些不激活的区域(根据RELU的公式很容易理解)，使得整个感受野存在一些非高斯连续。我相信如果换成Leakly RELU，相信这三者的ERF形状会非常的相似。</p>
<p><img src="/../../../images/DeepLearning/Basic/Convolution/ERF_Subsample_Dilation.png" alt="ERF_Subsample_Dilation"><center>图4.增大ERF</center>3.<strong>图4展示了Subsample和Dilation对感受野的影响，两者都能增加ERF。</strong></p>
<p><img src="/../../../images/DeepLearning/Basic/Convolution/ERF_size.png" alt="ERF_size"><center>图5.ERF的大小和层数以及理论感受野的关系</center>4.图5中左图展示了ERF的尺寸大小和层数的关系，右图展示了ERF和RF的比率随着层数的关系。这里用的是全卷积网络，没有subsample。因此这个可以作为一个理论参考，而在实际使用中会考虑subsample,所以真实的ERF会介于上述曲线拟合结果和理论的RF。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>1.<a href="https://zhuanlan.zhihu.com/p/28749411" target="_blank" rel="noopener">变形卷积核、可分离卷积？卷积神经网络中十大拍案叫绝的操作。</a><br>2.<a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener">AlexNet</a><br>3.<a href="https://arxiv.org/pdf/1610.02357.pdf" target="_blank" rel="noopener">Xception</a><br>4.<a href="https://arxiv.org/pdf/1707.01083.pdf" target="_blank" rel="noopener">ShuffleNet</a><br>5.<a href="https://arxiv.org/pdf/1511.07122.pdf" target="_blank" rel="noopener">Dilated Convolution</a><br>6.<a href="https://arxiv.org/pdf/1703.06211.pdf" target="_blank" rel="noopener">Deformable Convolution</a><br>7.<a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Noh_Learning_Deconvolution_Network_ICCV_2015_paper.pdf" target="_blank" rel="noopener">Deconvolution(转置卷积)</a><br>8.<a href="https://zhuanlan.zhihu.com/p/44106492" target="_blank" rel="noopener">卷积神经网络的感受野</a><br>9.<a href="https://fomoro.com/projects/project/receptive-field-calculator" target="_blank" rel="noopener">Receptive Field Calculator</a><br>10.<a href="https://arxiv.org/pdf/1701.04128.pdf" target="_blank" rel="noopener">Understanding the Effective Receptive Field in Deep Convolutional Neural Networks</a></p>
]]></content>
      <categories>
        <category>DeepingLearning</category>
      </categories>
      <tags>
        <tag>BasicConcept</tag>
      </tags>
  </entry>
  <entry>
    <title>Fast RCNN</title>
    <url>/2019/03/09/DeepLearning/ObjectDetection/Fast%20RCNN/</url>
    <content><![CDATA[<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1.引言"></a>1.引言</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;在神经网络的应用中，除了分类之外，还有一项重要的应用就是对象检测。其不仅要在一个图中找到鉴别出多个物体，同时还要定位中物体的位置。因此这是一项很难的任务。2013年RCNN的在对象检测中的应用，推动了神经网络在这一领域的发展。本文介绍是Fast RCNN,包括四块内容：RCNN存在的问题，Fast RCNN的创新点，Fast RCNN的网络结构和训练方式，以及论文中的试验验证。<br><a id="more"></a></p>
<h2 id="2-Fast-RCNN思维导图"><a href="#2-Fast-RCNN思维导图" class="headerlink" title="2.Fast RCNN思维导图"></a>2.Fast RCNN思维导图</h2><p><img src="/../../../images/DeepLearning/ObjectDetection/Fast_RCNN/Fast-RCNN.png" alt="Fast RCNN"></p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p>1.<a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf" target="_blank" rel="noopener">Fast R-CNN</a><br>2.<a href="https://www.merl.com/publications/docs/TR2016-144.pdf" target="_blank" rel="noopener">R-CNN</a><br>3.<a href="https://arxiv.org/pdf/1406.4729.pdf" target="_blank" rel="noopener">SPP-Net</a></p>
]]></content>
      <categories>
        <category>DeepingLearning</category>
      </categories>
      <tags>
        <tag>ObjectDetection</tag>
      </tags>
  </entry>
  <entry>
    <title>Loss</title>
    <url>/2019/03/05/DeepLearning/Basic/Loss/</url>
    <content><![CDATA[<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1.引言"></a>1.引言</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;神经网络的学习目标是使得损失函数最小化,不同的损失函数有不同的效果。对于损失函数的整理，可以是通过学习任务来划分：分类和回归。也可以根据应用的场景来划分。由于损失函数的分支非常的多，对于不同的学习任务其都会有不同的改进。因此本文主要对pytorch中主要应用的几个损失函数进行整理:L1,L2,SmoothL2,MSE,CrossEntropy,KLDivLoss,NLLLoss,DistanceLoss和CenterLoss。<br><a id="more"></a></p>
<h2 id="2-Loss思维导图"><a href="#2-Loss思维导图" class="headerlink" title="2.Loss思维导图"></a>2.Loss思维导图</h2><p><img src="/../../../images/DeepLearning/Basic/Loss/pytorch损失函数.png" alt="Normalization思维导图"></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>1.<a href="https://pytorch.org/docs/stable/nn.html?highlight=loss#torch.nn.functional.cosine_embedding_loss" target="_blank" rel="noopener">pytorch的损失函数官方文档</a><br>2.<a href="https://ydwen.github.io/papers/WenECCV16.pdf" target="_blank" rel="noopener">Center Loss</a></p>
]]></content>
      <categories>
        <category>DeepingLearning</category>
      </categories>
      <tags>
        <tag>BasicConcept</tag>
      </tags>
  </entry>
  <entry>
    <title>Optimization</title>
    <url>/2019/03/05/DeepLearning/Basic/Optimization/</url>
    <content><![CDATA[<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1.引言"></a>1.引言</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;神经网络的学习目标是使得损失函数最小化。数据+网络+损失函数构成了损失平面，如何找到损失平面的最小值(或者局部最优值)是我们的重点。因此许许多多的研究者重点都在研究如何优化。因此本文主要针对深度学习中一些优化方法做了一个简单的介绍，基本是基于SGD进行的改进：前进方向和学习率大小两个方面。<br><a id="more"></a></p>
<h2 id="2-Optimization思维导图"><a href="#2-Optimization思维导图" class="headerlink" title="2.Optimization思维导图"></a>2.Optimization思维导图</h2><p><img src="/../../../images/DeepLearning/Basic/Optimization/Optimization.png" alt="Optimization思维导图"></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>1.<a href="https://blog.csdn.net/u010089444/article/details/76725843" target="_blank" rel="noopener">优化方法总结：SGD，Momentum，AdaGrad，RMSProp，Adam</a><br>2.<a href="https://blog.csdn.net/qsczse943062710/article/details/76763739" target="_blank" rel="noopener">最全的机器学习中的优化算法介绍</a><br>3.<a href="https://blog.csdn.net/chenchunyue11/article/details/51290638" target="_blank" rel="noopener">机器学习常见的优化算法</a></p>
]]></content>
      <categories>
        <category>DeepingLearning</category>
      </categories>
      <tags>
        <tag>BasicConcept</tag>
      </tags>
  </entry>
  <entry>
    <title>Normalization</title>
    <url>/2019/03/05/DeepLearning/Basic/Normalization/</url>
    <content><![CDATA[<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1.引言"></a>1.引言</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;神经网络的归一化层目前比较流行的有Batch Normalization(2015)、Layer Normalization(2016)、Instance Normalization(2017)、Group Normalization(2018)、Switchable Normalization(2018)<br><a id="more"></a></p>
<h2 id="2-Normalization思维导图"><a href="#2-Normalization思维导图" class="headerlink" title="2.Normalization思维导图"></a>2.Normalization思维导图</h2><p><img src="/../../../images/DeepLearning/Basic/Normalizaiton/Normalization.png" alt="Normalization思维导图"></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>1.<a href="https://blog.csdn.net/liuxiao214/article/details/81037416" target="_blank" rel="noopener">BatchNormalization、LayerNormalization、InstanceNorm、GroupNorm、SwitchableNorm总结</a><br>2.<a href="https://arxiv.org/pdf/1502.03167.pdf" target="_blank" rel="noopener">Batch Normalization</a><br>3.<a href="https://arxiv.org/pdf/1607.06450v1.pdf" target="_blank" rel="noopener">Layer Normalizaiton</a><br>4.<a href="https://arxiv.org/pdf/1607.08022.pdf" target="_blank" rel="noopener">Instance Normalizaiton</a><br>5.<a href="https://arxiv.org/pdf/1803.08494.pdf" target="_blank" rel="noopener">Group Normalizaiton</a><br>6.<a href="https://arxiv.org/pdf/1806.10779.pdf" target="_blank" rel="noopener">Switchable Normalizaiton</a></p>
]]></content>
      <categories>
        <category>DeepingLearning</category>
      </categories>
      <tags>
        <tag>BasicConcept</tag>
      </tags>
  </entry>
  <entry>
    <title>Bag of Tricks for Image Classfication 论文阅读笔记</title>
    <url>/2018/12/22/DeepLearning/Classficaition/Bag%20of%20Trick%20for%20Image%20Classification/</url>
    <content><![CDATA[<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1.引言"></a>1.引言</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;图像分类的网络结构从AlexNet到NASNet已经有了很大的改进。最近的许多研究在图像分类领域主要是在训练的进程上修改,例如 <strong>数据增强(data augmentations)</strong>和 <strong>优化策略(Optimization methods)</strong>。<strong><em>但是在每篇论文里面这些细节都很少的提到</em></strong>,只有通过读源码才能理解。作者基于这个原因,写下了这篇文章,通过数据增强——&gt;高效训练——&gt;模型微改动——&gt;训练修改等几个方面的Trick来提升模型的精度,在几乎不增加模型复杂度的情况下将Resnet50的top1精度提升了4%。同时这些Trick在其他大量的模型上都是有效的,且提升后的模型作为迁移学习也提升了Object detection和Segmentation。<br><img src="/../../../images/DeepLearning/Classfication/Bag_of_Tricks_for_Classfication/model_acc.png" alt="Model Accuracy"><br><a id="more"></a><br>&nbsp;&nbsp;&nbsp;&nbsp;作者训练网络的整个过程如下图所示:<img src="../../../images/DeepLearning/Classfication/Bag of Tricks for Classfication/训练进程.png" alt="训练进程"></p>
<h2 id="2-数据处理"><a href="#2-数据处理" class="headerlink" title="2.数据处理"></a>2.数据处理</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;在数据预处理阶段,作者总共做了6部分:<br>1)随机采样一个图片并且转为32位的[0,255]之间的浮点型数据。<br>2)以宽高比为[4/3,3/4]的矩形进行随机裁剪图片,裁剪的区域大小随机8%-100%。之后将裁剪完的图片resize成[224,224]。<br>3)按0.5的概率随机水平翻转。<br>4)从[0.6,1.4]均匀分布的数据中随机缩放色调(hue)、饱和度(saturation)和亮度(brightness)。<br>5)增加一个PCA噪声,系数随机提取从普通的正太分布N(0,0.1)<br>6)归一化RGB通道通过减去[123.68,116.779,103.939]除以[58.393,57.12,57.375]。为什么是这几个值呢,是因为ImageNet那个数据集,RGB三个色彩的期望值,就是这3个值。<br>&nbsp;&nbsp;&nbsp;&nbsp;对于验证集,作者没有做其他处理,只是将图片resize成[256,256],然后从中间crop出[224,224]并归一化。对于网络,卷积层和全连接层初始化参数采用Xavier algorithm:通过从$a = \sqrt{6/(d_{in}+d_{out})}$的区间[-a,a]随机进行取值初始化卷积层。$d_{in}$和$d_{out}$是输入和输出的通道数,所有的偏差设置为0,对于BN层,$\gamma$设置为1,$\beta$设置为0。<br>batchsize设置为256,而学习率的设置为0.1,接下来每30个epoch变为1/10。<br><img src="/../../../images/DeepLearning/Classfication/Bag_of_Tricks_for_Classfication/数据增强的效果.png" alt="训练进程">从作者的试验中可以看出,数据增强对于ResNet50有了一定的提升。但是对于Inceptionv3和MobileNet可能由于训练细节的不同导致其并没有提升。</p>
<h2 id="3-高效训练"><a href="#3-高效训练" class="headerlink" title="3.高效训练"></a>3.高效训练</h2><h3 id="3-1-大批量训练"><a href="#3-1-大批量训练" class="headerlink" title="3.1 大批量训练"></a>3.1 大批量训练</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;使用大批量的训练方法可能会减慢训练的进程。对于训练相同epoch,使用较大的batch_size训练的模型比起相对较小batch_size训练的模型具有较低的验证准确度。这可能是由于较大的batch_size减少了随机噪声。作者将多种启发式解决方法进行了整理:<br><strong>1)线性放缩学习率:</strong>增加初始学习率为0.1xb/256。<br><strong>2)预热式学习:</strong>在开始训练的时候,如果学习率太大可能会导致数值不稳定,因此可采用预热式学习。比如拿前m个batch_size来进行预热,可根据这m个来设置线性递增的方式到达初始学习率。<br><strong>3)0 $\gamma$:</strong>初始化的时候,将残差块末端的BN层中的$\gamma$设置为0,所有的剩余块只返回其输入,相当于在初始训练阶段模仿只有较少层数且更容易的网络。(因为残差块只将x——&gt;x)这也提供了思路,可以先训练小网络,不够拟合的时候慢慢的将一些batch_norm的参数解开,相当于慢慢增加网络。<br><strong>4)对偏差不进行衰减:</strong>只对卷积核参数进行decay。<br>&nbsp;&nbsp;&nbsp;&nbsp;作者通过实验对这些trick进行了对比:仅通过线性缩放学习率将批量大小从256增加到1024导致前1精度降低0.9％,而其余三个启发式叠加能补偿这个精度降低的精度<img src="/../../../images/DeepLearning/Classfication/Bag_of_Tricks_for_Classfication/Heuristic.png" alt="Heuristic"></p>
<h3 id="3-2-低精度训练"><a href="#3-2-低精度训练" class="headerlink" title="3.2 低精度训练"></a>3.2 低精度训练</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;神经网络通常是在32位的浮点精度下训练,所有计算所得的参数也被储存为(FP32),但是,现在的硬件用于较低精度的数据类型可能具有增强的算术逻辑单元。例如,前Nvidia V100在FP32中提供14 TFLOPS,在FP16中提供超过100 TFLOPS。作者通过实验发现,将数值的精度从32位变为16位没有影响精度,同时在速度上提升了好几倍<img src="/../../../images/DeepLearning/Classfication/Bag_of_Tricks_for_Classfication/Low_precision_training.png" alt="低精度训练"></p>
<h2 id="4-模型修改"><a href="#4-模型修改" class="headerlink" title="4.模型修改"></a>4.模型修改</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;ResNet的结构可分解为如下几个部分:<img src="/../../../images/DeepLearning/Classfication/Bag_of_Tricks_for_Classfication/ResNet结构.png" alt="ResNet结构">在Input stem主要做的是降维操作,通常的改进方法主要是对7x7卷积的一个改进:将其用3x3卷积核替代,既有相同的感受野,又减少了计算量,如下图ResNet-C所示<img src="/../../../images/DeepLearning/Classfication/Bag_of_Tricks_for_Classfication/ResNet-C.png" alt="ResNet-C">另外有很多主要的改进在于对stage块的Down sampling进行改进。在原始的stage中用的是1x1的step为2的conv,这忽视了3/4的输入特征图。因此一个做法是将step为2的移到3x3卷积层上,如下图ResNet-B所示:<img src="/../../../images/DeepLearning/Classfication/Bag_of_Tricks_for_Classfication/ResNet-B.png" alt="ResNet-B">作者收到上述ResNet-B块改进的启发,他发现在Down sampling层的右支路上,1x1卷积块同样存在着忽视了输入特征图的3/4的数据,因此作者加了step为2的AvgPool层,如下图ResNet-D所示:<img src="/../../../images/DeepLearning/Classfication/Bag_of_Tricks_for_Classfication/ResNet-D.png" alt="ResNet-D">最终的试验结果如图所示:<img src="/../../../images/DeepLearning/Classfication/Bag_of_Tricks_for_Classfication/model_tweak.png" alt="model_tweak"><br>&nbsp;&nbsp;&nbsp;&nbsp;试验结果显示,ResNetB在下采样块中接收更多信息,验证集提升了0.5%,ResNetC的改进也增加了0.2%,ResNetD结合C比原始的ResNet50总的提升了1%左右。</p>
<h2 id="5-训练修改"><a href="#5-训练修改" class="headerlink" title="5.训练修改"></a>5.训练修改</h2><h3 id="5-1余弦学习率衰减"><a href="#5-1余弦学习率衰减" class="headerlink" title="5.1余弦学习率衰减"></a>5.1余弦学习率衰减</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;神经网络的学习和学习率的设置密切相关。前面也提到了预热式等方式初始化学习率。但是在进行训练多个epoch的时候,通常大家采用的都是step decay,比如He等人训练的时候,学习率从0.1开始每隔30个epoch降低为其1/10。衰减除了阶梯衰减之外还有余弦衰减:<script type="math/tex">\eta_t = \frac{1}{2}(1+cos(\frac{t\pi}{T}))\eta</script>:T为总的epoch,t为第i个epoch。<img src="/../../../images/DeepLearning/Classfication/Bag_of_Tricks_for_Classfication/cos_learningrate.png" alt="cos_learningrate">可以看出,余弦衰减在开始时慢慢降低学习速度,然后在中间变得几乎线性减少,并且减慢最后再次下降。与step衰减相比,余弦衰减从一开始就降低学习率,但是大小仍然很大(不像step衰减到那么低),这可能会改善训练进度。</p>
<h3 id="5-2标签平滑"><a href="#5-2标签平滑" class="headerlink" title="5.2标签平滑"></a>5.2标签平滑</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;标签平滑策略在<a href="https://jaryhuang.github.io/2018/12/17/GoogLeNet_V3%20and%20V4/" target="_blank" rel="noopener"><em>GooLeNetV3</em></a>中提及到了。每个样本的概率为:<script type="math/tex">q(i) = \frac{exp(z_i)}{\sum_{j=1}^{K}exp(z_j)}</script><br>$z_i$的公式如下图所示:<img src="/../../../images/DeepLearning/Classfication/Bag_of_Tricks_for_Classfication/zi.png" alt="zi">将i=y时的概率称之为gap:<img src="/../../../images/DeepLearning/Classfication/Bag_of_Tricks_for_Classfication/label_smooth.png" alt="label_smooth">上图中$\varepsilon$=0.1和K=1000(imagnet的类别数)。从上图4a可知,当$\varepsilon=0,gap=\infty$,随着$\varepsilon$增加,gap会随着减小。特别是当$\varepsilon=\frac{K-1}{K}$,gap为0。作者也分析了imagenet的数据集结果,图上清晰的展示了标签平滑使得整个扰动中心和理论值相同并且有更少的极端值。</p>
<h3 id="5-3知识蒸馏"><a href="#5-3知识蒸馏" class="headerlink" title="5.3知识蒸馏"></a>5.3知识蒸馏</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;知识蒸馏主要做的是利用一个老师模型来训练现在的学生模型。而老师模型一般具有预训练的基础并且有更高的精度,这样学生模型就能利用老师模型来提高自己的精度。作者用了ResNet152的模型作为老师模型来帮助训练ResNet50。损失函数改为:</p>
<script type="math/tex; mode=display">loss = \ell(p,softmax(z))+T^2\ell(softmax(r/T),softmax(z/T))</script><p>其中P是真实标签,z是学生模型预测出来的结果,r是教师模型训练出来的结果,T作为调节因子。从上述可以看出,模型不仅要拟合真值,而且还要拟合教师模型的值。两者的比重可以通过T来调节。</p>
<h3 id="5-4-混合训练"><a href="#5-4-混合训练" class="headerlink" title="5.4 混合训练"></a>5.4 混合训练</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;混合训练顾名思义就是对训练集进行混合,训练的时候随机抽取两个数据集,根据如下公式获得新的数据集:</p>
<script type="math/tex; mode=display">\hat{x} = \lambda x_i+(1-\lambda)x_j</script><script type="math/tex; mode=display">\hat{y} = \lambda y_i+(1-\lambda)y_j</script><p>$\lambda\in[0,1]且\lambda\in Beta(\alpha,\alpha)$ Beta分布的具体信息见<a href="https://www.zhihu.com/question/30269898" target="_blank" rel="noopener">如何通俗理解beta分布?</a>这样的混合方式使得结果中标签不会那么的不平滑。</p>
<h3 id="5-5-结果展示"><a href="#5-5-结果展示" class="headerlink" title="5.5 结果展示"></a>5.5 结果展示</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;<img src="/../../../images/DeepLearning/Classfication/Bag_of_Tricks_for_Classfication/Trainning_Refinements.png" alt="Trainning Refinements">如Table6所示,作者将上述第五点所述的trick分别用于ResNet50,InceptionV3,MobileNet中发现都有提升。但是知识蒸馏方法在Inception和MobileNet中没有提升,作者怀疑可能是教师模型ResNet152和InceptionV3等不是一类型的网络缘故。同时作者也在数据集MIT Places365 dataset进行了测试,Table7的结果显示了同样的trick在不同数据集上都是有一定的提升。</p>
<h2 id="6-迁移学习"><a href="#6-迁移学习" class="headerlink" title="6.迁移学习"></a>6.迁移学习</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;作者发现,新的trick训练的ResNet50模型作为迁移学习预训练模型来替换掉Faster-RCNN的Vgg-19,在分割和对象检测上分别做了测试。<img src="/../../../images/DeepLearning/Classfication/Bag_of_Tricks_for_Classfication/Transfer_Learning.png" alt="Transfer Learning">如上所示,在对象检测和分割上都有一定的提升,但是一些trick在分割上的表现一般,可能是语义分割在像素级别进行预测。 而使用标签平滑,蒸馏和混合培训的模型有利于软化标签,可能是模糊的像素级信息模糊并降低整体像素级精度。</p>
<h2 id="7-总结"><a href="#7-总结" class="headerlink" title="7.总结"></a>7.总结</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;上述在数据预处理、损失函数、学习率等方面的改进技巧始终如一的在各个模型上都表现出了提高模型精度的能力。更加好的是将这些trick叠加能更好的提高精度。因此在未来可以探索这些改变。</p>
<h2 id="8-参考"><a href="#8-参考" class="headerlink" title="8.参考"></a>8.参考</h2><ol>
<li>Bag of Tricks for Image Classification with Convolutional Neural Networks</li>
<li><a href="https://www.zhihu.com/question/30269898" target="_blank" rel="noopener">如何通俗理解beta分布?</a></li>
</ol>
]]></content>
      <categories>
        <category>DeepingLearning</category>
      </categories>
      <tags>
        <tag>Classfication</tag>
      </tags>
  </entry>
  <entry>
    <title>GoogLeNet_V3 and V4</title>
    <url>/2018/12/17/DeepLearning/Classficaition/GoogLeNet_V3%20and%20V4/</url>
    <content><![CDATA[<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1.引言"></a>1.引言</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;GooLeNetV3和GooLeNetV4主要是在<a href="https://jaryhuang.github.io/2018/10/22/GoogLeNet_V1/" target="_blank" rel="noopener"><strong>GooLeNetV1</strong></a>和<a href="https://jaryhuang.github.io/2018/11/02/BatchNorm%20And%20GooLeNetV2/" target="_blank" rel="noopener"><strong>BatchNorm</strong></a>基础上进行的改动。通过V3论文的标题:Rethinking the Inception Architecture for Computer Vision,我们也可以清楚的知道V3其主要对Inception进行改进。而V4的论文中除了Inception的应用,也把残差块引入了进来。由于篇幅原因,将这两个版本一起写。<br><a id="more"></a></p>
<h2 id="2-GooLeNetV3"><a href="#2-GooLeNetV3" class="headerlink" title="2.GooLeNetV3"></a>2.GooLeNetV3</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;作者发现Inception块结构复杂,很难对Inception块进行扩展。微小的改动都会引起结果的立即变化。因此很多人说GooLeNet这个网络太“刻意”了。基于改进的困难性,作者在文中提出了一些网络框架的理论设计准则:这些准则虽然有待试验验证,但作者建议尽量不要偏离这些准则:<br>&nbsp;&nbsp; <strong>1.不要急剧压缩特征:</strong> 信息流在前向传播的过程中不能经过高度的压缩层,否则会引起表达的瓶颈：在训练过程中,feature map的宽和高基本都会逐渐变小,但是不能一下子变小,否则将会丢失很多信息。而输出维度channal一般会逐渐的增多,否则网络会很难训练。但是特征维度channal并不代表信息的多少,只是一个粗略的估计手段。<br>&nbsp;&nbsp; <strong>2.增加卷积激活块:</strong> 高维的特征对网络的表征更有利。通过增加卷积网络中每个块的激活函数,这样会使网络训练的更快。个人的理解：多增加神经网络的激活层,相当于促进了其非线性化,使得网络表征往更高纬度走。<br>&nbsp;&nbsp; <strong>3.低纬度下可以空间聚合:</strong> 空间聚合可以在较低的维度上嵌入完成,而表征的能力没有太大或者任何的损失。假设相邻的单元之间的强相关,那么在降维期间信息损失的很小,于是这些信号的压缩会使得学习更快且精度不会损失多少,所以在GoogLeNet网络中先进行7x7。<br>&nbsp;&nbsp; <strong>4.平衡网络深度和宽度:</strong> 平衡网络的深度和宽度,深度增加了尽量减少宽度。</p>
<h3 id="2-1使用大过滤器尺寸分解卷积"><a href="#2-1使用大过滤器尺寸分解卷积" class="headerlink" title="2.1使用大过滤器尺寸分解卷积"></a>2.1使用大过滤器尺寸分解卷积</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;如何在保证精度的情况下提高计算效率是深度学习的一个研究的重点。首要研究的就是如何减少参数,Inception块是一个全卷积网络,所以从卷积下功夫了。<br>&nbsp;&nbsp; <strong>1)将因子分解为较小的卷积:</strong> 7x7,5x5的卷积层被3x3的卷积层所替代。这个跟vgg中一样,具体为何能够替代以及等效,在之前我做出了解释,具体可参照写的<a href="https://jaryhuang.github.io/2018/09/01/VggNet/" target="_blank" rel="noopener"><strong>VggNet</strong></a>,我解释的是在线性情况下,非线性激活对其是否有影响呢？<img src="/../../../images/DeepLearning/Classfication/GoogLeNet_V3/3x3_instead_5x5.png" alt="3x3 instead 5x5">从实验可看,卷积因子分解不仅减少了参数,同时精度上还有了一定的提升。所以,对于原先的Inception块,作者做了如下的改进：<img src="/../../../images/DeepLearning/Classfication/GoogLeNet_V3/Inception5.png" alt="小卷积Inception"><br>&nbsp;&nbsp; <strong>2)非对称卷积的空间分解:</strong> 什么是非对称卷积的空间分解?以下图为例,使用一个3x1的卷积后跟着一个1x3的卷积滑动相当于跟3x3具有相同的感受野,而对于这一转变可以减少33%的参数量。<img src="/../../../images/DeepLearning/Classfication/GoogLeNet_V3/非对称卷积.png" alt="非对称卷积">理论上所有的nxn卷积都可以转变为nx1后加1xn的模式:非对称卷积Inception形式<img src="/../../../images/DeepLearning/Classfication/GoogLeNet_V3/Inception6.png" alt="非对称卷积Inception形式">但是,作者在实践中发现,非对称卷积在早期的feature map中工作不是很好,在feature map尺寸为[m,m],m大约为12~20的时候效果很好,特别是取1x7与7x1的结合。在最后,作者对于feature map为8x8的特征提取采用如下的Inception块：<img src="、../../../images/DeepLearning/Classfication/GoogLeNet_V3/Inception7.png" alt="粗糙Inception"></p>
<h3 id="2-2辅助分类器的用处"><a href="#2-2辅助分类器的用处" class="headerlink" title="2.2辅助分类器的用处"></a>2.2辅助分类器的用处</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;带有辅助层训练的模型在训练前期的时候并没有什么提升,但是到了训练后期在精度上有了一定的提升,且有挺好的效果。本人在<a href="https://jaryhuang.github.io/2018/10/22/GoogLeNet_V1/" target="_blank" rel="noopener"><strong>GooLeNetV1的3.1中</strong></a>在cifar-10数据集上进行了实验,确实如作者所说。之前的解释是觉得这些侧边的辅助训练成有助于演变低级特征。但是作者通过移除了多个辅助层中的靠近低层的辅助分类器发现,低层的辅助分类器对结果无影响。因此,作者判断辅助分类层应该不是来演化低级特征的。但是也没有理论解释辅助层的作用,作者猜测可能是相当于一个正则化效果。通过的是一个弱的证据：<strong><em>在辅助训练层的加入了dropout层或者BatchNorm层之后效果更好。</em></strong></p>
<h3 id="2-3-有效的减小网格尺寸"><a href="#2-3-有效的减小网格尺寸" class="headerlink" title="2.3 有效的减小网格尺寸"></a>2.3 有效的减小网格尺寸</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;网络从低层到高层,主要是减少feature map,增加channal。最常用的模式是利用conv(1x1)+pooling,例如:[k,d,d]的feature map,要变成[2k,d/2,d/2],有以下两种方式:<img src="/../../../images/DeepLearning/Classfication/GoogLeNet_V3/网格尺寸减小方案.png" alt="网格尺寸减小方案">&nbsp;&nbsp;1)右图中采用的是Inception+pool的形式,主要问题是计算的参数量比较大。<br>&nbsp;&nbsp;2)左图pool+Inception模式,参数量有效的减少了,但是首先进行了池化,可能会使得部分数据丢失,失去了表征的代表性。这两种方式都是串行的,作者考虑了一种并行的方式,既然减少一些参数量,又能不失去表征的代表性：<img src="/../../../images/DeepLearning/Classfication/GoogLeNet_V3/并行网格尺寸减小.png" alt="并行网格尺寸减小"></p>
<h3 id="2-4-标签平滑"><a href="#2-4-标签平滑" class="headerlink" title="2.4 标签平滑"></a>2.4 标签平滑</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;标签平滑,作者花了整整一页的篇幅讲这件事。首先来了解下我们的交叉熵(cross entropy):<script type="math/tex">\ell=-\sum_{k=1}^{K}\log(p(k))q(k)</script>其中：$p(k|x) = \frac{\exp\left(z_{k}\right)}{\sum_{i= 1}^{K}\exp\left(z_{i}\right)}$是网络输出计算结果,而q(k)是onehot后的标签向量。</p>
<pre><code>First, it may result in over-fitting: if the model learns to assign full 
probability to the groundtruth label for each training example, it is not 
guaranteed to generalize. 
Second, it encourages the differences between the largest logit and all 
others to become large, and this, combined with the bounded gradient 
reduces the ability of the model to 
adapt. Intuitively, this happens because the model becomes too confident 
about its predictions.
</code></pre><p>&nbsp;&nbsp;&nbsp;&nbsp;这是摘取的原文,原本的交叉熵方法容易造成过拟合,减少了模型的适应能力。原因归结为出在设定的真实标签中。onehot标签法使得各个标签值比较突出,就是0-1量,因此对于模型训练就是对每个类尽量使其拟合为1,但实际在判断属于哪个类的时候,是依据相对概率大小,因此比如概率大于0.5就已经判定为此类了,而不必将概率达到0.9或者1.这应该也就是作者提到的模型太“自信”,因此作者提出了使得标签更soft的方法,称之为LSR。主要的改动就在真实标签q(k|x)上：<script type="math/tex">q^{\prime}(k|x)=(1-\epsilon)\delta_{k,y}+\epsilon u(k)</script>其中：$\delta_{k,y}$就是onehot标签,因此最直观的表现就是原本的onehot标签中比如第一类,其原本onehot标签在第一个位置上设定为1,基于上述平滑后可能是0.99。<br>本文实验中:</p>
<script type="math/tex; mode=display">u(k)=1/1000</script><script type="math/tex; mode=display">\epsilon = 0.1</script><h3 id="2-5-感受野大小-低分辨率"><a href="#2-5-感受野大小-低分辨率" class="headerlink" title="2.5 感受野大小(低分辨率)"></a>2.5 感受野大小(低分辨率)</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;采用更高分辨率感受野的模型往往会显著提高识别性能。作者不改变模型,测试了输入数据的不同分辨率对结果的影响,作者做了三个实验：<br>&nbsp;&nbsp;1).299×299感受野:第一层卷积采用步长为2以及池化层采用最大池化。<br>&nbsp;&nbsp;2).151×151感受野:第一层卷积采用步长为1以及池化层采用最大池化。<br>&nbsp;&nbsp;3).79×79感受野:第一层卷积采用步长为1且没有采用池化层。<br>测试结果如图：<img src="/../../../images/DeepLearning/Classfication/GoogLeNet_V3/感受野的影响.png" alt="感受野的影响"></p>
<h3 id="2-6-小结"><a href="#2-6-小结" class="headerlink" title="2.6 小结"></a>2.6 小结</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;GooLeNetV3版本中主要的贡献在于:</p>
<ul>
<li><strong>分解卷积</strong>:使卷积小型化、多层化,新提出<em>非对称卷积</em>比较新颖</li>
<li><strong>辅助分类器解释</strong>:对之前提出的辅助分类器有了新的解释,虽然不是理论论证,但是通过加入Dropout或者BatchNorm的优化方法提升了精度,改进了辅助分类</li>
<li><strong>减小feature map尺寸</strong>:提出一种并行的,计量小的缩小特征图的方法</li>
<li><strong>标签平滑</strong>：对于交叉熵损失函数进行了改进,提出了LSR方法</li>
</ul>
<h2 id="3-GooLeNetV4"><a href="#3-GooLeNetV4" class="headerlink" title="3.GooLeNetV4"></a>3.GooLeNetV4</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;ResNet的提出,使得更深层神经网络的精度有了显著性的提升,因此在训练非常深网络的时候残差连接(Residual connection)被认为有内在的必要性。论文作者基于这个思考,他也在考虑加深Inception网络是否更有效？Inception结构是否可以跟Residual connection结合一下？基于这些思考,谷歌大佬们在《Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning》主要讲两个框架,一个是Inception再深入一步,推出V4版本。另外一个就是将Residual connection和GoogLeNet结合形成Inception-resnet网络。</p>
<h3 id="3-1-InceptionV4"><a href="#3-1-InceptionV4" class="headerlink" title="3.1 InceptionV4"></a>3.1 InceptionV4</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;InceptionV4中总体没有大的改变,是对前三篇提出的一些方法进行一个架构的重组或者说是重建。所以我们对着它的架构来进行学习。<img src="/../../../images/DeepLearning/Classfication/GoogLeNet_V4/InceptionV4.png" alt="InceptionV4">总体架构如上图所示：[W,H]:299-&gt;35-&gt;17-&gt;8-&gt;1,[C]:3-&gt;384-&gt;1024-&gt;1538-&gt;1000。为何是这几个数字,估计只有作者能回答了。目前我还没找到思路来解释这个,希望有知道的大佬告知我一声。下面来详细了解下里面的分支结构,与InceptionV3中提出的规则做一下对应：<br><strong>1)首先是stem块</strong><img src="/../../../images/DeepLearning/Classfication/GoogLeNet_V4/stem.png" alt="stem">对应着第三条规则(空间聚合可以在较低的维度上嵌入完成),刚输入的图像相邻的单元之间的相关性应该比较强,因此在第一层一般会在[W,H]上进行一个大的降维。<br>2)<strong>Inception-A块</strong>主要做的是特征提取:<img src="/../../../images/DeepLearning/Classfication/GoogLeNet_V4/Inception-A.png" alt="Inception-A">他在架构上没做什么改进,依旧是5x5,3x3,1x1再加个自身的average pooling层,通过多个尺度来进行特征的学习,但是它叠加了四个,所以在总体上这一块的特征提取应该更强了。在35-17的降维上：<img src="/../../../images/DeepLearning/Classfication/GoogLeNet_V4/Reduction-A.png" alt="Reduction-A">通过conv(3x3,5x5,stride=2)以及maxpool来进行降维尺寸缩小2,这种降维方式,也是一种多尺度的降维,池化和不同的卷积组合降维的表达更加丰富。<br><strong>3)在17x17这个尺寸的图上,Inception-B</strong>:<img src="/../../../images/DeepLearning/Classfication/GoogLeNet_V4/Inception-B.png" alt="Inception-B">在这个结构上,其他没有大的改变,但是在卷积上采用了非对称卷积,而且1x7和7x1的一条支线上是一组,另外一条支线上是两组,也是通过不同的感受野的角度出发。<strong>在V3中也提到过,这种非对称卷积在[W,H]在12-20的时候比较好,因此作者也在降维到17x17之后才用。</strong>同样,在卷积特征提取之后,后续要进行降维,将17x17变为8x8,reduction模块的设计主要还是加入了非对称卷积上。<img src="/../../../images/DeepLearning/Classfication/GoogLeNet_V4/Reduction-B.png" alt="Reduction-B"><br><strong>4)降维之后,接下来继续对8x8的进行提取:</strong><img src="/../../../images/DeepLearning/Classfication/GoogLeNet_V4/Inception-C.png" alt="Inception-C">在这一block上,基本将几篇论文中提出的一些框架都应用了上来。在最后,通过Avarage Pooling<br>+Dropout(0.8)来进行分类,这里引入Dropout个人觉得是网络框架非常的复杂,容易引起过拟合,因此将Dropout重新引入了进来。</p>
<h3 id="3-2-Inception-resnet"><a href="#3-2-Inception-resnet" class="headerlink" title="3.2 Inception-resnet"></a>3.2 Inception-resnet</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;Inception-resnet网络框架如图所示：<img src="/../../../images/DeepLearning/Classfication/GoogLeNet_V4/Inception-resnet.png" alt="Inception-resnet">其主要就是Inception和残差连接(Residual connection)相结合。以Inception-resnet-A为例:<img src="/../../../images/DeepLearning/Classfication/GoogLeNet_V4/Inception-resnet-A.png" alt="Inception-resnet-A">这里为什么会多个1x1的conv呢,因为作者发现,当滤波器数量超过1000的时候,会出现不稳定,所以每次都利用1x1卷积来进行降维,具体细节可参考作者原文。</p>
<h2 id="4-总结"><a href="#4-总结" class="headerlink" title="4.总结"></a>4.总结</h2><h3 id="4-1-V3和V4版本的总结"><a href="#4-1-V3和V4版本的总结" class="headerlink" title="4.1 V3和V4版本的总结"></a>4.1 V3和V4版本的总结</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;上述V3和V4版本,个人感觉改进主要以V3为主,V4是将V3的想法继续应用并改进。<br><strong>V3主要提出：</strong><br>1)对网络的修改四个未经证实的建议<br>2)非对称卷积<br>3)网络尺寸减小也可以有“多尺度”的形式<br>4)标签平滑<br><strong>V4主要是：</strong><br>1)将上述提出的方法进行应用<br>2)将(Residual connection)引入到Inception中。</p>
<h3 id="4-2-三个网络实验对比"><a href="#4-2-三个网络实验对比" class="headerlink" title="4.2 三个网络实验对比"></a>4.2 三个网络实验对比</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;将上述三个网络在cifar-10上进行实验,首先是训练时间上(单个GPU1080Ti):<img src="/../../../images/DeepLearning/Classfication/GoogLeNet_V4/train_time.png" alt="Train_time">从上图中我们也清晰的看出,相比之下InceptionV3训练快很多,而V4和Inception-resnet的实际训练时间相接近。再从验证精度上看：<img src="/../../../images/DeepLearning/Classfication/GoogLeNet_V4/val_acc.png" alt="Val_Acc">在cifar10数据集上,V3和V4的整体表现V4略强。但是换来的是训练时间的加强。但是Inception_resnetV2更强于V4的。这也作为未来大家选择模型的时候，在模型大小和精度上做一个考量。</p>
<h2 id="5-参考"><a href="#5-参考" class="headerlink" title="5.参考"></a>5.参考</h2><ol>
<li>Szegedy C , Ioffe S , Vanhoucke V . Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning[J]. 2016.</li>
<li>Szegedy C , Vanhoucke V , Ioffe S , et al. Rethinking the Inception Architecture for Computer Vision[J]. 2015.</li>
<li>He K, Zhang X, Ren S, et al. Deep Residual Learning for Image Recognition[J]. 2015.</li>
</ol>
]]></content>
      <categories>
        <category>DeepingLearning</category>
      </categories>
      <tags>
        <tag>Classfication</tag>
      </tags>
  </entry>
  <entry>
    <title>BatchNorm and GoogLeNet_V2</title>
    <url>/2018/11/02/DeepLearning/Classficaition/BatchNorm%20And%20GooLeNetV2/</url>
    <content><![CDATA[<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1.引言"></a>1.引言</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;深度学习在训练的时候,每一层的输出随着参数改变而改变,这也导致均值以及方差在各个层中都不相同。<strong>众所周知,如果输入是均值为0,方差为1的数据训练则会加快很多。同时,随着训练的加深,很容易造成梯度饱和,导致无法训练或者训练很慢。</strong><br><img src="/../../../images/DeepLearning/Classfication/GoogLeNet_V2/sigmoid1.png" alt="sigmod">如图所示,在simoid函数中当$Z&gt;5 or Z&lt;-5$时,函数值趋向于1,梯度基本消失,参数无法进行一个学习,这对于神经网络来说非常的不利。如何来通过一些改善来解决整个问题呢？<br><img src="/../../../images/DeepLearning/Classfication/GoogLeNet_V2/sigmoid2.png" alt="sigmod">图二以sigmoid为例子,如果我们的参数范围在[-1,1]之间的话,那么在这一段的梯度是最大的,对于训练以及参数更新会是最快的,因此大家慢慢的想到了归一化。但是归一化又带来了一个问题,对sigmoid函数来说,采用直接归一化为[-1,1]之间的话又是使得sigmoid函数近似线性关系,丢失了其非线性关系,神经网络的散失了主要的能力。因此,要找到一种合适的归一化方式,以及确定归一化的范围,这是研究的重点。<br><a id="more"></a></p>
<h2 id="2-BatchNorm"><a href="#2-BatchNorm" class="headerlink" title="2.BatchNorm"></a>2.BatchNorm</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;相信大家只要用过深度神经网络,肯定用到过BatchNorm,不管是有意还是无意,用的很多网络中都自带了BN层,这也从侧面反映出了BN层的重要性。刚刚上述说的梯度消失问题,通常采用relu函数来解决:<img src="/../../../images/DeepLearning/Classfication/GoogLeNet_V2/relu.png" alt="relu">如上图所示,对于relu函数,右边的梯度不管多大,都是1。而对于未激活的神经元,其没有梯度,所以不会造成上述梯度消失的问题。但是我们考虑一个网络的损失函数为：</p>
<script type="math/tex; mode=display">\zeta = F_2(F_1(u,\theta_1)\theta_2)</script><p>其中$\theta_1,\theta_2$是参数,$F_1,F_2$是一个任意变换。当学习$\theta_2$的时候,令$x = F_1(u,\theta_1)$,那么损失函数简化为：</p>
<script type="math/tex; mode=display">\zeta = F_2(x,\theta_2)</script><p>当我们做梯度更新,根据公式<script type="math/tex">\theta_2 \leftarrow \theta_2 - \frac{\alpha}{m}\sum{i=1}_m \frac{\partial F_2(x_i,\theta_2)}{\partial \theta_2}</script><br>可知,输入的扰动属性使得训练更有效(参数更新),但其不能过大也不能过小。因此提出了归一化,<br>$\widehat{x}^{(k)}=\frac{x^{(k)}-E[x^{(k)}]}{\sqrt{Var[x^{(k)}]}}$ 使得输入数据的分布是0均值1方差。<br>&nbsp;&nbsp;&nbsp;&nbsp;<strong>但是简单的归一化输入数据,可能会引起输出的表达。</strong>这句话如何理解呢。以sigmoid为例子,如果在其前面加了简单的归一化层之后,其会约束sigmod函数的线性状态:<img src="/../../../images/DeepLearning/Classfication/GoogLeNet_V2/sigmoid2.png" alt="sigmod">如图所示,sigmod在[-1,1]之间为基本的线性状态,导致深度神经网络学到了线性网络的过程,影响了深度神经网络的非线性表达能力,使得其能力变弱。为了解决上述问题,作者提出了使得整个归一化转变能够表征对象。即$y^{(k)}=\gamma^{(k)}\hat{x}^{(k)}+\beta^{(k)}$,而$\gamma$和$\beta$是学习参数,通过改变这两个参数可以调节其限定区域。<br>&nbsp;&nbsp;&nbsp;&nbsp;基于此,我们已经了解了BatchNorm的整体设计思路。那么在使用的时候,咱也非常方便,参照论文中的思路：<img src="/../../../images/DeepLearning/Classfication/GoogLeNet_V2/batch_applied.png" alt="batch_applied">就能方便的应用。对于超参数的求解,同样应用backward的思路,因为loss对batchnorm中的各个参数都可导,所以容易求解,具体可参照参考文献中的原论文。</p>
<h3 id="2-1-BatchNorm层到底该放在卷积网络的什么位置"><a href="#2-1-BatchNorm层到底该放在卷积网络的什么位置" class="headerlink" title="2.1 BatchNorm层到底该放在卷积网络的什么位置?"></a>2.1 BatchNorm层到底该放在卷积网络的什么位置?</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;对于BN层应该放在哪个位置,首先得了解卷积网络大概的层次。对于卷积层来说,依据AlexNet网络,大致可分为：1.卷积2.非线性3.池化。BN层的设计就是为了解决训练的速度和梯度等问题,因此当加入我们的BN层之后,BN层的位置主要考虑在2前还是2之后。作者在论文中已有一些表述,卷积层前的输入可能是另一个非线性的输出,它的分布形状很可能在训练期间发生了变化,并且对其有一定的限制(通俗点将,就是非线性层函数有其特定的形式,比如relu函数是的一部分为0了,我们不应该通过BN层让其改变)。但有许多人的尝试中觉得BN层放在后面较好。<img src="/../../../images/DeepLearning/Classfication/GoogLeNet_V2/relu_acc.png" alt="batch_applied">这是利用cifar10将batch_norm层放在relu之前和之后的结果,图中的网络是inceptionv1,其算中等类型大小的网络,两者实际并没有太多的差别,如果真要找出细微的差别,我将精度最终放大,看到如下的结果。<br><img src="/../../../images/DeepLearning/Classfication/GoogLeNet_V2/relu_acc_big.png" alt="batch_applied"><br>显示差异基本微乎其微。<br>&nbsp;&nbsp;&nbsp;&nbsp;个人理解,原本的relu就有正则化效果,BN+RELU使得正则化的效果更明显,因为BN层将数据空间进行转换,再加上RELU,两者可配合使得网络参数更稀疏化。<br>&nbsp;&nbsp;&nbsp;&nbsp;而RELU+BN层的效果,在<a href="https://jaryhuang.github.io/2018/08/26/AlexNet/" target="_blank" rel="noopener"><strong>AleNet</strong></a>中讲过RELU的很多优点,但是实际中我们也很清楚的了解其也有一定的缺点。可能存在使得一些神经元永远未激活。但是在RELU后加入了BN层之后就变的不一样了,部分神经元通过BN层之后会变相被重新激活。因此通过上述我们可以猜测: <strong>BN+RELU适用于大网络(需要正则化防止过拟合),RELU+BN适用于小网络(小网络学习能力不够,需要更多的神经元激活)</strong> 基于这个假设,本人设计了3卷积层+1个全连接层的小网络来测试cifar-10数据集,计算结果如图所示：<img src="/../../../images/DeepLearning/Classfication/GoogLeNet_V2/small_network_test.png" alt="batch_applied">&nbsp;&nbsp;&nbsp;&nbsp;从图中看到,比较了几组试验。红线和蓝线可以做对比,两者都用的是Maxpool池化层,从图中可以看出,在训练的速度上(如达到30%的,BN层放置后面就快了很多),但是在两者的精度上,并没有很大的差别,原因从个人分析感觉是源于maxpool。不管BN在后的话,其使得数据重新分布,但是经过最大池化后,那些变相重新激活的0神经元被最大池化给滤波掉了, 基本没有效果。因此,本人将maxpool变为avgpool,测试了效果,果然有了整体的提升。并且,两者的差距也显示出来了。RELU_BN相比较BN_RELU涨了0.6%,这也印证了我上述对小网络的猜测。至于大网络,本人拟采用resnet框架来验证,等后续整理到那块的时候再进行说明,这个部分未完待续。</p>
<h3 id="2-2-BatchNorm层的优势"><a href="#2-2-BatchNorm层的优势" class="headerlink" title="2.2 BatchNorm层的优势"></a>2.2 BatchNorm层的优势</h3><p>1)提高学习率：传统深度网络中,过高的学习率通常会导致梯度爆炸或者梯度消失,BN层可以通过对输入空间的放大以及移动来避免这种情况。<br>2)正则化效果：加入了BN层之后,发现其有减少过拟合的效果,因此可以减弱或者移除Dropout层。这也间接说明了BN层的正则化效果。</p>
<h2 id="3-GoogLeNetV2网络的结构与测试"><a href="#3-GoogLeNetV2网络的结构与测试" class="headerlink" title="3.GoogLeNetV2网络的结构与测试"></a>3.GoogLeNetV2网络的结构与测试</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;GoogLeNetV2的改进点可归纳为以下几个点：<br>1)5x5的卷积层被2个3x3的卷积层所替代。这个跟vgg中一样,具体为何能够替代以及等效,在之前我做出了解释,具体可参照写的<a href="https://jaryhuang.github.io/2018/09/01/VggNet/" target="_blank" rel="noopener">VggNet</a>网络。3x3网络的替代之后,使得网络的最大深度增加了9层,参数量增加了25%。<br>2)在池化层方面,有时内部采用平均池化,有时采用最大池化。<br>3)最主要的改变就是在relu前面加入了BN层,这篇论文对后续的分类模型结构影响最大的一部分。<br>&nbsp;&nbsp;&nbsp;&nbsp;GoogLeNetV2主要结构如下图所示：<img src="/../../../images/DeepLearning/Classfication/GoogLeNet_V2/Inception_architecture.png" alt="Inception architecture">&nbsp;&nbsp;&nbsp;&nbsp;此结构图中含义可参考<a href="https://jaryhuang.github.io/2018/09/03/GoogLeNet_V1/" target="_blank" rel="noopener">GoogLeNet_V1</a>,但是在实际搭建网络的时候,发现按照此结构图中进行搭建存在问题,问题主要在inception(4c)inception(4d)的output size的channal并不等于与576,而是$160+160+160+128 = 608$,因此按照这个表格来搭建网络出现问题,具体是文中有错还是有我没注意的点,欢迎大家批评指正。个人将图中圈黄色的两处channal改为了128来进行网络搭建。</p>
<h3 id="3-2-GoogLeNet-V2测试"><a href="#3-2-GoogLeNet-V2测试" class="headerlink" title="3.2 GoogLeNet_V2测试"></a>3.2 GoogLeNet_V2测试</h3><p><img src="/../../../images/DeepLearning/Classfication/GoogLeNet_V2/model_ensemble.png" alt="model ensemble">&nbsp;&nbsp;&nbsp;&nbsp;这是论文中作者经过试验比较了V1和V2版本的效果,从上述来看,V2相比于V1降低了1.77%的百分点。当然,从这张表中可以学到的一些测试数据处理(测试数据进行切割)和模型融合(多个模型融合)的方法。本人在比赛中实操发现,特别是对于一些样本相对较少,且有一些不平衡来说,训练集和测试机的划分非常的重要。样本量少,可能导致某些特殊的样本并没有到训练集中,所以当随机划分的时候每划分一次训练的模型出来的效果可能偏差非常大,所以采用多个模型融合的方法可以缓解这个问题。<br>&nbsp;&nbsp;&nbsp;&nbsp;本文基于GooLeNet的Appendix利用pytorch进行了训练,其中增加了翻转的数据增强做一个简单的对比试验：<img src="/../../../images/DeepLearning/Classfication/GoogLeNet_V2/GooLeNetV2_test.png" alt="model test">从中可以看出GooLeNetV2达到了相对较好的准确度,当然对于cifar10数据集,简单的翻转增强也提高其精度,如果加入更多的数据增强手段,应该在精度上还是会有一些增加点。</p>
<h2 id="4-总结"><a href="#4-总结" class="headerlink" title="4.总结"></a>4.总结</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;GooLeNet_V2中提出的BatchNorm对后续的网络影响很大,在现今的网络大部分应用中都会加入了BN层。当然对于BN层我们也要引起注意的是其归一化是通过B,W,H这个尺度进行的(原图像为[B,C,W,H]).因此,BatchSize的大小对其有一定的影响,如果太小,对归一化影响非常大。</p>
<h2 id="5-参考"><a href="#5-参考" class="headerlink" title="5.参考"></a>5.参考</h2><ol>
<li>Ioffe S, Szegedy C. Batch normalization: Accelerating deep network training by reducing internal covariate shift[J]. arXiv preprint arXiv:1502.03167, 2015.</li>
</ol>
]]></content>
      <categories>
        <category>DeepingLearning</category>
      </categories>
      <tags>
        <tag>Classfication</tag>
      </tags>
  </entry>
  <entry>
    <title>GoogLeNet</title>
    <url>/2018/10/22/DeepLearning/Classficaition/GoogLeNet_V1/</url>
    <content><![CDATA[<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1.引言"></a>1.引言</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;GoogLeNet是由谷歌(Google)团队研发出来的,为什么要将整个L大写呢,大家看后面的LeNet,据说是为了向“LeNet”致敬,因此取名为“GoogLeNet”。<br>&nbsp;&nbsp;&nbsp;&nbsp;前面我已经介绍了<a href="https://jaryhuang.github.io/2018/09/01/VggNet/" target="_blank" rel="noopener">VggNet</a>,VGG在当年的ImageNet挑战赛中拿到了第二名的成绩,而GoogLeNet是获得了第一名。两者网络的设计方面都是在深度方面进行了加深,但是加深的方式又是不同的：VggNet是延续了LeNet或者说AlexNet等网络框架,按照标准的网络结构,由卷积层,归一化层,非线性层,池化层以及全连接层组成,其主要通过这些层的叠加来加深网络深度。在网络深度加深的同时,利用dropout层来减弱出现的过拟合现象。而对于GoogLeNet网络来说,在Going deeper with convolutions论文中提到了其对于deeper的两方面的理解：首先就是新的Inception模块,再者就是加深了网络的层数这个直接的含义了。因此,从其对深度的理解也能容易的看出其在网络结构方面做的改进。<br><a id="more"></a></p>
<h2 id="2-网络的创新点"><a href="#2-网络的创新点" class="headerlink" title="2.网络的创新点"></a>2.网络的创新点</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;提升神经网络能力的最直接方式就是增加典型环节的规格：他们包括增加网络的深度或者增加网络的宽度。这相对来说比较容易、安全的训练一个神经网络(大家改进的方向大多是沿着这个方向,而且很多文章都验证了深度能够提升神经网络的性能)。但是通过这种方式也带来了两个缺点：<br>&nbsp;&nbsp;&nbsp;&nbsp;1)更大的尺寸意味着更多的训练参数,使得网络更容易过拟合。<br>&nbsp;&nbsp;&nbsp;&nbsp;2)更多的训练参数增加了计算资源的利用,消耗更多的时间。<br>&nbsp;&nbsp;&nbsp;&nbsp;在神经网络的训练完成后,当查询最终模型参数的,会发现结果中大部分参数都是0,换个说法就是参数具有很强的稀疏性(参数中真正有用的是非零的数据,这才算是激活了的神经元)。因此,要解决上述两个缺点的根本方法就是从全连接层(在<a href="https://jaryhuang.github.io/2018/09/01/VggNet/" target="_blank" rel="noopener">VggNet</a>网络第4部分中以做出说明,网络参数多的原因主要就是聚集在全连接层)中解脱出来稀疏连接的结构。<br>&nbsp;&nbsp;&nbsp;&nbsp; <strong>如果数据集的概率分布可以由非常稀疏的神经网络表示出来,那么理论上可以逐层分析网络最后一层激活的相关统计数据,来构建一个最优的网络拓扑结构,聚集这些高度相关的神经元。</strong> 严格的数学证明需要非常强大的条件,但是这一想法和Hebbian准则不谋而合：neurons that fire together, wire together,这一思想引导着我们探索网络的稀疏结构。卷积滤波器个人理解其实际上就是一个稀疏结构,利用卷积滤波器来代替全连接层,最后的参数稀疏化为kernel size。但不同的是这个稀疏结构的size由人为进行设定,如1x1,3x3,5x5等。虽然自身无法构建整个强烈的数学关系来推出稀疏结构的尺寸,但是可以利用不同的感受野进行假设、组合以及叠加等。Google提出的Inception模块是对各个感受野进行了组合和叠加(其实这里还涉及到了一个问题,如何确定各层的感受野,个人感觉是一个很好的研究问题,在未来可以进行交流)。</p>
<h3 id="2-1-Inception模块"><a href="#2-1-Inception模块" class="headerlink" title="2.1 Inception模块"></a>2.1 Inception模块</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;正如上述所说我的理解中的Inception模块,其利用卷积这个稀疏结构来覆盖全连接层。但是由于卷积层的稀疏结构并没有直接的理论验证应该是什么尺寸,因此Inception中是利用了1x1,3x3,5x5的模块,如下图所示：<br><img src="/../../../images/DeepLearning/Classfication/GoogLeNet_V1/1.png" alt="Alt text">&nbsp;&nbsp;&nbsp;&nbsp;此外,作者在Inception模块中加入了池化操作。池化操作的加入使得网络不仅有了不同kernel size的感受野,同时加上了上一层的聚类信息,使得网络提取的特征更加的丰富。在每个卷积层之后加入了ReLU非线性层,更加提高了其非线性拟合能力。<br>&nbsp;&nbsp;&nbsp;&nbsp;由于Inception模块是通过各个结构的叠加,因此需要尺寸在W和H上保持相同,因此在使用过程中需要设定padding。对于不管是卷积还是池化操作中,遇到kernel size为1x1,3x3,5x5的padding分别为0,1,2。</p>
<h3 id="2-2-1x1卷积核降维的Inception"><a href="#2-2-1x1卷积核降维的Inception" class="headerlink" title="2.2 1x1卷积核降维的Inception"></a>2.2 1x1卷积核降维的Inception</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;Inception模块作为一个整体,可视为网络的一层。在网络的维度设置上,在低层的时候一般用较小的channel。随着网络层数的增加,网络的channel会增加,这预计会使得空间浓度降低,1x1卷积核主要提取的就是空间的信息,空间信息通过1x1卷积核应该能获取到其强烈的关系了,因此应该更关注与W和H层面的信息,意味着在更高层的时候需要增大3x3和5x5卷积核的比例。<br>&nbsp;&nbsp;&nbsp;&nbsp;但是Inception模块随着3x3和5x5卷积核的比例增加导致参数数量增大(最明显就是池化层了,使得每经历一个Inception模块的便会增加channal数量),越到高层数量越大,可能会导致阶段内计算爆炸,所以需要用到降维技术。1X1卷积核的另外一个重要的作用就是降维了,本人在<a href="https://jaryhuang.github.io/2018/07/13/SE-Net/" target="_blank" rel="noopener">SE-Net</a>的2.2部分讲了SE块中讲述了利用1X1卷积核来降维,具体可参考如何降维度(channel)。<br>&nbsp;&nbsp;&nbsp;&nbsp;基于此,论文中提出了降维的Inception模块(应该叫减少参数量的Inception模块应该更恰当),具体结构如图所示：<br><img src="/../../../images/DeepLearning/Classfication/GoogLeNet_V1/2.png" alt="Alt text"><strong>结构中的1x1卷积核除了降低维度之外,同时后面跟着非线性层(ReLU),增加了网络非线性拟合能力</strong></p>
<h3 id="2-3-Average-pooling代替全连接层"><a href="#2-3-Average-pooling代替全连接层" class="headerlink" title="2.3 Average pooling代替全连接层"></a>2.3 Average pooling代替全连接层</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;在GoogLeNet中还有一个很大的改进就是网络最后采用average pooling来代替全连接层,该想法来自NIN(Network in Network),作者实验的结果是准确率提升了0.6%,但是其在最后还是加了一个全连接层,为了方便对输出进行灵活的调整。</p>
<h2 id="3-GoogLeNet网络的结构与测试"><a href="#3-GoogLeNet网络的结构与测试" class="headerlink" title="3.GoogLeNet网络的结构与测试"></a>3.GoogLeNet网络的结构与测试</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;基于Inception模块构建的GoogLeNet总共有22层(如果计算入池化层的话,就是27层),网络结构如下：<br><img src="/../../../images/DeepLearning/Classfication/GoogLeNet_V1/3.jpg" alt="Alt text">&nbsp;&nbsp;&nbsp;&nbsp;网络额外增加了2个辅助的softmax用于向前传导梯度(辅助分类器)。辅助分类器是将中间某一层的输出用作分类,并按一个较小的权重(0.3)加到最终分类结果中,这样相当于做了模型融合,同时给网络增加了反向传播的梯度信号,也提供了额外的正则化,对于整个网络的训练很有裨益。而在实际测试的时候,这两个额外的softmax会被去掉。整体网络具体网络的细节图如图所示：<img src="/../../../images/DeepLearning/Classfication/GoogLeNet_V1/4.png" alt="Alt text">&nbsp;&nbsp;&nbsp;&nbsp;<strong>上表中的“#3x3 reduce”,“#5x5 reduce”表示在3x3,5x5卷积操作之前使用了1x1卷积的数量</strong>,下面以 <strong>Inception 3a层</strong>为例子进行下说明：输入channel是192<br>&nbsp;&nbsp; <em>1)64个1x1的卷积核,然后RuLU,输出28x28x64</em><br>&nbsp;&nbsp; <em>2)96个1x1的卷积核,作为3x3卷积核之前的降维,变成28x28x96,然后进行ReLU计算,再进行128个3x3的卷积（padding为1）,输出28x28x128</em><br>&nbsp;&nbsp; <em>3)16个1x1的卷积核,作为5x5卷积核之前的降维,变成28x28x16,进行ReLU计算后,再进行32个5x5的卷积(padding为2),输出28x28x32</em><br>&nbsp;&nbsp; <em>4)pool层,使用3x3的核(padding为1),输出28x28x192,然后进行32个1x1的卷积,输出28x28x32</em><br><em>将四个结果进行连接,对这四部分输出结果的第三维并联,即64+128+32+32=256,最终输出28x28x256</em></p>
<h3 id="3-1-网络深度以及辅助层的影响"><a href="#3-1-网络深度以及辅助层的影响" class="headerlink" title="3.1 网络深度以及辅助层的影响"></a>3.1 网络深度以及辅助层的影响</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;论文采用三个softmax的combine来作为训练,测试的时候直接利用softmax2的结果。依据作者的思路,本人先后测试了训练每个softmax以及combine的结果如下图所示：<img src="/../../../images/DeepLearning/Classfication/GoogLeNet_V1/5.png" alt="Alt text"><br>&nbsp;&nbsp;&nbsp;&nbsp;图中softmax0代表的是网络结构中softmax0的输出,其他同理。combine的结果是训练的时候,将softmax0到2的结果通过[0.3,0.3,0.4]的权重进行加权训练的结果。从图中清楚的发现,从softmax0到softmax2来讲,其训练以及测试的误差越来越小,表明通过Inception模块来增加网络深度的方式是有利于提高网络的精度。再者,图中的黄线(combine)的结果表明,通过三个结果的加权训练能够提高网络的精度。</p>
<h3 id="3-2-与VGG网络的对比"><a href="#3-2-与VGG网络的对比" class="headerlink" title="3.2 与VGG网络的对比"></a>3.2 与VGG网络的对比</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;GoogLeNet和VGG网络作为当年分类比赛的第一第二名,自然需要拿来进行比较比较。1.从参数方面来看,VGG-16的总参数量为1.3亿个参数,GoogLeNet-V1版本的参数0.1亿个,这里相差了13倍。参数的差别最直观的体现在训练的时间上：<img src="/../../../images/DeepLearning/Classfication/GoogLeNet_V1/7.png" alt="Alt text">&nbsp;&nbsp;&nbsp;&nbsp;图上清晰的看出来,从训练时间来看,GoogLeNet远远快于VGG网络。2.从网络精度来看,由于pytorch没找到GoogLeNet-V1的ImageNet预训练模型(V3的有),因此做训练精度的对比只能从两者零初始化来对比,这个对比没有那么的严格,仅供大家交流参考(有无预训练差别很大,在VGG网络里面利用VGG-11已经做过对比,如有问题,可以过去查看)：<img src="/../../../images/DeepLearning/Classfication/GoogLeNet_V1/6.png" alt="Alt text"><br>&nbsp;&nbsp;&nbsp;&nbsp;这是两者的误差对比,从零初始化来看GoogLeNet优于VGG-16网络。明显的看出,VGG-16网络其实处于过拟合状态,这也和其参数量有关。但是在很多的网络应用中,都是利用VGG网络的卷积层(只占11%的参数)来进行特征的提取,推出了很多的改良版本,每个网络都有其可借鉴之处。</p>
<h3 id="3-3-模型融合和测试数据增强"><a href="#3-3-模型融合和测试数据增强" class="headerlink" title="3.3 模型融合和测试数据增强"></a>3.3 模型融合和测试数据增强</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;GooLeNet_v1的论文中测试数据增强主要体现,将每张测试图片变为了$4\cdot 3\cdot 6\cdot 2 = 144$张图来进行softmax后的取平均。这144张图片如何处理的呢。<br>1)<strong>4：</strong>按图片的短边分别resize成256,288,320,352的图片.<br>2)<strong>3:</strong>将上述resize后的图片变为上中下或者左中右三幅。具体如何理解，比如上述按短边resize后为$[w,h] = [256,300]$(这里短边是w),那么按照上中下分别crop三张图都为$[256,256]$,出现左中右是因为短边有可能是h.<br>3)<strong>6:</strong>接下来将上述crop后的方形图片按照4个角落+1个中间+原图resize变为 $[224,224]$.<br>4)<strong>2:</strong>上述所有图片的镜像图片.<br>&nbsp;&nbsp;&nbsp;&nbsp;GooLeNet_v1的论文中除了测试数据增强外,其也应用了模型融合技术，采用7个模型融合。这7个模型改变的主要是初始化以及图片送进来的顺序。这些测试的trick给了模型很大的提升,可以参见论文中的图。<img src="/../../../images/DeepLearning/Classfication/GoogLeNet_V1/ensembleAndaugment.png" alt="Alt text"></p>
<h2 id="4-总结"><a href="#4-总结" class="headerlink" title="4.总结"></a>4.总结</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;GoogLeNet的出现,给了大家一些启示,1.Inception结构的应运而生,作者其核心思想是找出一个稀疏结构来表征神经网络,而稀疏化与卷积结合起来,因此出现了著名的Inception——“稀疏化全连接层”。这也给了研究者启发,除了运用标准的各个层加深网络之外,也可以从网络的宽度以及每层的结构上着手2.GoogLeNet中的应用1X1卷积核的降维方法,为网络在深度方面的提升给了很大的帮助。3.Average pooling代替全连接层减少了网络参数,同时提高了精度,有很大的指导意义。4.当然其在训练的时候有很多trick,如增加辅助层等。5.在测试的时候,通过模型融合以及图片的数据增强,显著降低了模型的误差。<br>&nbsp;&nbsp;&nbsp;&nbsp;本人先看的VGG网络论文,后看的GoogLeNet,考虑到VGG网络的3x3卷积核的叠加来代替5x5,后面发现在GoogLeNet-V2中正好也是应用了这个思路(证明笔者还是有思考的)。</p>
<h2 id="5-参考"><a href="#5-参考" class="headerlink" title="5.参考"></a>5.参考</h2><ol>
<li>Szegedy C, Liu W, Jia Y, et al. Going deeper with convolutions[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2015: 1-9.</li>
<li>Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. CoRR, abs/1312.4400, 2013.</li>
<li><a href="https://blog.csdn.net/imPlok/article/details/79932834" target="_blank" rel="noopener">大话CNN经典模型：GoogLeNet(从Inception v1到v4的演进)</a></li>
</ol>
]]></content>
      <categories>
        <category>DeepingLearning</category>
      </categories>
      <tags>
        <tag>Classfication</tag>
      </tags>
  </entry>
  <entry>
    <title>数据结构和算法----线性表</title>
    <url>/2018/10/17/Python/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%92%8C%E7%AE%97%E6%B3%952-%E7%BA%BF%E6%80%A7%E8%A1%A8/</url>
    <content><![CDATA[<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1.引言"></a>1.引言</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;数据结构中简单也比较常用的结构就是线性结构,本章围绕着线性表来进行学习。先提出一个问题,给你一个一元多项式：$f(x) = a_0+a_1\cdot x+…+a_{n-1}\cdot x^{n-1}+a_n\cdot x^n$,进行的主要运算是加减乘除。但当编程的时候,首要问题是要了解如何表示这个多项式。多项式的表示主要是项数$n$,系数$a_i$和次方$i$。<br><a id="more"></a></p>
<h3 id="1-1-顺序存储结构"><a href="#1-1-顺序存储结构" class="headerlink" title="1.1 顺序存储结构"></a>1.1 顺序存储结构</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;对于一元多项式的加减乘除计算,首先想到的就是顺序存储结构了,常用的如数组,列表等。既然称之为顺序存储结构,那么它的顺序表现在哪几个方面：<strong>1.申请的地址空间是按顺序增加的。2.元素有序排列,可根据下标访问。</strong>因此,采用数组进行存储如$f(x)=3\cdot x^4+2\cdot x+5$,数组就可以表示成$a = [5,2,0,0,3],a[i]$为系数,$i$为多项式的次方。这个问题只用了一个数组就能全部表达了,后续加减乘除都能解决。<br>&nbsp;&nbsp;&nbsp;&nbsp;看似很完美的解决了问题，但是实际上,这种存储方式造成了空间浪费。如果为$f(x)=3\cdot x^{5000}+2\cdot x+5$,这个数组长度为5001,但是其中有4998个0,这些存储没有任何意义。采用元组tuple的方式能够解决这个问题。将数据存储为如$[(3,5000),(2,1),(5,0)]$,其中元组的首项代表系数,元组的第二项代表次方,这个方法在多项式的存储中还比较常用。<br>&nbsp;&nbsp;&nbsp;&nbsp;数组和Python列表可用于实现许多不同的抽象数据类型。它们都以线性顺序存储数据,并能够轻松访问数据。<strong>但是使用数组和Python列表有几个缺点</strong>。<strong>首先</strong>,插入和删除操作通常需要移动物品以腾出空间或缩小差距。这个可能很耗时,特别是对于大型序列。<strong>第二</strong>,数组的大小是固定的,不能改变。虽然Python列表确实提供了可扩展的数组,但是扩展需要创建一个新的数组并且必须复制扩增后原始数组的元素。<strong>最后</strong>,无论数组大小如何,数组的元素都存储在连续的内存字节中,每次创建数组时,程序必须找到并分配足够大的内存块来存储整个数组。对于大型阵列,程序很难或不可能找到一块内存。对于Python列表,情况尤其如此,由于每次扩展需要,在程序执行期间会变大更大的内存块。</p>
<h3 id="1-2链式存储结构"><a href="#1-2链式存储结构" class="headerlink" title="1.2链式存储结构"></a>1.2链式存储结构</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;除了数组来存储之外,还有链式结构来存储。链表数据结构,是一种通用结构,可用于按顺序存储数据。链表改进了数组和Python列表的构造和管理,要求更小的内存分配,并且插入和删除没有元素移位。但是它确实消除了数组和Python列表可用的直接元素访问。因此,它不适合每个数据存储问题。链表有多种：单链表是一种线性结构,其中遍历从前面开始,一次一个元素,直到结束。其他变体包括循环链接,双链接和循环双链表。<br>&nbsp;&nbsp;&nbsp;&nbsp;所以,除了数组来存储之外,还有链式结构来存储。链表中每个结点存储多项式中的一个非零项,包括系数和指数两个数据域以及指针域。</p>
<pre><code>class PolyNode:
    def __init__(self,coef,expon,nextnode=None):
        self.coef = coef
        self.expon = expon
        self.nextnode = nextnode
</code></pre><p><img src="/../../images/Python/DataStructureAlgorthmLlinear/SingleLink.png" alt="Single Link">存储的结构如上图所示,第一个称之为头结点(head),最后一个称之为尾结点(tail)。</p>
<hr>
<h2 id="2-线性表"><a href="#2-线性表" class="headerlink" title="2.线性表"></a>2.线性表</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;对于刚刚的多项式问题,让我们了解了线性表的不同存储方法。什么是线性表呢？线性表(Linear List)由 <strong>同类型的数据元素</strong>构成的 <strong>有序</strong>序列的线性结构。线性表的抽象数据类型可由如下的方式描述：</p>
<pre><code>数据名称：线性表(List)
数据对象集：n个元素组成的有序性序列
操作集：L为List型,整数i表示位置,元素x为ElementType
    1.List MakeEmpty()：初始化一个空线性表
    2.ElementType FindKth(int K,List L):根据位序K,返回相应的元素
    3.Find(ElementType X,List L):在线性表L中查找X的第一次出现的位置
    4.Insert(ElementType X,int i,List L):在位序i前插入一个新元素X到L中
    5.Delete(int i,List L):删除指定位序i的元素
    6.Length(List L):返回线性表L的长度n
</code></pre><h3 id="2-1-线性表顺序存储实现"><a href="#2-1-线性表顺序存储实现" class="headerlink" title="2.1 线性表顺序存储实现"></a>2.1 线性表顺序存储实现</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;线性表的顺序存储结构非常好理解,利用数组的连续存储空间顺序的存放线性表的各元素,这里就简单介绍下想法。<br>1)建立空表肯定是没问题的。<br>2)对于FindKth查找问题,需要遍历一遍,如果在到达数据末尾前能找到数据,那就返回数据,否则返回-1。<br>3)对于Find问题,循环找到元素就返回位置,如果到末尾了还没找到,就返回-1.<br>4)对于Insert插入问题,要麻烦一点,首先从最后一个元素开始右移一位,依次进行到下标为$i-1$的元素,然后将数据插入到下标为$i-1$的位置。注意在开始前判断表是否满了以及位置合法不合法。<br>5)对于Delete删除问题同理,找到下标为$i-1$的删除,然后从下标$i$开始每个向前移一位,同样开始前判断表是否满了以及位置合法不合法。<br>6)对于返回长度就不用多说了。</p>
<h3 id="2-2-线性表链式存储实现"><a href="#2-2-线性表链式存储实现" class="headerlink" title="2.2 线性表链式存储实现"></a>2.2 线性表链式存储实现</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;通过上述引言,了解了什么是链式表后,接下来进行链式表的存储实现。创建表就不必说了,直接创建一个对象就好。</p>
<h4 id="2-2-1-FindKth-int-K-List-L"><a href="#2-2-1-FindKth-int-K-List-L" class="headerlink" title="2.2.1 FindKth(int K,List L)"></a>2.2.1 FindKth(int K,List L)</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;根据位序K,返回相应的元素问题,那么就需要遍历了。因为链式表他只能从前面找到后面,因此采用计数的方式来寻找就好。但是如果找到了末尾了,还是没找到,同样返回None.</p>
<pre><code>def FindKth(int K,List L):
    List p=L
    int i =1
    while p.nextnode!=None and i&lt;K:
        p = p.nextnode
        i+=1
    if i==K: return p  #找到就返回这个节点
    else: return None  #没找到就返回空
</code></pre><h4 id="2-2-2-Find-ElementType-X-List-L"><a href="#2-2-2-Find-ElementType-X-List-L" class="headerlink" title="2.2.2 Find(ElementType X,List L)"></a>2.2.2 Find(ElementType X,List L)</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;在线性表L中查找X的第一次出现的位置,跟上述差不多,只是循环结束的条件不同。</p>
<pre><code>def Find(ElementType X,List L):
    List p=L
    while p.nextnode!=None and p.coef!=X:
        p = p.nextnode
    return p
</code></pre><h4 id="2-2-3-Insert-ElementType-X-int-i-List-L"><a href="#2-2-3-Insert-ElementType-X-int-i-List-L" class="headerlink" title="2.2.3 Insert(ElementType X,int i,List L)"></a>2.2.3 Insert(ElementType X,int i,List L)</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;在位序i前插入一个新元素X到L中,相当于是在位序第i-1个结点的后面插入一个新的结点。那么思路上应该<br>1)构建一个结点s.<br>2)找到链表的第i个结点,s指向这个结点。<br>3)第i-1结点再指向s.但是这里要注意是否为表头以及插入的位置是否合理。</p>
<pre><code>def Insert(ElementType X,int i,List L):
    if i==1:
        s = PolyNode()
        s.coef =X
        s.nextnode = L
        return s
    p = FindKth(i-1,L)
    if p==None:
        print(&quot;参数i错误,请自查&quot;)
        return None
    else:
        s = PolyNode()
        s.coef =X
        s.nextnode = p.nextnode
        p.nextnode = s
        return L
</code></pre><h4 id="2-2-4-Delete-int-i-List-L"><a href="#2-2-4-Delete-int-i-List-L" class="headerlink" title="2.2.4 Delete(int i,List L)"></a>2.2.4 Delete(int i,List L)</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;删除指定位序i的元素,需要以下几个步骤:<br>1)先找到链表的第i-1个结点P.<br>2)用s指向P的下一个结点(要删除的这个点).<br>3)让p指向要s指向的结点,然后释放s的空间。</p>
<pre><code>def Delete(int i,List L):
    if i==1:
        s = L
        if L!=None: L=L.nextnode 
        else: return None
        del s
        return L
    p = FindKth(i-1,L)
    if p==None:
        print(&quot;第%d个结点不存在&quot;%i-1)
        return None
    elif: 
        p.nextnode = None
        print(&quot;第%d个结点不存在&quot;%i)
    else:
        s = p.nextnode
        p.nextnode = s.nextnode
        del s
        return L
</code></pre><h3 id="2-3-线性存储和链式存储如何选择"><a href="#2-3-线性存储和链式存储如何选择" class="headerlink" title="2.3 线性存储和链式存储如何选择"></a>2.3 线性存储和链式存储如何选择</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;讲完了线性存储和链式存储,肯定会有疑惑,他们都为线性表,有非常多的共同点,但是在使用的时候应该如何抉择呢？在<a href="https://jaryhuang.github.io/2018/09/22/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%92%8C%E7%AE%97%E6%B3%95/" target="_blank" rel="noopener"><strong>数据结构和算法基本概念</strong></a>对于算法的评价从空间复杂度和时间复杂度出发。<br>&nbsp;&nbsp;&nbsp;&nbsp;从 <strong>空间复杂度</strong>来说,链式存储更加自由,不需要提前开辟很多的空间,而线性存储相当于提前开了一篇空间,虽然能append,但是实际上还是在一个固定的内存空间内操作。而链式存储是增加一个申请一个,比较灵活。<br>&nbsp;&nbsp;&nbsp;&nbsp;从 <strong>时间复杂度</strong>来说,如下图所示：</p>
<p><img src="/../../images/Python/DataStructureAlgorthmLlinear/LinkListAndNormalListCompare.png" alt="Single Link">因此,如果有频繁的增加或者删除的操作的话建议利用链式结构来存储。</p>
<h2 id="3-堆栈-Stack"><a href="#3-堆栈-Stack" class="headerlink" title="3.堆栈(Stack)"></a>3.堆栈(Stack)</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;堆栈是具有一定操作约束的线性表,只在一端(栈顶)做插入(入栈Push)、删除(出栈Pop).它的一个最大的特点就是后入先出(LIFO Last In First Out),如下图所示:</p>
<p><img src="../images/DataStructure+Algorthm/linear list/stack.png" alt="Stack">堆栈的抽象数据类型有如下描述方式：</p>
<pre><code>    数据类型：堆栈(Stack)
    数据对象集：一个有0个或者多个元素的有穷线性表
    操作集：长度为MaxSize的堆栈S,堆栈元素item是ElementType
        1.Stack CreateStack(int MaxSize):生成空堆栈,其最大长度为MaxSize
        2.int IsFull(Stack S,int MaxSize):判断堆栈S是否已满
        3.Push(Stack S,ElementType item):将元素item压入堆栈
        4.IsEmpty(Stack S):判断堆栈是否为空
        5.Pop(Stack S):删除并返回栈顶元素
</code></pre><h3 id="3-1-线性表顺序存储实现堆栈"><a href="#3-1-线性表顺序存储实现堆栈" class="headerlink" title="3.1 线性表顺序存储实现堆栈"></a>3.1 线性表顺序存储实现堆栈</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;实现堆栈,首先想到的就是顺序存储,用Python的List来实现的话,相对是比较简单的。堆栈的出栈和入栈都是从List的尾巴进行操作,注意出栈的时候要判断是否为空栈。</p>
<pre><code>class Stack :
    #Creates an empty stack.
    def __init__( self ):
        self._theItems = list()

    # Returns True if the stack is empty or False otherwise.
    def isEmpty( self ):
        return len( self._theItems ) == 0

    # Returns the number of items in the stack.
    def __len__ ( self ):
        return len(self._theItems)

    # Removes and returns the top item on the stack.
    def pop(self):
        assert not self.isEmpty(self),&quot;Cannot peek at an empty stack&quot;
        return self._theItems.pop()

    # Push an item onto the top of the stack.
    def push( self, item ):
        self._theItems.append( item )
</code></pre><p>&nbsp;&nbsp;&nbsp;&nbsp;线性表实现堆栈操作,Pop和Push在时间复杂度上是$O(n)$,因为在最糟糕的情况下,用于实现Python列表的底层数组可能必须重新分配以适应顶部堆栈项的添加或删除。其他操作的复杂度都是$O(1)$</p>
<h3 id="3-2-线性表链式存储实现堆栈"><a href="#3-2-线性表链式存储实现堆栈" class="headerlink" title="3.2 线性表链式存储实现堆栈"></a>3.2 线性表链式存储实现堆栈</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;基于Python列表的实现可能不是堆栈的最佳选择,因为有大量的Push和Pop操作。list操作每个append()和pop()需要重新分配用于实现的底层数组列表。如果利用链表来实现可以避免内存空间的重新分配。但是,当用链表存储的时候,到底是用Tail出呢还是Head.对于链表来说,对Head的操作相对容易很多,因此将堆栈后进来的看成Head.</p>
<pre><code>class Stack:
    def __init__(self):
        self._top = None
        self._size = 0

    def IsEmpty(self):
        return self._top is None

    def __len__(self):
        return self._size

    def Pop(self):
        node = self._top
        self._top = self._top.next
        self._size -= 1
        return node.item

    def Push(self):
        self._top = _StackNode(item,self._top)
        self._size += 1

class _StackNode:
    def __init__(self,item,link):
        self.item = item
        self.next = link
</code></pre><p>&nbsp;&nbsp;&nbsp;&nbsp;利用链式存储方法,堆栈的所有操作的复杂度都是$O(1)$.</p>
<h2 id="4-队列-Queue"><a href="#4-队列-Queue" class="headerlink" title="4.队列(Queue)"></a>4.队列(Queue)</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;队列也是具有一定操作约束的线性表,只在一端做插入(入队列 AddQ)、删除(出队列 Delete).它的一个最大的特点就是先入先出(FIFO First In First Out)</p>
<p><img src="/../../images/Python/DataStructureAlgorthmLlinear/Queue.png" alt="Queue"><br>队列的抽象数据类型有如下描述方式：</p>
<pre><code>    数据类型：队列(Queue)
    数据对象集：一个有0个或者多个元素的有穷线性表
    操作集：长度为MaxSize的队列Q,队列元素item是ElementType
        1.Queue CreateQueue(int MaxSize):生成空队列,其最大长度为MaxSize
        2.len(Queue Q):返回队列长度
        3.EnQueue(Queue Q,ElementType item):将元素item插入队列
        4.IsEmpty(Queue Q):判断队列是否为空
        5.DeQueue(Queue Q):删除并返回队头的元素
</code></pre><h3 id="4-1-利用Python-List实现队列"><a href="#4-1-利用Python-List实现队列" class="headerlink" title="4.1 利用Python List实现队列"></a>4.1 利用Python List实现队列</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;List来实现队列的话相对比较简单,主要就是append和pop.</p>
<pre><code>    class Queue:
        def __init__(self):
            self._qList = list()

        def isEmpty(self):
            return len(self._qList)==0

        def __len__(self):
            return len(self._qList)

        def EnQueue(self,item)：
            self._qList.append(item)

        def DeQueue(self):
            assert not self.isEmpty(),&quot;Cannot dequeue from an empty queue&quot;
            return self._qList.pop(0)
</code></pre><p>&nbsp;&nbsp;&nbsp;&nbsp;使用List来创建队列,在入队和出队在时间复杂度上$O(n)$,当数据量增大的时候,其非常受限。</p>
<h3 id="4-2-利用Circular-Array实现队列"><a href="#4-2-利用Circular-Array实现队列" class="headerlink" title="4.2 利用Circular Array实现队列"></a>4.2 利用Circular Array实现队列</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;循环数组是在数组的基础上将头尾相连而成。具体结构如下图所示：</p>
<p><img src="/../../images/Python/DataStructureAlgorthmLlinear/CircularArray.png" alt="Circular Array"></p>
<pre><code>from array import Array

class Queue:
    def __init__(self,maxSize):
        self._count = 0
        self._front = 0
        self._back = maxSize - 1
        self._qArray = Array(maxSize)

    def isEmpty(self):
        return self._count == 0

    def isFull(self):
        return self._count == len(self._qArray)

    def __len__(self):
        return self._count

    def enqueue(self,item):
        assert not self.isFull(), &quot;Cannot enqueue to a full queue&quot;
        maxSize = len(self._qArray)
        self._back = (self._back+1)%maxSize
        self._qArray[self._back] = item
        self._count += 1

    def dequeue(self):
        assert not self.isEmpty(), &quot;Cannot dequeue from an empty queue.&quot;
        item = self._qArray[self._front]
        maxSize = len(self._qArray)
        self._front = (self._front+1)%maxSize
        self._count -= 1
        return item
</code></pre><p>&nbsp;&nbsp;&nbsp;&nbsp;利用Circular Array来创建队列,所有的操作在时间复杂度上$O(1)$,但是他有一个缺点就是最大能力受到了限制,跟初始化的maxSize有关。</p>
<h3 id="4-3-利用链表来实现队列"><a href="#4-3-利用链表来实现队列" class="headerlink" title="4.3 利用链表来实现队列"></a>4.3 利用链表来实现队列</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;用List来实现队列的主要缺陷就是在入队和出队的时间复杂度上,而循环数组能够解决这个问题,但是自身又带来新的问题：有限的队列尺寸。一个最好的解决方案就是使用链表的方式,并且利用头(head)和尾(tail)参考点：</p>
<p><img src="/../../images/Python/DataStructureAlgorthmLlinear/LinkList.png" alt="Link List"></p>
<pre><code>class Queue:
    def __init__(self,maxSize):
        self._qhead = None
        self._qtail = None
        self._count = 0

    def isEmpty(self):
        return self._qhead is None

    def __len__(self):
        return self._count

    def enqueue(self,item):
        node = _QueueNode(item)
        if self.isEmpty():
            self._qhead = node
        else:
            self._qtail.next = node
        self._qtail = node
        self._count += 1

    def dequeue(self):
        assert not self.isEmpty(), &quot;Cannot dequeue from an empty queue.&quot;
        node = self._qhead
        if self._qhead is self._qtail
            self._qtail = None
        self._qhead = self._qhead.next
        self._count -=1
        return node.item
class _QueueNode(object):
    def __init__(self,item):
        self.item = item
        self.next = None
</code></pre><h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5.总结"></a>5.总结</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;本文依据课程的第二章节进行了简单的梳理,主要是了解数据结构中的线性表以及其实现方式。学习了线性表的存储方式包括线性存储和链式存储,实现了线性表的一些基本操作功能。通过上述学习,我们还深入学习了堆栈和队列这两个具有一定操作约束的线性表。在接下来的编程实践中尽量利用这些方法来解决一些问题。另外,本次还参考了《Data Structures and Algorithms using Python》这本书,感觉非常的好用,推荐。</p>
<h2 id="6-参考"><a href="#6-参考" class="headerlink" title="6.参考"></a>6.参考</h2><p>1.中国大学MOOC课程《数据结构》——浙江大学<br>2.Data Structures and Algorithms using Python ——Rance D. Necaise</p>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Data structure and Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title>数据结构和算法----基本概念</title>
    <url>/2018/09/22/Python/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%92%8C%E7%AE%97%E6%B3%951-%E6%A6%82%E5%BF%B5/</url>
    <content><![CDATA[<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1.引言"></a>1.引言</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;有人说,你学会了编程相当于你学会了砌砖;你了解了数据结构,你会建造一座房子;你明白了算法,才能快速有效的建成这套房子。现在我才刚进入数据结构和算法的学习阶段,因此还无法对这段话进行评价,希望在整个课程结束之后,我能够重新再来理解下这句话。<br><a id="more"></a></p>
<hr>
<h2 id="2-数据结构"><a href="#2-数据结构" class="headerlink" title="2.数据结构"></a>2.数据结构</h2><h3 id="2-1-解决问题方法的效率-跟数据的组织方式有关"><a href="#2-1-解决问题方法的效率-跟数据的组织方式有关" class="headerlink" title="2.1 解决问题方法的效率,跟数据的组织方式有关"></a>2.1 解决问题方法的效率,跟数据的组织方式有关</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;了解数据结构,首先从一个我们生活中耳熟能详的例子开始,<strong>如何在书架上摆放图书问题</strong>:对于图书的摆放,有两个关键的问题1)插入:如何插入新书。2)寻找：如何找到指定的书。<br><strong>方法一：随便放(放容易查找难)</strong>,随便放的方式对于放的问题,是很容易的,有空的位置就直接放书。但是对于查找的问题,这就复杂了(遍历所有的书找过去)<br><strong>方法二：按字母顺序(放难查找容易)</strong>,这种方式对于新书放的问题比较复杂了。假设只有一层的话,首先得查找到新放的书的位置,每次放还需要挪动所有的书。但是对于查找的话容易了许多,利用二分法进行查找。<br><strong>方法三：分类+按字母顺序(折中)</strong>,这种方式对于找书问题,先定类别,然后再二分查找。对于新书插入时,一样是先定类别,然后二分查找确定位置,只是多出了一步叫移出空位(当然移动是必须移动的,优秀的算法就是移动很少的量)。<br>&nbsp;&nbsp;&nbsp;&nbsp;以上三种方式,大家也清楚现在书店和图书馆普遍用的就是第三种方式,但是这里也有几个问题可以进行讨论的：<strong>类别应该分多细?</strong>太细的话相当于没分了,100本书分100类没有任何意义。第二个问题：<strong>每个类别应该分配给它多少的空间?</strong>图书馆或者书店总的面积是有限,每个类别的书他的数量本身就是不一样的,以及每个月增加的书肯定是有部分多有部分少。如果每个类别都均分,就会导致有些类别的书没地方放,有些类别的书架一直空空如也。所以要解决放图书问题要注意组织方式。</p>
<h3 id="2-2-解决问题方法的效率-跟空间的利用效率相关"><a href="#2-2-解决问题方法的效率-跟空间的利用效率相关" class="headerlink" title="2.2 解决问题方法的效率,跟空间的利用效率相关"></a>2.2 解决问题方法的效率,跟空间的利用效率相关</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;<strong>问题：写一个程序实现传入一个正整数N,能顺序的打印从1到N的全部正整数?</strong><br>对于这么一个问题,首先简单的就是循环实现(Python语言)：</p>
<pre><code>def PrintN(N):
    for i in range(N):
        print(i)
if __name__ == &#39;__main__&#39;:
    PrintN(10000)
</code></pre><p>给定10000,实现的结果如图所示：<img src="/../../images/Python/DataStructureAlgorthmConcept/1.png" alt="Alt text"><br>还有一种方式是采用递归的方法实现：</p>
<pre><code>def PrintN2(N):
    if N:
        PrintN2(N - 1)
        print(N)
if __name__ == &#39;__main__&#39;:
    PrintN2(10000)
</code></pre><p>实现的结果如图所示：<img src="/../../images/Python/DataStructureAlgorthmConcept/2.png" alt="Alt text"><br>&nbsp;&nbsp;&nbsp;&nbsp;两者在给定10、100等都能打印,但是到了10000之后,循环打印一切如常,但是递归的方式就打印不出来了。这原因相当好理解,当利用循环方式的时候,我们的i只是一个临时变量,他一直占着计算机中的一个坑,只是这个数字进进出出,坑内的数据在变化而已。但是对于递归算法则不一样了。递归算法首先先储存,来了一个N储存一个,然后直到1的时候。接下来返回打印1到N。随着N的增大,其储存的空间也就越来越大了,会导致存储爆掉。</p>
<h3 id="2-3-解决问题方法的效率-跟算法的巧妙程度有关"><a href="#2-3-解决问题方法的效率-跟算法的巧妙程度有关" class="headerlink" title="2.3 解决问题方法的效率,跟算法的巧妙程度有关"></a>2.3 解决问题方法的效率,跟算法的巧妙程度有关</h3><p><strong>问题：写程序计算给定多项式在给定x处的值,其中<script type="math/tex">f(x)=\sum_{i=0}^n{i*x^{(i)}}</script></strong><br>对于多项式累加和问题,我们第一个想到的是pow函数,可以算次方:</p>
<pre><code>import numpy as np 
def f1(a,x):
    p = a[0]
    for i in range(1,len(a)):
        p+=np.pow(x,i)
    return p 
</code></pre><p>另外一种方式,提取x:</p>
<pre><code>def f2(a, x):
    p = a[-1]
    for i in range(len(a) - 1, 0, -1):
        p = x * p + a[i - 1]
    return p
</code></pre><p>&nbsp;&nbsp;&nbsp;&nbsp;由于当数据小的时候跑的飞快,两者基本没什么差别。但是当我们的数据量大的时候,就很容易看出问题了,设n=100000,下面两张图是两种算法跑的时间。f2的方法比f1的方法快了20倍由于,相差一个数量级了。随着n的继续变大,两者的时间相差会更大,因此算法对问题的解决影响很大。<br>下图是f1方法得到的结果,用了14秒钟计算。<img src="/../../images/Python/DataStructureAlgorthmConcept/3.png" alt="Alt text">f2方法得到的结果,用了0.7秒钟计算。<br><img src="/../../images/Python/DataStructureAlgorthmConcept/4.png" alt="Alt text"><strong>到底什么是数据结构呢？</strong><br>数据结构,是指数据对象在计算机中的组织方式,组织方式包含着：<br>1)逻辑结构：<br><strong>线性结构</strong>：以书店为例子,书店的书假设看成都放在架子一层上面,给每本书一个编号,那么对于每一本书来说,它前面只有一本书,后面只有一个书,每个编号对应的唯一一本书,这就是 <strong>一对一</strong>的关系,这称之为线性结构。</p>
<p><strong>树</strong>：另外一个组织方式,如果按照类别进行放书(第三种方法),给每一类进行编号,那么每个编号会对应着一堆书,这是 <strong>一对多</strong>的关系,称之为树结构。</p>
<p><strong>图</strong>：在书店里,假设还需要统计一些数据,比如这本书有哪些人买过,买过这本书的人又买过其他什么书。其实就是一本书对应着很多人,一个人又对应着很多书,这就是一个很复杂的关系网,一种 <strong>多对多</strong>的关系,称之为图的结构。<br>2)物理存储结构：<br>&nbsp;&nbsp;&nbsp;&nbsp;数据对象在计算机里面的存储方式,是连续的放还是东一个西一个放,到底是数据的形式还是链表的形式,这就是物理存储结构。<br>&nbsp;&nbsp;&nbsp;&nbsp;描述数据结构有一个很好的方法很好的方法叫 <strong>抽象数据类型(Abstract Data Type)：</strong>这里有两个概念1)数据类型:数据对象集(是什么东西)。2)数据集合相关联的操作集。如果理解面向对象的语言(如C++)你会发现类,其有类的对象以及操作,并且封装在一个类里面。2)抽象：抽象是指描述数据类型的方法不依赖于具体实现(与存放数据的机器,数据存储的物理结构,实现操作的算法和变成语言等无关),只描述数据对象集和相关操作 <strong>是什么</strong>,并不涉及 <strong>如何做到</strong>。<br>大家可以对着这个矩阵的抽象数据集类型定义来进行理解。<br><img src="/../../images/Python/DataStructureAlgorthmConcept/5.jpg" alt="Alt text"><strong>抽象到底有什么好处呢？个人理解,抽象可以在整体上把握架构,避免做重复性的工作,也就是通用性(或者说适用性广)</strong>。</p>
<h2 id="3-算法"><a href="#3-算法" class="headerlink" title="3.算法"></a>3.算法</h2><h3 id="3-1-什么是算法"><a href="#3-1-什么是算法" class="headerlink" title="3.1 什么是算法"></a>3.1 什么是算法</h3><p>算法,是对一系列数据对象的操作所用的方法,其有一些特征：<br>&nbsp;&nbsp;1)一个有限的指令集<br>&nbsp;&nbsp;2)接受一些输入(有些情况下不需要输入)<br>&nbsp;&nbsp;3)产生输出<br>&nbsp;&nbsp;4)有限步骤之后终止<br>&nbsp;&nbsp;5)每一条指令明确(明确的定义、计算机可处理范围内、不依赖于任何一种语言与具体实现手段)</p>
<h3 id="3-2-算法的评价标准"><a href="#3-2-算法的评价标准" class="headerlink" title="3.2 算法的评价标准"></a>3.2 算法的评价标准</h3><p>评价算法的好坏从两个指标出发：<strong>空间复杂度S(n)</strong>和 <strong>时间复杂度T(n)</strong>。</p>
<h4 id="3-2-1-空间复杂度S-n"><a href="#3-2-1-空间复杂度S-n" class="headerlink" title="3.2.1 空间复杂度S(n)"></a>3.2.1 空间复杂度S(n)</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;算法的空间复杂度是指根据算法程序在执行时占用的存储单元的长度,一般与输入数据的规模有关。空间复杂度过高的算法可能导致使用的内存超限,造成程序的非正常中断,如2.2中的递归PrintN函数所示,其空间复杂度$S(N) = C*N$,与输入的N成正比。</p>
<h4 id="3-2-2-时间复杂度T-n"><a href="#3-2-2-时间复杂度T-n" class="headerlink" title="3.2.2 时间复杂度T(n)"></a>3.2.2 时间复杂度T(n)</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;时间复杂度是根据算法写成的程序在执行时耗费时间的长度,与输入数据的规模有关。时间复杂度低效的算法可能会导致我们等不到运行结果。如对2.3中的累加问题(由于计算机计算乘除的速度比加减慢的多,因此可以大致估算乘除所需的时间来估计),pow函数的乘法次数为$\sum_{i=0}^n{i=\frac{n^2+n}{2}}$,因此时间复杂度<script type="math/tex">T(n) = C_1*n^2+C_2*n</script>,而对于2.3中f2的运算方式,时间复杂度$T(n) = C*n$</p>
<h4 id="3-2-3-复杂度的渐进表示"><a href="#3-2-3-复杂度的渐进表示" class="headerlink" title="3.2.3 复杂度的渐进表示"></a>3.2.3 复杂度的渐进表示</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;对于复杂度来说,直接精确的很难获得,一般都采用渐进表示来代表,以T(n)为例：<br>1)$T(n) = O(f(n))$代表T(n)有上界<script type="math/tex">C*f(n)</script><br>2)$T(n) = \Omega(g(n))$代表T(n)有下界$C*g(n)$<br>3)$T(n) = \theta(h(n))$代表T(n)又有上界又有下界。<br>下图是常见的几个复杂度的函数：<br><img src="/../../images/Python/DataStructureAlgorthmConcept/6.jpg" alt="Alt text"><br><img src="/../../images/Python/DataStructureAlgorthmConcept/7.jpg" alt="Alt text"><br>从上图可以明显的看出来,对于$n^2$以上的这类复杂度随输入爆炸性增长,因此当设计算法的时候,如果复杂度为$n^2$,则可以考虑降到$nlogn$等。</p>
<h2 id="4-算法应用实例"><a href="#4-算法应用实例" class="headerlink" title="4.算法应用实例"></a>4.算法应用实例</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;最大子列问题：给定N个整数的序列$\{A_1,A_2,…,A_N\}$,求函数$f(i,j) = max\{0,\sum_{k=i}^j{A_k}\}$的最大值。</p>
<h3 id="4-1-计算所有的子序列和"><a href="#4-1-计算所有的子序列和" class="headerlink" title="4.1 计算所有的子序列和"></a>4.1 计算所有的子序列和</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;第一种方法是最开始想到的,如果一个序列,知道左边界,知道右边界。接下来从左边界加到右边界,把所有子序列的和都求出来做比较。</p>
<pre><code>def MaxSubseqfunc_1(seq):
    max_sum = 0
    for i in range(0, len(seq)):
        for j in range(i, len(seq)):
            seq_sum = 0
            for k in range(i, j):
                seq_sum += seq[k]
            if(seq_sum &gt; max_sum):
                max_sum = seq_sum
    return max_sum
</code></pre><p><img src="/../../images/Python/DataStructureAlgorthmConcept/8.png" alt="Alt text"><br>&nbsp;&nbsp;&nbsp;&nbsp;这个算法容易的算出其复杂度,三个循环复杂度$T(n)=C*n^3$由于电脑性能比较好,所以怕看不出效果,所以总的循环了100000次。<br>&nbsp;&nbsp;&nbsp;&nbsp;对于上述方法,其实理解下,在每次计算的时候,都是从左计算到右边,比如$A_1,…,A_4$和$A_1,…,A_5$,多次的重复计算。因此进行了改进：</p>
<pre><code>def MaxSubseqfunc_2(seq):
    max_sum = 0
    for i in range(0, len(seq)):
        seq_sum = 0
        for j in range(i, len(seq)):
            seq_sum += seq[j]
            if(seq_sum &gt; max_sum):
                max_sum = seq_sum
    return max_sum
</code></pre><p><img src="/../../images/Python/DataStructureAlgorthmConcept/9.png" alt="Alt text">&nbsp;&nbsp;&nbsp;&nbsp;这个算法2个循环,复杂度$T(n)=C*n^2$基于上述改进,从运行时间上看,减少了1/3有余,当参数更大的时候,效果应该会更加明显。</p>
<h3 id="4-2-分治法"><a href="#4-2-分治法" class="headerlink" title="4.2 分治法"></a>4.2 分治法</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;上述方法虽然已经实现了功能,但是复杂度是$T(n)=C*n^2$,随着n的增大还是会更加复杂,得想办法将其继续降低。在这里运用分治法来进行处理。分治法的核心是“分而治之”：首先是分,将数据进行切分,一长段序列分为N段小序列。基于分成的N段小序列,应用同样的算法来进行治(递归)。<br><img src="/../../images/Python/DataStructureAlgorthmConcept/11.jpg" alt="Alt text">分治法的原理以及复杂度计算,最后的NlogN是将K=logN带入计算得到的。我们编写了代码进行了测试：</p>
<pre><code>def max3(A, B, C):
        # 返回三个数中的最大值
    return max(A, B, C)

def DivedeAndConquer(seq, left, right):
    &#39;&#39;&#39;
    分治法求List[left]到List[right]的最大子列和
    int MaxLeftSum, MaxRightSum
    存放左右子问题的解
    int MaxLeftBorderSum, MaxRightBorderSum
    存放跨分界线的结果
    int LeftBorderSum, RightBorderSum
    int center, i
    &#39;&#39;&#39;
    if left == right:
     # 递归终止条件,子列只有一个数字
        if seq[left] &gt; 0:
            return seq[left]
        else:
            return 0
    #-----分的过程------
    # 找到中分点
    center = int((left + right) / 2)
    # 递归求得两边子列的最大和
    MaxLeftSum = DivedeAndConquer(seq, left, center)
    MaxRightSum = DivedeAndConquer(seq, center + 1, right)
    # 求跨分界线的最大子列和
    MaxLeftBorderSum = 0
    LeftBorderSum = 0
    # 从中线向左扫描
    for i in range(center, left - 1, -1):
        LeftBorderSum += seq[i]
        if LeftBorderSum &gt; MaxLeftBorderSum:
            MaxLeftBorderSum = LeftBorderSum
    # 左边扫描结束
    MaxRightBorderSum = 0
    RightBorderSum = 0
    for i in range(center + 1, right + 1, 1):
        # 从中线开始扫描
        RightBorderSum += seq[i]
        if(RightBorderSum &gt; MaxRightBorderSum):
            MaxRightBorderSum = RightBorderSum
    # 右边扫描结束
    # 下面返回“治”的结果
    return max3(MaxLeftSum, MaxRightSum, MaxLeftBorderSum + MaxRightBorderSum)

def MaxSubseqSum3(seq):
    # 保持和前两种算法相同的函数接口
    return DivedeAndConquer(seq, 0, len(seq) - 1)
</code></pre><p>基于分治法的代码编写已经增加了注释了,因此大家可以对照着来进行看以及理解。接下来同样运行100000次,运行结果如图所示：<br><img src="/../../images/Python/DataStructureAlgorthmConcept/10.png" alt="Alt text">可见在时间上是有明显的减少的。</p>
<h3 id="4-3-在线处理"><a href="#4-3-在线处理" class="headerlink" title="4.3 在线处理"></a>4.3 在线处理</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;对于最大子列和问题,还有一个在线处理的方式。称之为在线处理是指每输入一个数据就进行即时处理,在任何一个地方终止输入,算法都能正确给出当前的解。</p>
<pre><code>def MaxSubseqSum4(seq):
    ThisSum = MaxSum = 0
    for i in range(len(seq)):
        #向右累加
        ThisSum+=seq[i]
        #发现更大和则更新当前结果   
        if ThisSum&gt;MaxSum:
            MaxSum = ThisSum
        elif ThisSum &lt;0：
            #如果子列和负,则不可能使后面的部分和增大,抛弃
            ThisSum=0
    return MaxSum
</code></pre><p>从代码上我们可以清楚的看到这个算法的复杂度为N(就一个循环)。运行结果如图所示,这应该是目前为止最快的算法了,100000次采用了0.3秒。<br><img src="/../../images/Python/DataStructureAlgorthmConcept/12.png" alt="Alt text"></p>
<h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5.总结"></a>5.总结</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;本文主要依据课程描述了数据结构和算法的概念,了解了数据结构的逻辑结构、物理存储结构以及其表示方式为抽象数据类型。而算法作为对一系列数据对象操作的方法,在设计的时候必须考虑其空间复杂度和时间复杂度。通过一个最大子列和的例子列举了常见的一些算法以及改进的策略。</p>
<h2 id="6-参考"><a href="#6-参考" class="headerlink" title="6.参考"></a>6.参考</h2><p>1.中国大学MOOC课程《数据结构》——浙江大学</p>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Data structure and Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title>VggNet</title>
    <url>/2018/09/01/DeepLearning/Classficaition/VggNet/</url>
    <content><![CDATA[<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1.引言"></a>1.引言</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;基于上一篇文章<a href="https://jaryhuang.github.io/2018/08/26/AlexNet/" target="_blank" rel="noopener">AlexNet</a>网络,我们了解其创新机制，以及发现这些机制有助于提升网络的精度，但是利用AlexNet训练cifar-10有一个显而易见的问题：其验证集精度并不是很高，在利用文中提出的trick之后网络虽能达到78%的精度，当然如果调节一些参数网络估计能达到80%的精度，但是对于视觉任务来说，这精度远远不够，因此最直接的方法就是对网络进行改进。<br>&nbsp;&nbsp;&nbsp;&nbsp;深度学习的网络改造有几种方式：1.从网络的各个层上进行改进，比如AlexNet中对网络中的激活函数进行改进。2.从网络宽度出发,通过各种的方式拓宽网络的宽度。3.从网络的深度出发，通过堆叠加深网络的深度。当然除了这几种方式还有从结构上进行完全创新的改进等。本次介绍的VGG网络就是从深度角度来对网络进行改进。<br><a id="more"></a></p>
<h2 id="2-VGG的网络结构"><a href="#2-VGG的网络结构" class="headerlink" title="2.VGG的网络结构"></a>2.VGG的网络结构</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;为了了解VGG网络，我们首先来看看VGG网络的框架：</p>
<p><img src="/../../../images/DeepLearning/Classfication/VggNet/1.png" alt="Alt text">&nbsp;&nbsp;&nbsp;&nbsp;这是作者文中提到的几个网络架构，其中比较有名的就是VGG-16(图中D)和VGG-19(图中E)。<br>&nbsp;&nbsp;&nbsp;&nbsp;<strong>VGG-16包含了13个卷积层和3个全连接层</strong><br>&nbsp;&nbsp;&nbsp;&nbsp;<strong>VGG-19包含了16个卷积层和3个全连接层</strong><br>&nbsp;&nbsp;&nbsp;&nbsp;VGG网络十分的简洁，其主要就是有3x3卷积层，ReLU非线性层再加2x2的池化层组成。图中的conv3-64代表的就是3x3的卷积核，通道数为64，其他依次同理。</p>
<h2 id="3-网络的创新点"><a href="#3-网络的创新点" class="headerlink" title="3.网络的创新点"></a>3.网络的创新点</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;VGG网络相较于AlexNet最大的改进点就在于加深了网络的深度和使用的非常小的感受野(3x3的卷积层)。VGG使用的3x3感受野,步长step=1的卷积层堆叠来代替AlexNet中的11x11,7x7,5x5的感受野。对于给定的感受野(与输出有关的输入图片的局部大小),采用堆积的小卷积核会优于采用大的卷积核,因为多层ReLU(每个卷积层后面都会跟着非线性层)可以增加网络深度来保证学习更复杂的模式，而且代价还比较小（参数更少）。简洁点说，就是在VGG网络中,利用2个3x3的卷积核代替5x5的卷积核,3个3x3的卷积核代替7x7的卷积核。这样做使得在保证具有相同感受野的情况下，通过堆叠小的卷积核自然而然的加深了网络的深度，紧跟着多引入的非线性层又能使得网络学习更强的非线性关系，同时在相同的效果下减少了网络的参数。<strong>但是为何堆叠的3x3卷积核堆叠能够代替5x5,7x7等的呢?同时其代价还比较小(参数少了多呢)?</strong> 以5x5的卷积核为例，5x5的卷积核可以看成5x5的全连接层在整张图上滑动。那么我们首先可以先用一个3x3卷积核卷积，然后再用一个3x3的卷积核对上述的结果进行全连接，这样加起来就相当于一个5x5的卷积核了。具体可以借用下图来进行一个说明：</p>
<p><img src="/../../../images/DeepLearning/Classfication//VggNet/2.jpg" alt="Alt text"><br>&nbsp;&nbsp;&nbsp;&nbsp;在上图中我们可以看到，当底下的5x5平面，首先利用一个3x3的步距为1的卷积核进行卷积，得到一个3x3的平面，我们再利用3x3的卷积核全连接，那么就得到一个值了。这与直接利用5x5的卷积层效果相当。同时，两个3x3的卷积核若中间不加入非线性层，其总能通过线性作用寻找到一个和5x5一样的效果。但在网络中实际上是加入了非线性层，为的是提高非线性表达，增强网络的学习能力。5x5的卷积核进行了解释，那么7x7的大家也就能直接推理了。<br>&nbsp;&nbsp;&nbsp;&nbsp;解决了上述为何能堆叠代替之后，剩下的一个问题就是，为何其参数会减少。这里利用作者论文中的计算方式来说明：假设利用3个3x3的卷积核代替7x7。为了简化，假设输入输出通道都为C,通过3个3x3的卷积层进行堆叠的参数数量为$3\times(3\times3\times C\times C)=27C^2$,利用单个7x7的卷积核产生的参数为$7\times7\times C\times C=49C^2$两者相比较，参数量少了很多。<br>&nbsp;&nbsp;&nbsp;&nbsp;在了解了上述网络的机制值后，我们需要测试下上述A-E的网络在CIFAR-10数据集上的效果。上述A-E的网络都是层层递进并且有一定的对比关系，本人也在此基础上对其进行验证。</p>
<h3 id="3-1-pre-train"><a href="#3-1-pre-train" class="headerlink" title="3.1 pre-train"></a>3.1 pre-train</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;深度学习的参数初始化是一门学问，如果参数初始化不好，可能导致算法收敛的局部最小值。当然，对于深度学习来说，陷入局部最小值很正常，但是真正要找的是如果找到一个极优的局部最小值。现在最常见的方法就是在大的数据集上进行初始化，然后将其的参数作为预训练结果，然后根据自己的数据集进行微调。这也称之为一种迁移学习，这里利用VGG-11网络来对cifar10进行训练，测试了零初始化以及在imgnet上的预训练后的模型。</p>
<p><img src="/../../../images/DeepLearning/Classfication/VggNet/4.png" alt="Alt text"><br>&nbsp;&nbsp;&nbsp;&nbsp;红色的曲线是零初始化的结果，蓝色的曲线是在imgnet上预训练初始化的结果。从图中清晰的看出，基于预训练的模型其损失在训练集上和验证集上在初始化时就比较小。在经过微调之后，预训练模型的损失达到了稳定，在验证集上也有很好的表现，保持在0.4左右。但是对于零初始化的模型，其从损失函数反应出模型存在了过拟合现象：训练集上损失持续减小，但是在验证集上先下降后上升。所以通过上述可知，经过预训练的参数初始化能够减小过拟合现象。<br>&nbsp;&nbsp;&nbsp;&nbsp;那么从错误率上来讲，如下图所示：</p>
<p><img src="/../../../images/DeepLearning/Classfication/VggNet/3.png" alt="Alt text">&nbsp;&nbsp;&nbsp;&nbsp;两者在训练集和验证集上的错误率趋势相同，都是减小。但是经过预训练的模型，其在验证集上的错误率较零初始化的模型又较大的减少，网络的精度达到93.6%左右，较零初始化的79%的精度有了非常大幅度的提升。因此，预训练模型对于网络拟合能力有极其重要的作用。</p>
<h3 id="3-2-网络深度的影响"><a href="#3-2-网络深度的影响" class="headerlink" title="3.2 网络深度的影响"></a>3.2 网络深度的影响</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;Vgg论文中提到了A-E多种模型，其中主要网络的深度为11,13,16,19，测试结果如图所示：</p>
<p><img src="/../../../images/DeepLearning/Classfication/VggNet/5.png" alt="Alt text"><br>&nbsp;&nbsp;&nbsp;&nbsp;从训练上来看，当都达到5%的error时，随着网络深度的增加，所需的时间也更长(epoch更大)，这是网络深度的增加使得参数量增大有关。从验证集上看，随着网络深度增加，验证集的精度也随着增加，特别是VGG-16和VGG-19,其相对有了明显的提升。究其原因，VGG16和VGG19其主要的改变是在后面堆叠了更多的3x3的滤波器，相当于增大了感受野，有助于提升网络的学习能力。</p>
<h2 id="4-总结"><a href="#4-总结" class="headerlink" title="4.总结"></a>4.总结</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;VGG网络作为AlexNet在深度方面的改进版，其网络结构在ILSVRC-2014 challenge的classification任务中拿到了第二名的好成绩，同时在目标定位方面拿到了第一名的成绩(这个在未来整理检测章节的时候再进行整理)。VGG具有很多优点，在结构方面非常简单，整个网络都使用相同的3x3卷积核以及2x2最大池化层,在参数数量方面，其用几个小滤波器(3x3)的卷积层的堆叠来代替大的滤波器，为模型参数的减少提供了很多的帮助。<br>&nbsp;&nbsp;&nbsp;&nbsp;VGG网络最大的贡献在于，其验证了不断加深网络的结构能够提升网络的性能。但网络结构的加深也带来更多的计算资源的需求，其计算主要来源于其全连接层。如图所示列出了VGG-16的各个层的参数数量:</p>
<p><img src="/../../../images/DeepLearning/Classfication/VggNet/6.png" alt="Alt text"><br>&nbsp;&nbsp;&nbsp;&nbsp;图中清晰的看出，VGG-16总共的参数为134301514，其中第一个全连接层参数为102764544，占了总参数的76.5%,而所有的全连接层加起来的参数为119586826，占总数的89.04%。因此卷积层实际上并不占用太多的参数资源，主要在全连接层上，因此后续有人研究利用卷积层来代替全连接层，其有很多的优势(这里不做叙述，未来研究中会继续进行说明)。</p>
<h2 id="5-参考"><a href="#5-参考" class="headerlink" title="5.参考"></a>5.参考</h2><p>1.<a href="https://mp.weixin.qq.com/s/vWuGW4iMD1MjVDZVCqH_FA" target="_blank" rel="noopener">一文读懂VGG网络</a><br>2.Simonyan K, Zisserman A. Very Deep Convolutional Networks for Large-Scale Image Recognition[J].Computer Science, 2014.</p>
]]></content>
      <categories>
        <category>DeepingLearning</category>
      </categories>
      <tags>
        <tag>Classfication</tag>
      </tags>
  </entry>
  <entry>
    <title>AlexNet</title>
    <url>/2018/08/26/DeepLearning/Classficaition/AlexNet/</url>
    <content><![CDATA[<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1.引言"></a>1.引言</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;自2012年AlexNet网络的出现,卷积神经网络得到了快速的发展,AlexNet也称之为现代意义上的深度卷积神经网络的开端。网络的结构如下图所示：</p>
<p><img src="/../../../images/DeepLearning/Classfication/AlexNet/1.png" alt="Alt text">&nbsp;&nbsp;&nbsp;&nbsp;AlexNet整体架构包含了8层结构,前5层是卷积层,后面接着3个全连接层。网络的输入图像是224x224x3的三通道彩图。<br>&nbsp;&nbsp;&nbsp;&nbsp;第一个卷积层是有96组11x11的卷积核,卷积步长stride=4,同时后面跟着LRN层,ReLU层以及pooling层。<br>&nbsp;&nbsp;&nbsp;&nbsp;第二个卷积层是由256组5x5大小的卷积核组成,分为两个group,每个group通道数为128,卷积步长stride=1,同时后面跟着LRN层,ReLU层以及pooling层。<br>&nbsp;&nbsp;&nbsp;&nbsp;第三层有384组3X3的卷积核,分为两个group,每个group通道数为192,卷积步长stride=1,后面跟着ReLU层。<br>&nbsp;&nbsp;&nbsp;&nbsp;第四层依旧有384组3X3的卷积核,分为两个group,每个group通道数为192,卷积步长stride=1,后面跟着ReLU层。<br>&nbsp;&nbsp;&nbsp;&nbsp;第五个卷积层是由256组3x3大小的卷积核组成,分为两个group,每个group通道数为128,卷积步长stride=1,同时后面跟着LRN层,ReLU层以及pooling层。<br>&nbsp;&nbsp;&nbsp;接下来是3个全连接层,第1第2个全连接层每一层都是分为两个group,输入输出都是(2048,2048),第三个全连接层输入输出是(4096,1000)。<br><a id="more"></a></p>
<h2 id="2-网络的创新点"><a href="#2-网络的创新点" class="headerlink" title="2.网络的创新点"></a>2.网络的创新点</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;如作者所说,AlexNet网络其提出了很多trick,而且在过程中他根据这些trick的重要性就行了验证。笔者根据作者的思路对这些trick进行了简单的验证,但是由于设备以及数据的原因,笔者没有按照作者的一模一样的思路以及方法进行训练处理,笔者利用的GPU是GTX1080TI,数据集采用的是cifar-10,数据量不是很大,有利于短时间内的验证。</p>
<h3 id="2-1-ReLU激活函数"><a href="#2-1-ReLU激活函数" class="headerlink" title="2.1 ReLU激活函数"></a>2.1 ReLU激活函数</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;之前的神经网络大多采用的是$f(x)=tanh(x)$或者$f(x)=(1+e^{-x})^{-1}$,但是这些非线性函数有一个显著的缺点：随着训练的增加,这些非线性函数的梯度会趋近于0导致学习的速率急速的下降,对深度学习的影响很大,作者利用了ReLU函数$f(x)=max(0,x)$。<img src="/../../../images/DeepLearning/Classfication/AlexNet/relu_train_loss.png" alt="Alt text">&nbsp;&nbsp;&nbsp;&nbsp;如上图所示是在分别利用激活函数ReLU和tanh训练网络的训练损失图,图中蓝色虚线是利用tanh激活函数训练的Training Loss,红色虚线是利用ReLU激活函数训练的Training Loss。从图中可以看出两者的趋势相同,但也明显的在训练时间上以及训练Loss的减少上,ReLU有明显的优势,利用ReLU激活函数只需要9个epoch后loss达到0.25的时候,而tanh需要14个epoch。究其原因是由于ReLU函数的特性：1)其线性抑制使得一些神经元不会被激活,部分神经元一直为0能够加快训练。2)其被激活部分X的导数恒为1,不会因为训练的增加和使得梯度为0或者消失,因此能够加速其训练。<br>同时,在训练误差方面如图所示：<img src="/../../../images/DeepLearning/Classfication/AlexNet/relu_error.png" alt="Alt text">&nbsp;&nbsp;&nbsp;&nbsp;从图中显而易见,ReLU激活函数相较于tanh函数在训练集和测试集的训练误差上有显著的减少。通过ReLU函数,在cifar-10数据集网络的误差降低了4%,效果非常显著。ReLU函数能够加快训练这个容易理解,为何其能帮助提高精度呢？这得以与上述提到的1,其使得一些神经元不被激活,输出值为0,使得网络参数稀疏化,起到了类似L1的正则化的效果,也在一定程度上防止了过拟合。</p>
<h3 id="2-2-Dropout机制"><a href="#2-2-Dropout机制" class="headerlink" title="2.2 Dropout机制"></a>2.2 Dropout机制</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;在机器学习中,如果模型的参数过多,而数据样本太少很容易造成过拟合现象。在训练神经网络的过程特别容易出现(通常神经网络的参数非常的多)。而常用的方法是在损失函数中加入惩罚项,在神经网络中,还有一种机制是Dropout。Dropout是hintion提出的,为了防止模型过拟合,Dropout可以作为一种trikc供选择。在hinton的论文摘要中指出,在每个训练批次中,通过忽略一半的特征检测器(让一半的隐层节点值为0),可以明显地减少过拟合现象。这种方式可以减少特征检测器间的相互作用,检测器相互作用是指某些检测器依赖其他检测器才能发挥作用。<br>&nbsp;&nbsp;同时,在传统的机器学习中有很多ensemble的方法,比如boosting,bagging,stacking等等。这些集成方法有助于减少网络的误差。但是其在深度神经网络中应用不是很方便,主要原因还是深度神经网络模型训练的周期比较长,可这样的代价太昂贵了。但是,dropout的出现给了深度神经网络提供了一个模型融合的方式。通过隐藏一些神经元,使得每次mini-batch训练的都是一个新的网络,最后相当于达到了模型融合的效果。<br>&nbsp;&nbsp;&nbsp;&nbsp;因此综上两点,理论上Dropout能够防止过拟合以及提高精度,根据AlexNet作者的说明,在全连接层中加入了dropout层,并使得probability=0.5,对比结果如图所示：<br><img src="/../../../images/DeepLearning/Classfication/AlexNet/dropout.png" alt="Alt text">&nbsp;&nbsp;&nbsp;&nbsp;在没引入dropout机制之前,网络训练的时候出现了一个问题：在训练集上损失函数以及错误率很低,但是在测试集上错误率比较高,这是一种过拟合的表现。因此在引入dropout之后,很明显其抑制了训练的错误降低,同时提高了验证集的准确度,提升了2%,有较明显的效果。</p>
<h3 id="2-3-重叠最大池化Overlapping-Pooling"><a href="#2-3-重叠最大池化Overlapping-Pooling" class="headerlink" title="2.3 重叠最大池化Overlapping Pooling"></a>2.3 重叠最大池化Overlapping Pooling</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;在神经网络中我们通过会用到池化层,其将局部信息进行汇总,通过特定的算法来表征某一块的局部信息,可以称之为叫特征化。比如2x2的maxpooling,就是用在2x2区域内的最大值来表征这2x2的区域,以期找到有效特征。池化层最重要的作用是能够减少参数的数量,通过池化层,将图像的W和H进行缩小,减少了网络的参数数量,能够加速学习。<br>&nbsp;&nbsp;&nbsp;&nbsp;池化层通常选择的是步长等于池化宽度。比如假设池化宽度是Z,步长是S,一般Z=S。如很多网络中都是2x2的池化层,步长也为2。但是这也间接导致了信息的损失,因为每次跨步进行池化。因此在AlexNet中,作者提出了S&lt;Z,即步长小于Z。这样能够减少信息的损失,但是图像的尺寸每次只缩小S像素。如下图所示是训练的时间：<br><img src="/../../../images/DeepLearning/Classfication/AlexNet/pooling_time.png" alt="Alt text">&nbsp;&nbsp;&nbsp;&nbsp;显而易见,由于图像尺寸每次只缩小S像素,导致每一层卷积以及池化之后参数量相较之前大很多,因此在训练时间上马上有了体现。而在训练与测试误差方面,本文根据AlexNet网络,修改所有的池化层分别为S=2,Z=2和S=1,Z=2来做对比,对比结果如下图所示：<br><img src="/../../../images/DeepLearning/Classfication/AlexNet/pooling.png" alt="Alt text">&nbsp;&nbsp;&nbsp;&nbsp;在训练误差和测试误差方面,两者的趋势一致,但S=1相较于S=2在训练误差和测试误差都有了一定的下降。原作者在imagnet上top1和top5错误率减少了0.4%和0.3%,本人在cifar-10数据集上错误率减少了0.2%。因此通过Overlapping Pooling能够达到减少误差的目的,但是效果可能没有那么明显。在对具体问题分析的时候,需要综合考虑错误率和训练速度的因素。</p>
<h3 id="2-4-数据增强-data-augmentation"><a href="#2-4-数据增强-data-augmentation" class="headerlink" title="2.4 数据增强(data augmentation)"></a>2.4 数据增强(data augmentation)</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;数据增强,通常会从现有数据中生成额外的数据,以丰富数据样本,提升模型能力,并达到减少过拟合的效果。在AlexNet网络中也使用了一些技巧：<br>1.<strong>水平镜像翻转：</strong>通过水平镜像翻转,原本是一只狗,那么经过水平镜像翻转那么还应该是。这丰富了数据样本,训练数据变为原来的两倍。<br>2.<strong>随机裁剪：</strong>通过随机裁剪实现数据增强,即原始数据的移位。作者从大小为256×256的图像中随机裁剪出大小为224×224的图像,作为网络的输入。<br>&nbsp;&nbsp;&nbsp;&nbsp;作者通过这两种方式实现了数据扩增2048倍,$(256-224)\cdot (256-224)\cdot 2=2048$ 如何理解呢?<br>在256x256的左上角(32x32)范围内,以每个点为图像的顶角,既(w+224,h+224)来进行提取,那么提取的图像就是$32\cdot32=1024$,再加上一个水平反射可得2048张图.</p>
<h2 id="3-总结"><a href="#3-总结" class="headerlink" title="3.总结"></a>3.总结</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;AlexNet作为深度神经网络的开端,在2012提出之后有非常大的影响力,至今的引用量超过了1.4万,后面很多的研究以及提出的方法都是基于AlexNet网络来进行改进。因此笔者也是从这开始,慢慢的跟着研究者的思路进行学习,同时根据作者在文章中提出的一些创新机制进行验证并理解,以期在以后能够灵活应用其机制。基于上述的验证,同时发现在CIFAR-10数据集上AlexNet笔者的最后的验证精度是78%左右,其精度相对比较低,这也可能是网络的一个限制,因此也是基于此,接下来利用cifar-10数据集来研究各个网络在此基础上的改进以及如何提升。</p>
<h2 id="4-参考"><a href="#4-参考" class="headerlink" title="4.参考"></a>4.参考</h2><p>1.<a href="https://blog.csdn.net/stdcoutzyx/article/details/49022443" target="_blank" rel="noopener">理解dropout</a><br>2.Krizhevsky A, Sutskever I, Hinton G E. ImageNet classification with deep convolutional neural networks[C] International Conference on Neural Information Processing Systems. Curran Associates Inc. 2012:1097-1105.</p>
]]></content>
      <categories>
        <category>DeepingLearning</category>
      </categories>
      <tags>
        <tag>Classfication</tag>
      </tags>
  </entry>
  <entry>
    <title>构建网络模型的方法以及torch.nn的相关函数</title>
    <url>/2018/08/04/DeepLearning/Pytorch/pytorch%20create%20net/</url>
    <content><![CDATA[<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1.引言"></a>1.引言</h2><p>pytorch由于其与python的库完美结合，感觉就是用numpy一样受到大家青睐。其建立神经网络模型也是十分的方便，以下来介绍创建几种。<br>首先来了解一下，创建网络的基础架构。首先第一步定义一个网络类继承nn.Module。其内部主要有两大函数<strong><strong>init</strong></strong>负责定义网络的架构以及一些必要的参数(如是否训练，卷积层、池化层等)<strong><strong>forward</strong></strong>叫前向通道，当执行这个模型输入图像时，模型会调用forward函数，按顺序执行forward函数里的代码，然后return结果。</p>
<pre><code>import torch
import torch.nn.functional as F
class Net(nn.Module):
    def __init__(self):
    def forward(self, x):
</code></pre><a id="more"></a>
<h2 id="2-torch-nn的了解"><a href="#2-torch-nn的了解" class="headerlink" title="2.torch.nn的了解"></a>2.torch.nn的了解</h2><p>构建网络主要运用的是torch.nn这个功能类，因此有必要来全面了解一下其有哪些函数以及功能。torch.nn有Parameters、Containers等，这些模块会在后续神经网络的预训练这章会介绍，这次只介绍pytorch里面关于神经网络各个层的一些函数。</p>
<h3 id="2-1-Convolution-Layers"><a href="#2-1-Convolution-Layers" class="headerlink" title="2.1 Convolution Layers"></a>2.1 Convolution Layers</h3><p><strong>torch.nn.Conv2d(in_channels,out_channels,kernel_size,stride=1,padding=0, dilation=1,groups=1,bias=True)</strong><br>参数的概念这里就不大解释了，一般应用关注输入通道(in_channels)，输出通道(out_channels)，核的尺寸(kernel_size)以及步距(stride)和填充(padding)卷积核的输入输出有如下关系：<br>其他也就默认为1了，因此输入输出也就简单的有如下公式<br>输入为$(N,C_{in},H_{in},W_{in})$<br>输出为$(N,C_{out},H_{out},W_{out})$此时</p>
<script type="math/tex; mode=display">H_{out} = floor((H_{in} + 2 * padding[0] - (kernel\_size[0] - 2) / stride[0] + 1)</script><script type="math/tex; mode=display">W_{out} = floor((W_{in} + 2 * padding[1] -  (kernel\_size[1] - 2) / stride[1] + 1)</script><h3 id="2-2-Pooling-Layers"><a href="#2-2-Pooling-Layers" class="headerlink" title="2.2 Pooling Layers"></a>2.2 Pooling Layers</h3><h4 id="2-2-1-最大池化"><a href="#2-2-1-最大池化" class="headerlink" title="2.2.1 最大池化"></a>2.2.1 最大池化</h4><p><strong>torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)</strong><br>最大池化的参数顾名思义，kernel_size中找出最大值，stride默认是kernel_size的尺寸。<br>其不改不变输入输出通道数量，$H_{out},W_{out}$的输出与2.1的计算方法一样。</p>
<h4 id="2-2-2-平均池化"><a href="#2-2-2-平均池化" class="headerlink" title="2.2.2 平均池化"></a>2.2.2 平均池化</h4><p><strong>torch.nn.AvgPool2d(kernel_size,stride=None,padding=0,ceil_mode=False, count_include_pad=True)</strong><br>平均池化是在kernel_size中取平均值，其他同理与最大池化。</p>
<h3 id="2-3-Non-linear-Activations"><a href="#2-3-Non-linear-Activations" class="headerlink" title="2.3 Non-linear Activations"></a>2.3 Non-linear Activations</h3><p>1)<strong>校正线性单元ReLU</strong><br><strong>torch.nn.ReLU(inplace=False)</strong>，这个是目前常用的非线性激活函数。${ReLU}(x)= max(0, x)$,参数只有一个inplace,如果设置为True,就取代掉原来的值。除了校正线性单元之外在torch.nn这里还有很多的变体，这里不做过多的介绍。<br>2)<strong>Sigmoid函数</strong><br><strong>torch.nn.Sigmoid()</strong>，其没有参数，集体的计算如下$f(x) = 1 / ( 1 + exp(-x))$<br>3)<strong>正切函数</strong><br><strong>torch.nn.Tanh()</strong>,$f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))$<br>4)<strong>SoftPlus函数</strong><br><strong>torch.nn.Softplus(beta=1, threshold=20)</strong>,其是ReLU的平滑近似输出，可约束输出始终为正。<br>5)<strong>Softmax函数</strong><br><strong>torch.nn.Softmax(dim=None)</strong>，将Softmax函数应用于n维输入Tensor重新缩放它们，使得n维输出Tensor的元素位于范围(0,1)中并总的和为1。$f_i(x) = \frac{\exp(x_i)}{\sum_j \exp(x_j)}$<br>然还有很多很多的非线性激活函数，这里也是简单的介绍几个，如有需要可参考官网。</p>
<h3 id="2-4-Normalization-layers"><a href="#2-4-Normalization-layers" class="headerlink" title="2.4 Normalization layers"></a>2.4 Normalization layers</h3><p><strong>torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True)</strong><br>$y = \frac{x - mean[x]}{ \sqrt{Var[x] + \epsilon}} * gamma + beta$它的从参数这里简单的做一个介绍。eps参数就是公式里的$\epsilon$,num_features是期望输入的通道数[batch_size,num_feature,height,width]gamma和beta是学习参数。</p>
<h3 id="2-5-Linear-layers"><a href="#2-5-Linear-layers" class="headerlink" title="2.5 Linear layers"></a>2.5 Linear layers</h3><p><strong>torch.nn.Linear(in_features, out_features, bias=True)</strong>线性层非常好理解，就是通过$y = Ax + b$输出结果。主要参数就是输入的维度(in_feature)以及输出的维度(out_features)</p>
<h3 id="2-6-Dropout-layers"><a href="#2-6-Dropout-layers" class="headerlink" title="2.6 Dropout layers"></a>2.6 Dropout layers</h3><p><strong>torch.nn.Dropout(p=0.5, inplace=False)</strong>正则化层，防止过拟合。P可能被抑制的比例。</p>
<h2 id="3-构建网络的方法"><a href="#3-构建网络的方法" class="headerlink" title="3.构建网络的方法"></a>3.构建网络的方法</h2><p>简单的介绍了上面的函数之后，接下来我们介绍利用pytorch来构建网络模型的方法。<br>pytorch构建网络的方法有很多，这里也简单的介绍几种方式。<br>假设构建一个网络模型有如下形式：<strong>卷积层—-&gt;ReLu层—-&gt;池化层—-&gt;Normalization层—-&gt;卷积层—-&gt;ReLu层—-&gt;池化层—-&gt;Dropout层—-&gt;全连接层</strong></p>
<h3 id="3-1-方法1"><a href="#3-1-方法1" class="headerlink" title="3.1 方法1"></a>3.1 方法1</h3><pre><code>import torch
class myNet1(torch.nn.Module):
    def __init__(self):
        super(myNet1,self).__init__()
        self.conv1 = torch.nn.Conv2d(3, 32, (3,3),1)
        self.relu1 = torch.nn.ReLU(inplace = True)
        self.max1 = torch.nn.MaxPool2d((2,2))
        self.nor1 = torch.nn.BatchNorm2d(32)
        self.conv2 = torch.nn.Conv2d(32, 64, (2,2),1)
        self.relu2 = torch.nn.ReLU(inplace = True)
        self.avg2 = torch.nn.AvgPool2d((2,2))
        self.drop2 = torch.nn.Dropout2d(inplace = True)
        self.dense2 = torch.nn.Linear(64,2)
    def forward(self,x):
        x = self.conv1(x)
        x = self.relu1(x)
        x = self.max1(x)
        x = self.nor1(x)

        x = self.conv2(x)
        x = self.relu2(x)
        x = self.avg2(x)
        x = self.drop2(x)

        x = x.view(x.size(0),-1)
        x = self.dense2(x)
        return x
</code></pre><p>这种方法很常用，基本就是流程型的，需要哪些结构，定义好机构调用，基本刚开始的时候都是基于这种方法来构建网络。<br><img src="/../../../images/DeepLearning/pytorch/create_net/1.png" alt="Alt text"></p>
<h3 id="3-2-方法2"><a href="#3-2-方法2" class="headerlink" title="3.2 方法2"></a>3.2 方法2</h3><pre><code>class myNet2(torch.nn.Module):
    def __init__(self):
        super(myNet2,self).__init__()
        self.conv_lay1 = torch.nn.Sequential(
            torch.nn.Conv2d(3, 32, (3,3),1),
            torch.nn.ReLU(inplace = True),
            torch.nn.MaxPool2d((2,2)),
            torch.nn.BatchNorm2d(32)
            )
        self.conv_lay2 = torch.nn.Sequential(
            torch.nn.Conv2d(32, 64, (2,2),1),
            torch.nn.ReLU(inplace = True),
            torch.nn.AvgPool2d((2,2)),
            torch.nn.Dropout2d(inplace = True)
            )
        self.dense2 = torch.nn.Linear(64,2)
    def forward(self,x):
        x = self.conv_lay1(x)
        x = self.conv_lay2(x)

        x = x.view(x.size(0),-1)
        x = self.dense2(x)
        return x
</code></pre><p>方法二是采用Sequential的方式进行封装，这和另外一种定义卷积网络的方式相对应：一般将conv+relu+pool三合一称为神经网络的一层。这样封装之后能更容易的理解网络的内容。<br><img src="/../../../images/DeepLearning/pytorch/create_net/2.png" alt="Alt text">封装是封装好了，但是对于各个层内部表示采用序号标识，不容易做区分。</p>
<h3 id="3-3-方法3"><a href="#3-3-方法3" class="headerlink" title="3.3 方法3"></a>3.3 方法3</h3><pre><code>class myNet3(torch.nn.Module):
    def __init__(self):
        super(myNet3,self).__init__()
        self.conv_lay1 = torch.nn.Sequential()
        self.conv_lay1.add_module(&quot;conv1&quot;,torch.nn.Conv2d(3, 32, (3,3),1))
        self.conv_lay1.add_module(&quot;relu1&quot;,torch.nn.ReLU(inplace = True))
        self.conv_lay1.add_module(&quot;max1&quot;,torch.nn.MaxPool2d((2,2)))
        self.conv_lay1.add_module(&quot;nor1&quot;,torch.nn.BatchNorm2d(32))

        self.conv_lay2 = torch.nn.Sequential()
        self.conv_lay2.add_module(&quot;conv2&quot;,torch.nn.Conv2d(32, 64, (2,2),1))
        self.conv_lay2.add_module(&quot;relu2&quot;,torch.nn.ReLU(inplace = True))
        self.conv_lay2.add_module(&quot;avg2&quot;,torch.nn.AvgPool2d((2,2)))
        self.conv_lay2.add_module(&quot;drop2&quot;,torch.nn.Dropout2d(inplace = True))
        self.dense2 = torch.nn.Linear(64,2)

    def forward(self,x):
        x = self.conv_lay1(x)
        x = self.conv_lay2(x)

        x = x.view(x.size(0),-1)
        x = self.dense2(x)
        return x
</code></pre><p>基于方法2的序列表示不清晰，因此可采用add_module()的方法来进行，对每个添加一个字符标识。<br><img src="/../../../images/DeepLearning/pytorch/create_net/3.png" alt="Alt text"></p>
<h3 id="3-4-方法4"><a href="#3-4-方法4" class="headerlink" title="3.4 方法4"></a>3.4 方法4</h3><pre><code>import torch
from collections import OrderedDict
class myNet4(torch.nn.Module):
    def __init__(self):
        super(myNet4,self).__init__()
        self.conv_lay1 = torch.nn.Sequential(
            OrderedDict(
                [
                    (&quot;conv1&quot;,torch.nn.Conv2d(3, 32, (3,3),1)),
                    (&quot;relu1&quot;,torch.nn.ReLU(inplace = True)),
                    (&quot;pool1&quot;,torch.nn.MaxPool2d((2,2))),
                    (&quot;nor1&quot;,torch.nn.BatchNorm2d(32))
                ]
        ))
        self.conv_lay2 = torch.nn.Sequential(
            OrderedDict(
                [
                    (&quot;conv2&quot;,torch.nn.Conv2d(32, 64, (2,2),1)),
                    (&quot;relu2&quot;,torch.nn.ReLU(inplace = True)),
                    (&quot;pool2&quot;,torch.nn.AvgPool2d((2,2))),
                    (&quot;drop2&quot;,torch.nn.Dropout2d(inplace = True))
                ]
            ))
        self.dense2 = torch.nn.Linear(64,2)
    def forward(self,x):
        x = self.conv_lay1(x)
        x = self.conv_lay2(x)

        x = x.view(x.size(0),-1)
        x = self.dense2(x)
        return x
</code></pre><p>方法3虽然多了标识，但是在对于网络的构建上相当于是方法1的思路了，流程性的写，只是网络展示的时候多了序列标识，因此方法4采用OrderedDict字典的方式加入，单独设置层的名称。</p>
<h2 id="4参考"><a href="#4参考" class="headerlink" title="4参考"></a>4参考</h2><p>1.<a href="https://www.cnblogs.com/denny402/p/7593301.html" target="_blank" rel="noopener">denny的学习专栏</a></p>
<p>2.<a href="https://pytorch.org/docs/0.3.1/nn.html" target="_blank" rel="noopener">torch.nn</a></p>
]]></content>
      <categories>
        <category>DeepingLearning</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch 数据导入和预处理</title>
    <url>/2018/07/30/DeepLearning/Pytorch/pytorch%20Data%20Loading%20and%20Processing/</url>
    <content><![CDATA[<h2 id="1-常用数据集的导入"><a href="#1-常用数据集的导入" class="headerlink" title="1.常用数据集的导入"></a>1.常用数据集的导入</h2><p>对于一些常用的数据集(torchvision.datasets)：<br>(1)MNIST<br>(2)Fashion-MNIST<br>(3)EMNIST<br>(4)COCO<br>(5)LSUN<br>(6)ImageFolder<br>(7)DatasetFolder<br>(8)Imagenet-12<br>(9)CIFAR<br>(10)STL10<br>(11)SVHN<br>(12)PhotoTour<br>可以用torchvision.datasets直接读取，以cifar10为例子：</p>
<pre><code>import torchvision
transforms = transforms.Compose([
transforms.ToTensor()])           

# torch.utils.data.DataLoader
cifarSet = torchvision.datasets.CIFAR10(root = &quot;../data/cifar/&quot;, train= True, download = True, transform = transforms )
cifarLoader = torch.utils.data.DataLoader(cifarSet, batch_size= 10, shuffle= False, num_workers= 2)
</code></pre><p>简单理解下，torchvision.datasets.CIFAR10()，torchvision.datasets.CIFAR100()中等的参数都一样，从字面上都好理解。transforms是将得到的数据转化为tensor。<br>在得到数据集后需要载入，利用torch.utils.data.DataLoader(),可以调节我们的batch_size,是否打乱数据(shuffle)等。因此，利用这个库可以轻松的得到一些公共数据集。<br><a id="more"></a></p>
<h2 id="2-创建自己的数据集"><a href="#2-创建自己的数据集" class="headerlink" title="2.创建自己的数据集"></a>2.创建自己的数据集</h2><h3 id="2-1-类的介绍"><a href="#2-1-类的介绍" class="headerlink" title="2.1 类的介绍"></a>2.1 类的介绍</h3><p>pytorch读取数据集非常便捷，主要是用到两个类:<br><strong>1)torch.utils.data.Dataset</strong><br><strong>2)torch.utils.data.DataLoader</strong><br>上述提到的torchvision.datasets也是基于这两个的重写。因此要构建自己的数据集，首先得去读一读这两个类的<a href="https://pytorch.org/tutorials/beginner/data_loading_tutorial.html" target="_blank" rel="noopener">官方源码</a><br>其主要分为三个部分。</p>
<pre><code>class myDataset(torch.utils.data.Dataset):
    def __init__(self):   
    def __getitem__(self,index):
        return img.......
    def __len__(self):
        return 
</code></pre><p><strong><strong>init</strong>(self)</strong>是进行初始化，初始化数据集的一些参数，比如要读取的图片数据集列表(img_list)是否为训练数据(istrain),是否打乱数据(shuffle)，是否数据预处理等<br><strong><strong>getitem</strong>(self,index):</strong>是进行数据读取了，根据初始化参数，在这里读取数据与处理数据，index标签是用来描述读取哪张图片。此函数是有返回值的，返回的值即是所需的。比如train_data 和 val_data要返回的就是img and label了，如果是test_data的，可以只返回img<br><strong><strong>len</strong>(self):</strong>这个非常的简洁，返回的要读取数据的总长度，如要读取的图片类数据的总数。</p>
<h3 id="2-2-制作存储数据集和标签名的文件"><a href="#2-2-制作存储数据集和标签名的文件" class="headerlink" title="2.2 制作存储数据集和标签名的文件"></a>2.2 制作存储数据集和标签名的文件</h3><p>如刚刚上述所说，重写torch.utils.data.Dataset()的时候最主要是数据集列表如何获取。以我的数据为例：<br>要分类的两类数据集，标签分别为0和1。<br><img src="/../../../images/DeepLearning/pytorch/data_load/1.png" alt="Alt text"><br>图片数据集如下图所示<br><img src="/../../../images/DeepLearning/pytorch/data_load/2.png" alt="Alt text"></p>
<h4 id="2-2-1-获取文件夹信息"><a href="#2-2-1-获取文件夹信息" class="headerlink" title="2.2.1 获取文件夹信息"></a>2.2.1 获取文件夹信息</h4><p>由于在文件夹下有可能存在除了标签类的文件夹之外还有其他的一些描述性的文件，因此首先我们的获取文件夹的标签列表。</p>
<pre><code>def traversalDir_FirstDir(path):
    dir_list = []
# 判断路径是否存在
    if (os.path.exists(path)):
        # 获取该目录下的所有文件或文件夹目录
        files = os.listdir(path)
        for file in files:
            # 得到该文件下所有目录的路径
            m = os.path.join(path, file)
            # 判断该路径下是否是文件夹
            if (os.path.isdir(m)):
                dir_list.append(m)
    return dir_list    
</code></pre><h4 id="2-2-2-获取具体的数据名称以及标签"><a href="#2-2-2-获取具体的数据名称以及标签" class="headerlink" title="2.2.2 获取具体的数据名称以及标签"></a>2.2.2 获取具体的数据名称以及标签</h4><p>当获取完上述标签文件夹后，下一步便是遍历标签文件夹获取数据名称以及标签，变为tuple存入list中。这里注意下，在ubuntu和windows系统中存在着’/‘和’\’的问题，因此在不同系统上请做如下备注中的修改。</p>
<pre><code>def require_img_label(path):
    img_label = []
    for i in traversalDir_FirstDir(path):
        for root, dirs, files in os.walk(i):
            #s = root.split(&quot;\\&quot;)     #windows
            s = root.split(&quot;/&quot;)       #linux
            for filename in files:
                if filename.endswith(&quot;jpg&quot;):
                    f1 = root + &#39;\\&#39; + filename
                    f1 = f1.replace(&#39;\\&#39;, &#39;/&#39;)
                    tup = tuple((f1, s[-1]))
                    img_label.append(tup)
    return img_label
</code></pre><h4 id="2-2-3-打乱数据集并且存储"><a href="#2-2-3-打乱数据集并且存储" class="headerlink" title="2.2.3 打乱数据集并且存储"></a>2.2.3 打乱数据集并且存储</h4><p>将其存储为txt,每行写入name和tuple</p>
<pre><code>def write_txt(path_out, image_list):
    with open(path_out, &#39;w&#39;) as fout:
        for i, item in enumerate(image_list):
            line = &#39;%s\r&#39; % item[0]
            line += item[1]+&#39;\n&#39;
            fout.write(line)
</code></pre><p>重点就是make_txt文件了，这里将参数做一个解释。<br><strong>filepath</strong>：为训练的图片集所在的文件夹，即上述0,1文件夹所在的路径，如’data/train/‘<br><strong>shuffle</strong>：是否进行数据打乱，一般作为训练集和验证机，一般会进行打乱，设置为True<br><strong>chunks</strong>：将数据分成多少堆，如1<br><strong>train_ratio</strong>：训练数据的比例 如0.9<br><strong>test_ratio</strong>：验证数据的比例，如0.1</p>
<pre><code>def make_txt(path, rand_shuff, chunks, tra_ratio, tes_ratio):
    imglab_list = require_img_label(path)
    if rand_shuff is True:
        np.random.seed(100)
        np.random.shuffle(imglab_list)
    N = len(imglab_list)
    chunk_size = (N + chunks - 1) 

    for i in range(chunks):
        chunk = imglab_list[i * chunk_size:(i + 1) * chunk_size]
        if chunks &gt; 1:
            str_chunk = &#39;_%d&#39; % i
        else:
            str_chunk = &#39;&#39;
        sep = int(chunk_size * tra_ratio)
        sep_test = int(chunk_size * tes_ratio)
        if tra_ratio == 1.0:
            write_txt(path+str_chunk + &#39;train.txt&#39;, chunk)
        else:
            if tes_ratio:
                write_txt(path+str_chunk + &#39;_val.txt&#39;, chunk[:sep_test])
            if tra_ratio + tes_ratio &lt; 1.0:
                write_txt(str_chunk + &#39;_val.txt&#39;, chunk[sep_test + sep:])
            write_txt(path+str_chunk + &#39;_train.txt&#39;, chunk[sep_test:sep_test + sep])
</code></pre><h4 id="2-2-4-测试"><a href="#2-2-4-测试" class="headerlink" title="2.2.4 测试"></a>2.2.4 测试</h4><p>将其用于到本人的数据集进行实验：</p>
<pre><code># coding: utf8
import os
import numpy as np
#上述的所有函数复制到这

filepath = &#39;data/val/&#39;  # 图片所在文件夹
shuffle = True
chunks = 1
train_ratio =0.9
test_ratio = 0.1
if __name__ == &#39;__main__&#39;:
    make_txt(filepath, shuffle, chunks, train_ratio, test_ratio)
</code></pre><p>运行后在data/val/下出现了_train.txt和_val.txt。<br><img src="/../../../images/DeepLearning/pytorch/data_load/3.png" alt="Alt text"><br>随便打开_train.txt，如上图所示，那么标签数据集算是完成了。<br><img src="/../../../images/DeepLearning/pytorch/data_load/4.png" alt="Alt text"></p>
<h3 id="2-3-构建myDataset-torch-utils-data-Dataset"><a href="#2-3-构建myDataset-torch-utils-data-Dataset" class="headerlink" title="2.3 构建myDataset(torch.utils.data.Dataset)"></a>2.3 构建myDataset(torch.utils.data.Dataset)</h3><p>基于2.2已经得到我们所需要的txt文件了，接下来一步便是得到构建myDataset(torch.utils.data.Dataset)。直接看代码：</p>
<pre><code>class myData(torch.utils.data.Dataset):
    def __init__(self,root,datatxt,transform=None,train=True, target_transform=None):
        f = open(root + datatxt, &#39;r&#39;)
        img_label = []
        for line in f:
            line = line.rstrip()
            words = line.split(&#39;\r&#39;)
            if len(words) &gt;= 2:
                img_label.append((words[0],words[1]))
                continue
        self.root = root
        self.img_label = img_label
        self.transform = transform
        self.target_transform = target_transform
        self.train = train
    def __getitem__(self,index):
        image_path, label = self.img_label[index]
        img = Image.open(self.root+image_path).convert(&#39;RGB&#39;) 
        if self.transform is not None:
            img = self.transform(img)
        if(self.train ==True):
            return img,int(label)
        else:
            return img
    def __len__(self):
        return len(self.img_label)
</code></pre><p>有了2.1的介绍，相信对这部分非常容易的理解。重点关注下<strong>getitem</strong>的返回值，需要什么内容则就返回什么内容。调用方式如下：</p>
<pre><code>train_data = myData(root = root,datatxt=&#39;train.txt&#39;,transform = data_transforms[&#39;train&#39;])
val_data = myData(root = root,datatxt = &#39;val.txt&#39;,transform =data_transforms[&#39;val&#39;])

train_loader = Data.DataLoader(dataset=train_data,batch_size = 50,shuffle = True)
val_loader = Data.DataLoader(dataset = val_data,batch_size = 50)
</code></pre><p>DataLoader和刚刚创建的数据集myData,来创建dataloader，loader的长度是有多少个batch，所以和batch_size有关，此处是设置为50。<br>那么在训练或者测试中如何应用train_loader呢，看如下代码：</p>
<pre><code>for step,batch  in enumerate(train_loader):
    inputs,labels = batch
</code></pre><p>dataloader返回值如上述所说，train_loader是一个有很多batch的数组，得到的每个batch含有<strong>getitem</strong>方法return的image和label。<br>input:[batch_size,channal,width,height]<br>labels：[batch_size,label]</p>
<h3 id="2-4-Data-DataLoader"><a href="#2-4-Data-DataLoader" class="headerlink" title="2.4 Data.DataLoader"></a>2.4 Data.DataLoader</h3><p>当构建好数据集后要进行导入，但是不可能将所有的数据集都放入内存中(内存就爆掉了)，需要定义迭代器，pytorch定义好了torh.utils.data.DataLoader()<br>参数为：<br>    datasets:2.3中定义好的dataset。<br>    batch_size:batch_size的大小<br>    shuffle:是否打乱顺序<br>    number_works:采用几个线程<br>等等这些参数，具体可以去<a href="https://github.com/pytorch/pytorch/blob/master/torch/utils/data/dataloader.py" target="_blank" rel="noopener">官网查看</a>。<br>这里介绍下一个重要的参数为collate_fn，它默认是default_collate()函数，输入一个list,list的长度和batch_size相等，list是从<strong>getitem</strong>()中获得的，所以它的作用就是，比如将一张图片tensor=[3,224,224]，如果batch_size=8，通过第一维堆叠成[8,3,224,224]的8张图片，输入到我们的网络。<br>我们本身是可以自定义my_collate_fn，插入dataloader(collate_fn = my_collate_fn)。<br>里面的惭怍就是对我们输入的list进行操作。信息可以参考<a href="https://zhuanlan.zhihu.com/p/30385675" target="_blank" rel="noopener">PyTorch实现自由的数据读取</a></p>
<h2 id="3-数据转换以及处理"><a href="#3-数据转换以及处理" class="headerlink" title="3.数据转换以及处理"></a>3.数据转换以及处理</h2><p>在上述<strong>getitem</strong>(self,index)中调用了self.transform的方法，它是传入的转换参数，也称之为数据处理。因此，这里简单的介绍下pytorch的一些数据处理方式: <strong>torchvision.transforms</strong></p>
<h3 id="3-1-PIL-Image-transform"><a href="#3-1-PIL-Image-transform" class="headerlink" title="3.1 PIL Image transform"></a>3.1 PIL Image transform</h3><p>PIL图像读入是一个对象，非numpy矩阵，下面介绍的函数都是对PIL Image图像直接操作。</p>
<h4 id="3-1-1-CenterCrop-size"><a href="#3-1-1-CenterCrop-size" class="headerlink" title="3.1.1 CenterCrop(size)"></a>3.1.1 CenterCrop(size)</h4><p>将给定的图像根据中心点向两边裁剪，输入的size可以为一个值或者序列(w,h)。如果为一个值则表明w和h相等。</p>
<h4 id="3-1-2-ColorJitter-brightness-0-contrast-0-saturation-0-hue-0"><a href="#3-1-2-ColorJitter-brightness-0-contrast-0-saturation-0-hue-0" class="headerlink" title="3.1.2 ColorJitter(brightness=0, contrast=0, saturation=0, hue=0)"></a>3.1.2 ColorJitter(brightness=0, contrast=0, saturation=0, hue=0)</h4><p>随机更改图像的亮度，对比度，饱和度。更改后各个值归一化到[max(0, 1-brightness),1+ brightness]之间，其他几个参数也类似。</p>
<h4 id="3-1-3-FiveCrop-size"><a href="#3-1-3-FiveCrop-size" class="headerlink" title="3.1.3 FiveCrop(size)"></a>3.1.3 FiveCrop(size)</h4><p>裁剪给定的PIL图像为4个角落+1中心部分，size的参数描述和 <strong>3.1.1</strong>一样。</p>
<pre><code>transform = Compose([
    FiveCrop(size), 
    Lambda(lambda crops: torch.stack([ToTensor()(crop) for crop in crops])) 
    # returns a 4D tensor
])
#In your test loop you can do the following:
input, target = batch # input is a 5d tensor, target is 2d
bs, ncrops, c, h, w = input.size()
result = model(input.view(-1, c, h, w)) # fuse batch size and ncrops
result_avg = result.view(bs, ncrops, -1).mean(1) # avg over crops
</code></pre><h4 id="3-1-4-Grayscale-num-output-channels-1"><a href="#3-1-4-Grayscale-num-output-channels-1" class="headerlink" title="3.1.4 Grayscale(num_output_channels=1)"></a>3.1.4 Grayscale(num_output_channels=1)</h4><p>将图像转换为灰度图，如果num_output_channels=3的话，返回的还是rgb的图。</p>
<h4 id="3-1-5-Pad-padding-fill-0-padding-mode-’constant’"><a href="#3-1-5-Pad-padding-fill-0-padding-mode-’constant’" class="headerlink" title="3.1.5 Pad(padding, fill=0, padding_mode=’constant’)"></a>3.1.5 Pad(padding, fill=0, padding_mode=’constant’)</h4><p>给这个PIL图像所有的边附上给的pad值。<br>pad:int(四个边补上这个尺寸的数)，tuple(如果是2个常数，则第一个是左右，第二个是上下。如果是四个参数，则代表上下左右)<br>fill:默认是补上0，如果有给出一个tuple(r,g,b)，则补上的为这个值。<br>padding_mode：决定补上的值到底是fill,还是图像的edge等</p>
<h4 id="3-1-6-RandomHorizontalFlip-p-0-5-or-RandomVerticalFlip-p-0-5"><a href="#3-1-6-RandomHorizontalFlip-p-0-5-or-RandomVerticalFlip-p-0-5" class="headerlink" title="3.1.6 RandomHorizontalFlip(p=0.5) or RandomVerticalFlip(p=0.5)"></a>3.1.6 RandomHorizontalFlip(p=0.5) or RandomVerticalFlip(p=0.5)</h4><p>随机翻转给定的图像，概率有P来指定。</p>
<h4 id="3-1-7-RandomRotation-degrees-resample-False-expand-False-center-None"><a href="#3-1-7-RandomRotation-degrees-resample-False-expand-False-center-None" class="headerlink" title="3.1.7 RandomRotation(degrees, resample=False, expand=False, center=None)"></a>3.1.7 RandomRotation(degrees, resample=False, expand=False, center=None)</h4><p>将图像旋转degree个角度。</p>
<h4 id="3-1-8-Resize-size-interpolation-2"><a href="#3-1-8-Resize-size-interpolation-2" class="headerlink" title="3.1.8 Resize(size, interpolation=2)"></a>3.1.8 Resize(size, interpolation=2)</h4><p>重新框定图像的尺寸，比较常用。</p>
<h3 id="3-2-Tensor-transform"><a href="#3-2-Tensor-transform" class="headerlink" title="3.2 Tensor transform"></a>3.2 Tensor transform</h3><h4 id="3-2-1-Normalize-mean-std"><a href="#3-2-1-Normalize-mean-std" class="headerlink" title="3.2.1 Normalize(mean, std)"></a>3.2.1 Normalize(mean, std)</h4><p>根据均值和方法归一化图像。mean:($M_1,M_2…,M_n$) std:($S_1,S_2…,S_n$）<br>n是通道值。</p>
<h4 id="3-2-2-ToPILImage-mode-None"><a href="#3-2-2-ToPILImage-mode-None" class="headerlink" title="3.2.2 ToPILImage(mode=None)"></a>3.2.2 ToPILImage(mode=None)</h4><p>转变tensor(CxHxW)或array(HxWxC)为PIL Image</p>
<h4 id="3-2-2-ToTensor"><a href="#3-2-2-ToTensor" class="headerlink" title="3.2.2 ToTensor()"></a>3.2.2 ToTensor()</h4><p>转变PIL Image或array(HxWxC)为tensor(CxHxW),转变后的tensor值为[0,1]之间</p>
<h2 id="4-总结"><a href="#4-总结" class="headerlink" title="4 总结"></a>4 总结</h2><p>综上，我们了解了torchvision.datasets给出的常用数据集和导入方式，并介绍了根据具体问题制作自己的数据集，重点说明了data.txt的创建，以及torch.utils.data.Dataset的重写和<br>torch.utils.data.DataLoader应用。在数据集导入的过程中简单的数据预处理比分，也根据torchvision.transforms给的一些例子做了一点小整理。深度模型学习的成功与否和数据集息息相关。因此如何做好数据预处理至关重要，opencv提供了很多数据处理的方法，等后续再专门写一章节介绍。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>1.<a href="https://pytorch.org/tutorials/beginner/data_loading_tutorial.html" target="_blank" rel="noopener">Data Loading and Processing Tutorial</a></p>
<p>2.<a href="https://blog.csdn.net/Teeyohuang/article/details/79587125" target="_blank" rel="noopener">Pytorch打怪路（三）Pytorch创建自己的数据集1</a></p>
<p>3.<a href="https://pytorch.org/docs/stable/torchvision/transforms.html" target="_blank" rel="noopener">torchvision.transforms</a></p>
<p>4.<a href="https://zhuanlan.zhihu.com/p/30385675" target="_blank" rel="noopener">PyTorch实现自由的数据读取</a></p>
]]></content>
      <categories>
        <category>DeepingLearning</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>pytorch入门</title>
    <url>/2018/07/29/DeepLearning/Pytorch/pytorch%20Introduction/</url>
    <content><![CDATA[<h2 id="1-PyTorch简介"><a href="#1-PyTorch简介" class="headerlink" title="1.PyTorch简介"></a>1.PyTorch简介</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;pytorch 是一个基于python的计算包，其两大特点1)利用GPU来解决Numpy计算问题。2)一个深度学习的研究平台，非常灵活而且速度快。<br><a id="more"></a></p>
<h2 id="2-PyTorch的基本操作和概念"><a href="#2-PyTorch的基本操作和概念" class="headerlink" title="2.PyTorch的基本操作和概念"></a>2.PyTorch的基本操作和概念</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;PyTorch的主要操作库是torch.首先了解一下tensor,官方文档介绍的非常简洁，tensor跟ndarray是没有太大的区别，主要是tensor能够被用于gpu加速。</p>
<h3 id="2-1创建tensor"><a href="#2-1创建tensor" class="headerlink" title="2.1创建tensor"></a>2.1创建tensor</h3><p>Torch创建tensor有以下几种方式：</p>
<pre><code>torch.empty(5,3)                   #创建一个5x3的空tensor
torch.rand(5,3)                    #创建一个5x3的随机初始化tensor
torch.zeros(5,3,dtype=torch.long)  #创建一个5x3类型为long的空tensor，值为0
torch.tensor([5.5,3])              #直接创建一个[5.5,3]的1x2的tensor
</code></pre><p>当然，也可以根据已有的tensor来进行创建：</p>
<pre><code>x = x.new_ones(5,3,dtype=torch.double)    #new_*的方法很多，可以是zeros等 
x = torch.rand_like(x,dtype = torch.float)#如果不指定参数，相当于所有属性都拷贝
</code></pre><p>如果想获取我们tensor的尺寸，利用其size属性就行。</p>
<pre><code>print(x.size()) #其结果是一个tuple,支持tuple的所有操作。
</code></pre><h3 id="2-2-操作tensor"><a href="#2-2-操作tensor" class="headerlink" title="2.2 操作tensor"></a>2.2 操作tensor</h3><p>对于tensor的操作有很多，以加法乘法为例子。</p>
<pre><code>x = torch.rand(5,3)
y = torch.rand(5,3)
print(x+y)                  #直接相加
result = torch.empty(5,3)
print(torch.add(x,y,out = result))       #torch.add函数操作
print(y.add_(x))           #这个y会被替换掉，变为结果。特别注意，有_的操作蕴含着    
取代的意思,比如add和add_做一个区分。x.copy_(y)等都会改变x
</code></pre><p>对于tensor的resize,pytorch提供了一些操作：</p>
<pre><code>print(x[:,1]) #输出tensor矩阵的第一列的值
x = torch.randn(4, 4)  #4x4 tensor
y = x.view(16)         #16  tensor
z = x.view(-1, 8)      #2x8 tensor 
print(x.size(), y.size(), z.size())
</code></pre><p>对于tensor的输出,其和numpy的操作没其他的差别,如果想得到某个元素，利用其item()的方法来获得</p>
<pre><code>print(x[:,1]) #输出tensor矩阵的第一列的值
x = torch.randn(1)
print(x)         #输出tensor
print(x.item())  #输出元素
</code></pre><h3 id="2-3-tensor和numpy的转换"><a href="#2-3-tensor和numpy的转换" class="headerlink" title="2.3 tensor和numpy的转换"></a>2.3 tensor和numpy的转换</h3><p>Torcch张量和NumPy数组共享它们的底层内存位置，更改一个将更改另一个。<br><strong>tensor转为numpy:</strong></p>
<pre><code>a = torch.ones(5)
print(a)           #tensor([ 1.,  1.,  1.,  1.,  1.])
b = a.numpy()      #.numpy()能将其变为numpy类型,[1. 1. 1. 1. 1.]
a.add_(1)          #替换a变为a+1
print(a)           #tensor([ 2.,  2.,  2.,  2.,  2.])
print(b)           #[2. 2. 2. 2. 2.]
</code></pre><p><strong>numpy转为tensor：</strong></p>
<pre><code>import numpy as np
a = np.ones(5)
b = torch.from_numpy(a)  #torch.form_numpy()能将numpy变为tensor
np.add(a,1,out =a)
print(a)                #[2. 2. 2. 2. 2.]
print(b)                #tensor([2.,2.,2.,2.,2.],dtype=torch.float64)
</code></pre><p><strong>tensor能够通过.to的方式移动到任何的device：</strong></p>
<pre><code>if torch.cuda.is_available():
    device = torch.device(&quot;cuda&quot;)          # a CUDA device object
    y = torch.ones_like(x, device=device)  #创建tensor在gpu
    x = x.to(device)                       #利用.to(device)也可以导gpu
    z = x + y                               
    print(z)                            #tensor([2.3159], device=&#39;cuda:0&#39;)
    print(z.to(&quot;cpu&quot;, torch.double))    #利用.to来改变设备同时还能改变类型 
</code></pre><h2 id="3-PyTorch的Autograd"><a href="#3-PyTorch的Autograd" class="headerlink" title="3.PyTorch的Autograd"></a>3.PyTorch的Autograd</h2><p>PyTorch的核心就是Autograd包，其为张量上的所有操作提供了自动的微分(Automatic Differentiation),这种逐行定义的方式意味着我们的bp是由代码运行的方式定义，迭代都可以不同。<br>在0.4版本之前，变量(Variable) 它包装了张量(Tensor),支持几乎所有的张量上的操作。<br>在0.4版本以后，将其与tensor合到了一起，所以两者的很多属性都是一样的，共用的。这里以0.4版为例子。</p>
<pre><code>from torch.autograd import Variable
</code></pre><h3 id="3-1-tensor的属性"><a href="#3-1-tensor的属性" class="headerlink" title="3.1 tensor的属性"></a>3.1 tensor的属性</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;你可以通过.data属性来访问变量中的原始张量,关于这个变量的梯度被计算放入.grad属性中torch.Tensor中的一个 <strong>.requires_grad</strong>属性如果设置True,则会开始跟踪其所有操作。完成计算后,调用 <strong>.backward()</strong>并自动计算所有梯度。最后该张量的梯度将累积到 <strong>.grad</strong>属性之中。<br>&nbsp;&nbsp;&nbsp;&nbsp;如果不要张量的跟踪历史记录，可以调用 <strong>.detach()</strong>将其与历史分离，进行切断。还有一种方法是将代码块包装起来，比如在评估模型的时候。因为模型具有requires_grad=True使得整个模型可训练，但是我们不需要，因此可用：<strong>with torch.no_grad():</strong>还有一个很重要的类是Function,其与tensor进行交互的。每一个tensor(除了用户创建的之外)都有 <strong>.grad_fn</strong>属性。如果想计算张量的导数，可用调用 <strong>.backward()</strong>如果tensor是一个标量就不要指定，如果有其他很多元素，则需要指定gradient匹配形状的张量参数。</p>
<pre><code>import torch
x = torch.ones(2,2,require_grad=True)  #设置requires_grad=True来跟踪它的计算
print(x)            #tensor([[ 1.,  1.],[ 1.,  1.]])
y = x+2         
print(y)            #tensor([[ 3.,  3.],[ 3.,  3.]])
print(y.grad_fn)    #y是结果，所以有grad_fn的属性
z = y * y * 3
out = z.mean()
print(z, out)       #tensor([[27.,27.],[27.,27.]])  tensor(27.)
</code></pre><p>当然，<strong>.require_grad()</strong>能够改变张量的属性。</p>
<h3 id="3-2-Gradients"><a href="#3-2-Gradients" class="headerlink" title="3.2 Gradients"></a>3.2 Gradients</h3><p>梯度是通过.backward()来实现的，执行:</p>
<pre><code>out.backwark()
print(x.grad)      #print d(out)/dx
</code></pre><script type="math/tex; mode=display">o = \frac{1}{4}\sum_i z_i</script><script type="math/tex; mode=display">z_i = 3(x_i+2)^2 and z_i\bigr\rvert_{x_i=1} = 27</script><script type="math/tex; mode=display">\frac{\partial o}{\partial x_i} = \frac{3}{2}(x_i+2) and \frac{\partial o}{\partial x_i}\bigr\rvert_{x_i=1} = \frac{9}{2} = 4.5</script><p>当我们应用测试的时候，减少内存的消耗，不进行梯度的运算，因此采用with torch.no_grad():</p>
<pre><code>print(x.requires_grad)
print((x ** 2).requires_grad)
with torch.no_grad():
    print((x ** 2).requires_grad)
</code></pre><h2 id="4-Neural-Networks"><a href="#4-Neural-Networks" class="headerlink" title="4.Neural Networks"></a>4.Neural Networks</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;pytorch里有一个专门建立神经网络的包交torch.nn，一个nn.Module包含里面的一些卷积层，同时包含forward(input)方法，以及最后返回output。<br><img src="/../../../images/DeepLearning/pytorch/mnist.png" alt="Alt text"><br>手写体识别的简单前馈神经网络，从他我们来看简单的一个神经网络该流程是如何：<br>1）定义神经网络网络架构(包括参数或者权重等）<br>2）初始化网络参数<br>3)循环输入数据到网络<br>4)计算损失函数<br>5)BP法计算并更新参数。</p>
<h3 id="4-1-定义神经网络"><a href="#4-1-定义神经网络" class="headerlink" title="4.1 定义神经网络"></a>4.1 定义神经网络</h3><pre><code>import torch
import torch.nn as nn 
import torch.nn.functional as F 
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__() 
        self.conv1 = nn.Conv2d(1,6,5)
        self.conv2 = nn.Conv2d(6,16,5)
        self.fc1 = nn.Linear(16*5*5,120)
        self.fc2 = nn.Linear(120,84)
        self.fc3 = nn.Linear(84,10)
    def forward(self,x):
        x = F.max_pool2d(F.relu(self.conv1(x)),(2,2))
        x = F.max_pool2d(F.relu(self.conv2(x), 2))

        x = x.view(-1,self.num_flat_features(x))
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc1(x))
        x = self.fc3(x)
        return x
    def num_flat_features(self,x):
        size = x.size()[1:]
        num_features = 1
        for s in size:
            num_features*=s
        return num_features
net = Net()
print(net)
</code></pre><p>其中这里的super(Net, self).<strong>init</strong>()介绍一下，其是对类的继承。首先他先找到Net的父类nn.module，然后Net就转换为nn.module的对象，可以调用nn.module的函数。此处我们在初始化的时候定义了卷积层，同时写了forward()函数。</p>
<h3 id="4-2-损失函数"><a href="#4-2-损失函数" class="headerlink" title="4.2 损失函数"></a>4.2 损失函数</h3><p>nn包中有很多的损失函数，可以参考官方链接，这里以nn.MSELOSS为例子。</p>
<pre><code>output = net(input)
target = torch.arange(1, 11) 
target = target.view(1, -1) 
criterion = nn.MSELoss()
loss = criterion(output, target)
print(loss)                   #tensor(38.7042)

net.zero_grad()     # 0初始化梯中的所有参数
print(&#39;conv1.bias.grad before backward&#39;)
print(net.conv1.bias.grad)
loss.backward()
print(&#39;conv1.bias.grad after backward&#39;)
print(net.conv1.bias.grad)
</code></pre><p>上述定义的网络+损失函数就组成了一个前馈神经网络，整个程序的路径是<br>input -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d<br>-&gt; view -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear-&gt; MSELoss-&gt; loss<br>而我们执行loss.backward的时候就是沿着整个路径反向执行。</p>
<h3 id="4-3-优化算法"><a href="#4-3-优化算法" class="headerlink" title="4.3 优化算法"></a>4.3 优化算法</h3><p>所谓优化算法，就是进行我们的参数更新，这里torch里有一个torch.optim包，提供饿了许多算法，比如SGD,Nesterov-SGD等等。</p>
<pre><code>import torch.optim as optim
optimizer = optim.SGD(net.parameters(), lr=0.01)   #创建优化器并且设置学习率lr
optimizer.zero_grad()                               #0初始化所有梯度
output = net(input)                                 
loss = criterion(output, target)                    
loss.backward()                                     #计算梯度
optimizer.step()    # Does the update               #参数更新
</code></pre><h2 id="5总结"><a href="#5总结" class="headerlink" title="5总结"></a>5总结</h2><p>大概了解了这么几个部分，对于pytorch也算是有一点了解了。但是还是遗留下很多问题，比如如何做数据预处理，如何实现各种网络等等，这个还需要后面探索。接下来后面也会继续的深入学习</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>1.<a href="https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py" target="_blank" rel="noopener">pytorch</a></p>
]]></content>
      <categories>
        <category>DeepingLearning</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>TFRecord创建</title>
    <url>/2018/07/21/DeepLearning/Tensorflow/TFrecord/</url>
    <content><![CDATA[<h2 id="1-TFRecord"><a href="#1-TFRecord" class="headerlink" title="1.TFRecord"></a>1.TFRecord</h2><p>TFRecord宏观上将是一种数据存储格式。其主要的工作是允许你将任意的数据格式的数据比如图片，标签等打包成一个文件叫TFRecord。tensorflow支持TFRecord数据格式，其能非常容易的和网络架构相匹配。当然，做过工程项目或者比赛的都清楚，数据一般的给出都是.csv的格式，读取非常方便。但是通常读取的时候，一般是一次性读出到内存。要是内存小的话，倒是可以分块读取，但是其也是不能一下子全部读进来的。 对于图片数据，由于图片的文件格式，图像大小以及所存储的位置等因素在，想要在训练或者测试阶段直接使用，就显得非常麻烦了。因此，需要对数据做统一处理，那么TFRecord就是对于数据做统一管理的一个工具。<br><a id="more"></a></p>
<h2 id="2-TFRecord重要的API"><a href="#2-TFRecord重要的API" class="headerlink" title="2.TFRecord重要的API"></a>2.TFRecord重要的API</h2><p>下面来介绍几个常用而且重要的API以及对应的函数，对于所有的请参照对应的类去进行查找。</p>
<h3 id="2-1-tf-TFRecordWriter"><a href="#2-1-tf-TFRecordWriter" class="headerlink" title="2.1 tf.TFRecordWriter"></a>2.1 tf.TFRecordWriter</h3><p><strong>主要功能作用是将数据写入到TFRecord文件</strong> </p>
<pre><code>__init__(path,options=None)
作用:创建一个TFRecordWriter对象,这个对象就负责写记录到指定的文件中去了.
参数:
    path: TFRecords 文件路径
    options: (可选) TFRecordOptions对象
close()
作用：关闭对象
write(record)
作用:把字符串形式的记录写到文件中去. 
参数: 
    record: 字符串,待写入的记录
</code></pre><h3 id="2-2-tf-TFRecordReader"><a href="#2-2-tf-TFRecordReader" class="headerlink" title="2.2 tf.TFRecordReader"></a>2.2 tf.TFRecordReader</h3><p><strong>主要功能作用是从TFRecord中读取记录</strong></p>
<pre><code>__init__(Name=None)
作用:创建一个TFRecordRead对象,这个对象就负责读记录. 
参数:无

read(queue, name=None) 
作用：返回读取的记录
参数：
    queue：队列的文件名
    name:操作的名字
返回值：
    键值对(key,value)：value表示读取的文件
</code></pre><h3 id="2-3-tf-train-Example"><a href="#2-3-tf-train-Example" class="headerlink" title="2.3 tf.train.Example"></a>2.3 tf.train.Example</h3><p><strong>作用：TFRecord文件中的数据都是通过tf.train.Example Protocol Buffer的格式进行存储的</strong></p>
<pre><code>__init__(**kwargs)
作用：这个函数是初始化函数,会生成一个Example对象,一般我们使用的时候,是传入一个tf.train.Features对象进去.

SerializeToString()
作用:把example序列化为一个字符串,在写入到TFRcorde,write方法的参数是字符串.
</code></pre><h3 id="2-4-tf-train-Features"><a href="#2-4-tf-train-Features" class="headerlink" title="2.4 tf.train.Features"></a>2.4 tf.train.Features</h3><p><strong>注意,是features,作用：其属性为特征，就是将我们的数据转为特征后进行存储</strong></p>
<pre><code>__init__(**kwargs) 
作用:初始化Features对象,一般我们是传入一个字典,字典的键是一个字符串,表示名字,字典的值是一个tf.train.Feature对象.
</code></pre><h3 id="2-5-tf-train-Feature"><a href="#2-5-tf-train-Feature" class="headerlink" title="2.5 tf.train.Feature"></a>2.5 tf.train.Feature</h3><p><strong>作用：其是上面的类输入字典的值</strong></p>
<pre><code>bytes_list =  tf.train.BytesList(value = [XXXX])
float_list = tf.train.FloatList(value = [XXXX])
int64_list = tf.train.Int64List(value = [XXXX])
</code></pre><h2 id="3-Create-TFRecord"><a href="#3-Create-TFRecord" class="headerlink" title="3.Create TFRecord"></a>3.Create TFRecord</h2><p> 由于在训练的时候，本人给出的label标签是onehot后的编码，但是在上述tf.train.Feature中其int64和float都是单个数值，因此将其变为bytes形式进行存储。</p>
<pre><code># coding:utf-8
import os
import tensorflow as tf
from PIL import Image
import numpy as np
import datetime
import time
from sklearn.preprocessing import OneHotEncoder
enc = OneHotEncoder()
classes = np.array(np.arange(2))
enc.fit(classes.reshape(-1, 1))

count = 0
cwd = &#39;F:/VCAQT/data/&#39;  # 分类好的0-1文件夹的目录地址
writer = tf.python_io.TFRecordWriter(&quot;32x32train.tfrecords&quot;)
t1 = time.time()
for index, name in enumerate(classes):
    class_path = cwd + str(name) + &#39;/&#39;
    print(class_path)
    for img_name in os.listdir(class_path):
        img_path = class_path + img_name

        img = Image.open(img_path)
        img = img.resize((32, 32))
        img_raw = img.tobytes()

        label_raw = enc.transform(index).toarray()
        label_raw = label_raw.tobytes()

        example = tf.train.Example(features=tf.train.Features(feature={
            &quot;label&quot;: tf.train.Feature(bytes_list=tf.train.BytesList(value=[label_raw])),
            #int64_list=tf.train.Int64List(value=[name])
            &#39;img_raw&#39;: tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_raw]))
        }))

        writer.write(example.SerializeToString())
        count += 1
        log_str = &#39;&#39;&#39;{},img_name：{},Count: {},label_name: {}&#39;&#39;&#39;.format(
            datetime.datetime.now().strftime(&#39;%m-%d %H:%M:%S&#39;),
            img_name,
            count,
            name)
        print(log_str)
writer.close()
</code></pre><h2 id="4-Reader-TFRecord"><a href="#4-Reader-TFRecord" class="headerlink" title="4.Reader TFRecord"></a>4.Reader TFRecord</h2><p>这里是进行文件的读取，文件读取方面要和存储相对应，比如图片的尺寸以及存储的标签等等。</p>
<pre><code>import tensorflow as tf
import os
import numpy as np

def fileExist(filename):
    if(os.path.exists(filename)):
        return 1
    else:
        print(&#39;&#39;&#39;Not {} in this path&#39;&#39;&#39;.format(filename))
        return 0

def read_and_decode(filename):
    if(fileExist(filename) == 1):
        filename_queue = tf.train.string_input_producer([filename])
        reader = tf.TFRecordReader()
        _, serialized_example = reader.read(filename_queue)
        features = tf.parse_single_example(serialized_example,
                                       features={&#39;label&#39;: tf.FixedLenFeature([], tf.string),
                                                 &#39;img_raw&#39;: tf.FixedLenFeature([], tf.string),
                                                 })
        img = tf.decode_raw(features[&#39;img_raw&#39;], tf.uint8)
        img = tf.reshape(img, [32, 32, 3])
        img = tf.cast(img, tf.float32) * (1. / 255) - 0.5
        label = tf.decode_raw(features[&#39;label&#39;], tf.float64)
        label = tf.reshape(label, [2])
        return img, label
    else:
        pass

if __name__ == &#39;__main__&#39;:
    img, label = read_and_decode(&quot;32x32train.tfrecords&quot;)
    img_batch, label_batch = tf.train.shuffle_batch([img, label], batch_size=50,
                                                    capacity=2000, min_after_dequeue=1000)
    init = tf.global_variables_initializer()
    with tf.Session() as sess:
        sess.run(init)
        coord = tf.train.Coordinator()
        threads = tf.train.start_queue_runners(sess=sess, coord=coord)
        print(img.shape, label.shape)
        for i in range(10):
            images, labels = sess.run([img_batch, label_batch])
        print(&quot;batch shape = &quot;, images.shape, &quot;labels = &quot;,
              labels.shape)
        coord.request_stop()
        coord.join()
</code></pre><h2 id="5-注意事项"><a href="#5-注意事项" class="headerlink" title="5.注意事项"></a>5.注意事项</h2><h3 id="5-1-在利用TFRecord进行读写的时候，如果你的文件名不对，一般程序是会卡死的"><a href="#5-1-在利用TFRecord进行读写的时候，如果你的文件名不对，一般程序是会卡死的" class="headerlink" title="5.1 在利用TFRecord进行读写的时候，如果你的文件名不对，一般程序是会卡死的"></a>5.1 在利用TFRecord进行读写的时候，如果你的文件名不对，一般程序是会卡死的</h3><p><img src="/../../../images/DeepLearning/Tensorflow/TFRecord/文件名不对python卡死.png" alt="Alt text"></p>
<h3 id="5-2-因为我上述存的是-0-1-oneHot之后的，但是在读取的时候"><a href="#5-2-因为我上述存的是-0-1-oneHot之后的，但是在读取的时候" class="headerlink" title="5.2 因为我上述存的是[0.,1.]oneHot之后的，但是在读取的时候"></a>5.2 因为我上述存的是[0.,1.]oneHot之后的，但是在读取的时候</h3><pre><code>label = tf.decode_raw(features[&#39;label&#39;], tf.float32)
label = tf.reshape(label, [2])
</code></pre><p>会出现如下错误：<br>at queue_ops.cc:105 : Invalid argument: Shape mismatch in tuple component 1. Expected [2], got [4]。说明变为四项，这个跟存储有关系。原本0,1是float64的，读进来为float32之后就相当于四个数了，这个错误也记录一下。</p>
<h2 id="6-参考"><a href="#6-参考" class="headerlink" title="6.参考"></a>6.参考</h2><p>1.<a href="https://blog.csdn.net/xierhacker/article/details/72357651" target="_blank" rel="noopener">保存TFRecord文件</a><br>2.<a href="http://www.tensorfly.cn/tfdoc/api_docs/python/python_io.html" target="_blank" rel="noopener">Data IO (Python functions)</a></p>
]]></content>
      <categories>
        <category>DeepingLearning</category>
      </categories>
      <tags>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>SENet</title>
    <url>/2018/07/13/DeepLearning/Classficaition/SE-Net/</url>
    <content><![CDATA[<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1.引言"></a>1.引言</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;卷积神经网络的发展主要从几个方向走，从网络深度上讲，CNN发展方向朝着层数变得更多的方向走，如ResNet是AlexNet的20多倍。通过网络深度的增加，网络能够利用增加的层数以及非线性函数来得到目标函数的近似结构，以获得更好的特征表达。当然也有研究者从内部出发，在卷积网络表达上从时间轴上或者从特征空间上(feature map)进行attention。而我们今天谈的SE-Net主要是从图像的通道(channal)上进行的。其主要思想是图像的表达中，通道这个维度对我们的识别非常的有效果，因此提出了SE块来对通道的权重进行学习，并构建一个基于权重的特征空间。<br><a id="more"></a></p>
<h2 id="2-SE块"><a href="#2-SE块" class="headerlink" title="2.SE块"></a>2.SE块</h2><p><img src="/../../../images/DeepLearning/Classfication/SE-Net/se_bolck.png" alt="Alt text">&nbsp;&nbsp;&nbsp;&nbsp;描述如上图所示：首先是任意给定的X，其形状为$(\acute{W},\acute{H},\acute{C})$分别代表图像的长度、宽度以及通道数。其经过$U = F_{tr}\times X$计算后变成了U，形状为$(W,H,C)$。其中$F_{tr}$是一些卷积作用。接下来在U的基础上加上我们的<strong>SE</strong>块。这里介绍下我们的SE块的全称叫挤压和激励(Squeeze-and-Excitation)其分为挤压块和激励块。</p>
<h3 id="2-1挤压块-Squeeze"><a href="#2-1挤压块-Squeeze" class="headerlink" title="2.1挤压块(Squeeze)"></a>2.1挤压块(Squeeze)</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;挤压块目的是获取我们全局特征表达中通道的描述。此例子中采用的是全局平均池化(global average pooling)去得到一个值来描述通道的统计特征。因为U是有C个通道的卷积层，因此得到的统计特征$Z\epsilon R^c$,有c个维度。</p>
<script type="math/tex; mode=display">Z_c = F_{sq}(U_c)=\frac{1}{W\times H}\sum_{i=1}^W{\sum_{j=1}^H{u_c(i,j)}}</script><p>&nbsp;&nbsp;&nbsp;&nbsp;每一个维度的值就等于其图像的所有的点的平均值。当然这里是单单就用了全局平均池化,但实际上其实有很多复杂的聚合策略低可以用来描述统计信息。</p>
<h3 id="2-2刺激块-Excitation"><a href="#2-2刺激块-Excitation" class="headerlink" title="2.2刺激块(Excitation)"></a>2.2刺激块(Excitation)</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;为了利用挤压(Squeeze)中聚合的信息来捕获通道的依赖项，作者设计了这么一个刺激块。首先他肯定要能表达非线性特征，其次他能够学会一种非互斥关系(防止一个通道被激活，而其他通道没有作用，则达不到效果，这句话我是这么理解的，不知道确不确切，后续可以交流)。基于上述两个设计，作者采用了基于sigmod输出的门控机制。</p>
<script type="math/tex; mode=display">scale = F_{ex}(z,W)=\sigma(g(z,W))=\sigma(W_2\delta(W_1z))</script><p>&nbsp;&nbsp;&nbsp;&nbsp;因为最后输出的是输入的C个通道权重值，所以这里用scale(权重的另外一个说法叫缩放尺度)来描述。$\delta$函数是参考了修正线性单元(ReLU)，$W_1\epsilon R^{\frac{C}{r}\times C}$,$W_2\epsilon R^{C\times \frac{C}{r}}$,这里$W_1$的作用是其是将C维度的信息变为$\frac{C}{r}$维度的，这是降维度。同时$W_2$的作用就是升维度，还是将其变为C维度。$F_{ex}$是sigmod函数，那么最后输出的是C维度的值，且大小都在(0,1)之间，这就是权重了。<br><img src="/../../../images/DeepLearning/Classfication/SE-Net/useful_seblock.png" alt="Alt text"><br>&nbsp;&nbsp;&nbsp;&nbsp;图中所示，这就是一个完整的SE块的应用了。其输入维度和输出维度相同，因此可以添加在任何的层之间。<br>&nbsp;&nbsp;&nbsp;&nbsp;聊到这儿了，之前本人对刺激块为什么要先降维度再升维度有一些疑惑。为何不直接一个全连接层呢。作者在其论文<strong>3.1</strong>里面做了解释：为了限制模型的复杂性和辅助推广，在非线性范围内形成了一个具有两个全连通(FC)层，即一个具有参数W1的降维层，一个具有W2的维数增加层。这里可以做一个计算。对于输入为$w_i\times h_i\times c_i$的层，一个$s_k\times s_k$的卷积核的参数个数为$s_k\times s_k\times c_i$。当然这是单个卷积的，相当于输出单通道的，我们输出为多个通道的如$w_o\times h_o\times c_o$总参数为$s_k\times s_k\times c_i\times c_o$。因此总的过程是：</p>
<script type="math/tex; mode=display">w_i\times h_i\times c_i--->c_o个s_k\times s_k\times c_i卷积核--->w_o\times h_o\times c_o</script><p>&nbsp;&nbsp;&nbsp;&nbsp;从第二个到第三个是如何变的呢，其实是在$c_i$通道上做了累加。理解了这一层之后，我们接下来再看看为什么降维度。如我所说，如果用一个全连接层，参数数量是$1\times 1\times c\times c$，用了全两个全连接层之后，参数的数量是</p>
<script type="math/tex; mode=display">1\times 1\times c\times \frac{c}{r}+1\times 1\times \frac{c}{r}\times c=1\times 1\times c\times \frac{2c}{r}</script><p>&nbsp;&nbsp;&nbsp;&nbsp;因此从中可以看出，其通过这样的方式，多了一个参数r来控制模型的训练参数的数量，以根据模型的情况进行更改，当然作者也给出了其试验的比较好的值r=16。</p>
<h2 id="3-SE块在其他网络中的应用"><a href="#3-SE块在其他网络中的应用" class="headerlink" title="3.SE块在其他网络中的应用"></a>3.SE块在其他网络中的应用</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;SE块由于其输出的是输入的feature map在每个通道上的权重,最后输出<script type="math/tex">X^~=F_{scale}(X,Scale)</script>,因此其可以嵌入到任何已知的网络。作者介绍了其插入Inception module 和Resnet的方法。<br><img src="/../../../images/DeepLearning/Classfication/SE-Net/se-Inception.png" alt="Alt text"><br><img src="/../../../images/DeepLearning/Classfication/SE-Net/se-ResNet.png" alt="Alt text"></p>
<h2 id="4-代码分析"><a href="#4-代码分析" class="headerlink" title="4.代码分析"></a>4.代码分析</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;本次代码采用的是在github上<a href="https://github.com/taki0112/SENet-Tensorflow" target="_blank" rel="noopener">SENet-Tensorflow</a>进行解析。</p>
<pre><code>def Squeeze_excitation_layer(self, input_x, out_dim, ratio, layer_name):
    with tf.name_scope(layer_name) :
        squeeze = Global_Average_Pooling(input_x)
        excitation = Fully_connected(squeeze, units=out_dim / ratio, layer_name=layer_name+&#39;_fully_connected1&#39;)
        excitation = Relu(excitation)
        excitation = Fully_connected(excitation, units=out_dim, layer_name=layer_name+&#39;_fully_connected2&#39;)
        excitation = Sigmoid(excitation)
        excitation = tf.reshape(excitation, [-1,1,1,out_dim])
        scale = input_x * excitation
        return scale
</code></pre><p>&nbsp;&nbsp;&nbsp;&nbsp;主要就是这一段程序，因为其可以嵌入到各种网络中。其实理解了上面的SE会那么对这一段程序也就理解了。<br>&nbsp;&nbsp;&nbsp;&nbsp;这里几个输入的参数理解一下。input_x这个就是我们上一层的输出，out_dim是我们的输出权重的维度，ratio就是我们上述推到的r,用来降维度和升维度来减少参数的。首先经过全局平均，接下来全连接层降维，经过线性修正单元后再升维度。最后经过sigmoids，再将其与输入相乘，得到最后SE层处理后的值。</p>
<h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5.总结"></a>5.总结</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;经过上述的描述，相信对SE块有了一定的认识。其相当于是一个模块，用于通道维度的信息更好的利用起来进行特征的表达，通过实验发现，其效果确实很显著。因此将其嵌入到很多已有的网络能达到很好的效果。本人写这篇也是基于这篇论文浅显的理解，有任何错误希望指正。接下来会在这基础上对网络进行训练测试效果。</p>
<h2 id="6-参考"><a href="#6-参考" class="headerlink" title="6.参考"></a>6.参考</h2><p>1.Hu J, Shen L, Sun G. Squeeze-and-Excitation Networks[J]. 2017.<br>2.<a href="https://github.com/taki0112/SENet-Tensorflow" target="_blank" rel="noopener">SENet-Tensorflow</a></p>
]]></content>
      <categories>
        <category>DeepingLearning</category>
      </categories>
      <tags>
        <tag>Classfication</tag>
      </tags>
  </entry>
  <entry>
    <title>Sublime text3+Anaconda</title>
    <url>/2018/07/04/MachineLearning/Sublime%20text3+Anaconda/</url>
    <content><![CDATA[<h2 id="1-下载sublimetext3"><a href="#1-下载sublimetext3" class="headerlink" title="1.下载sublimetext3"></a>1.下载sublimetext3</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;可以在官网下载<a href="https://www.sublimetext.com/3" target="_blank" rel="noopener">sublimme text3</a>支持linux,window等等，下载自己对应的版本，正常安装就可以了。<br><a id="more"></a></p>
<h2 id="2-下载Anaconda"><a href="#2-下载Anaconda" class="headerlink" title="2.下载Anaconda"></a>2.下载Anaconda</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;可以在官网下载<a href="https://www.anaconda.com/download/" target="_blank" rel="noopener">Anaconda</a>也是支持linux,window等等，下载对应的版本安装即可。<br>安装好了Anconda之后，如果你修改了安装路径之后你要进行环境配置。这里验证一下：打开你的cmd命令窗口，输入python.如果出现“python不是命令”等提示，那么就是环境变量还没配置，接下来可以采用如下配置：<br>(1)右击桌面上的此电脑，选择属性。<br>(2)在最左边控制面板主页那一栏有一个高级系统设置。<br>(3)进入界面之后，选择环境变量。<br><img src="/../../images/MachineLearning/sublime+Anaconda/environment.jpg" alt="Alt text">(4)之后选择系统变量的path,点击编辑。加上你的Anconda目录。比如我是安装在D:\Anaconda点击应用即可。<br>(5)接下来你再在cmd中输入python，你会发现已经可以用了<br><img src="/../images/sublime+Anaconda/environment test.jpg" alt="Alt text"></p>
<h2 id="3插件安装"><a href="#3插件安装" class="headerlink" title="3插件安装"></a>3插件安装</h2><h3 id="3-1PackageControl安装"><a href="#3-1PackageControl安装" class="headerlink" title="3.1PackageControl安装"></a>3.1PackageControl安装</h3><h4 id="3-1-1在线安装"><a href="#3-1-1在线安装" class="headerlink" title="3.1.1在线安装"></a>3.1.1在线安装</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;打开<a href="https://packagecontrol.io/installation#st3" target="_blank" rel="noopener">PackageControl安装</a>找到你对应的版本sublime.<img src="/../../images/MachineLearning/sublime+Anaconda/packcontrol.jpg" alt="Alt text">使用Ctrl+`(或View&gt;ShowConsolemenu)打开SublimeText控制台，将上面的Python代码粘贴到控制台里，按enter然后等待。</p>
<h4 id="3-1-2手动安装"><a href="#3-1-2手动安装" class="headerlink" title="3.1.2手动安装"></a>3.1.2手动安装</h4><p>如果不能自动安装的话，可以进行手动安装：<br>(1)点击菜单栏Preferences-&gt;BrowsePackages会打开文件夹如…..\SublimeText3\Packages3<br>(2)进入上层的..\SublimeText3<br>(3)到PackageControl官网下载插件：PackageControl.sublime-package安装包，下载地址，并复制文件到InstalledPackages里面。<img src="/../../images/MachineLearning/sublime+Anaconda/setup.jpg" alt="Alt text">使用Ctrl+`(或View&gt;ShowConsolemenu)<br>(4)重启SublimeText插件。</p>
<h3 id="3-2Anconda插件安装与配置"><a href="#3-2Anconda插件安装与配置" class="headerlink" title="3.2Anconda插件安装与配置"></a>3.2Anconda插件安装与配置</h3><h4 id="3-2-1安装步骤"><a href="#3-2-1安装步骤" class="headerlink" title="3.2.1安装步骤"></a>3.2.1安装步骤</h4><p>(1)打开PackageControl[Preferences&gt;&gt;PackageControl]输入install选择关联出来的installpackage<img src="/../../images/MachineLearning/sublime+Anaconda/插件安装1.jpg" alt="Alt text"><br>(2)输入Anaconda，因为我已经装了，所以这边没显示，第一次安装的时候是有显示的。然后等待一会儿可以了。<img src="/../../images/MachineLearning/sublime+Anaconda/插件安装2.jpg" alt="Alt text"><br>(3)安装完之后可以通过【Preferences&gt;&gt;PackageSettings】中查看到已安装的插件<br><img src="/../../images/MachineLearning/sublime+Anaconda/插件安装3.jpg" alt="Alt text"></p>
<h4 id="3-2-2Anaconda环境变量配置"><a href="#3-2-2Anaconda环境变量配置" class="headerlink" title="3.2.2Anaconda环境变量配置"></a>3.2.2Anaconda环境变量配置</h4><p>接下来可以进行sublimetext3中的Anaconda配置<br>(1)选择Preferens-&gt;PackageSetting-&gt;Anaconda-&gt;setting-Default中。<br>(2)利用ctrl+f来进行搜索，查找python_interpreter<br>(3)接下来将其修改为你的Anconda路径：并点击保存<img src="/../../images/MachineLearning/sublime+Anaconda/Anaconda配置1.jpg" alt="Alt text">(4)再选择Preferens-&gt;PackageSetting-&gt;Anaconda-&gt;setting-User中<br>(5)输入：</p>
<pre><code>{
&quot;python_interpreter&quot;:**&quot;D:/Anaconda/python.exe&quot;**,
&quot;suppress_word_completions&quot;:true,
&quot;suppress_explicit_completions&quot;:true,
&quot;complete_parameters&quot;:true,
}
</code></pre><p>注意，这里的路径还是你的Anconda的路径哦。点击保存即可</p>
<p>(6)重启sublimetext3，但是如果在每次启动的时候都会出现一个什么Anconda……….settingthe’swallow_startup_errors’totrue.此时如果不影响使用的情况下可以选择Preferens-&gt;PackageSetting-&gt;Anaconda-&gt;setting-Default中查找到’swallow…，并且设置为true<img src="/../../images/MachineLearning/sublime+Anaconda/Anaconda配置2.jpg" alt="Alt text"></p>
<h3 id="3-3剩余插件安装"><a href="#3-3剩余插件安装" class="headerlink" title="3.3剩余插件安装"></a>3.3剩余插件安装</h3><p><strong>Sublimecodeintel:</strong>这个插件是负责代码提示的，可以根据python进行自带代码提示。</p>
<p><strong>Pylinter:</strong>Python基本主题</p>
<p><strong>SublimeTmpl：</strong>新建文件模板插件，可以支持多种语言例如Python、PHP等，下面可以配置自己的信息。</p>
<pre><code>{
&quot;disable_keymap_actions&quot;:false,//&quot;all&quot;;&quot;html,css&quot;
&quot;date_format&quot;:&quot;%Y-%m-%d%H:%M:%S&quot;,
&quot;attr&quot;:{
&quot;author&quot;:&quot;hzw&quot;,
&quot;email&quot;:&quot;21732008@zju.edu.cn&quot;,
&quot;link&quot;:&quot;https://jaryhuang.github.io/&quot;
}
</code></pre><p>将Python的创建模板命令也做了修改，在keybindings-user配置使得ctrl+alt+p就可以创建一个新的Python模板：</p>
<pre><code>[
{
&quot;caption&quot;:&quot;Tmpl:Createpython&quot;,&quot;command&quot;:&quot;sublime_tmpl&quot;,
&quot;keys&quot;:[&quot;ctrl+alt+p&quot;],&quot;args&quot;:{&quot;type&quot;:&quot;python&quot;}
},
]
</code></pre><p><strong>Terminal:</strong>打开一个命令窗口，用于各种命令操作.</p>
<h2 id="4-安装用于Python的MATLAB引擎API"><a href="#4-安装用于Python的MATLAB引擎API" class="headerlink" title="4.安装用于Python的MATLAB引擎API"></a>4.安装用于Python的MATLAB引擎API</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;要在Python会话内启动MATLAB引擎，必须先安装Python包形式的引擎API。MATLAB提供了标准的Python setup.py文件，用于通过distutils模块编译和安装引擎。您可以使用相同的setup.py命令在Windows、Mac或Linux系统上编译和安装引擎。<br>(1)<strong>在安装之前，确认您的Python和MATLAB配置</strong><br>&nbsp;&nbsp;&nbsp;&nbsp;系统具有受支持的Python版本和MATLABR2014b或更新版本。要检查您的系统上是否已安装Python，请在操作系统提示符下运行Python。将包含Python解释器的文件夹添加到您的路径（如果尚未在该路径中）。这步的测试和之前的一样，在cmd中输入python，如果可以用就无需配置，如果不可以用，请参考Anaconda环境配置，或者我的文章python+Anconda+sublime Text3的配置。<br>(2)<strong>找到MATLAB文件夹的路径：</strong><br>&nbsp;&nbsp;&nbsp;&nbsp;启动MATLAB，并在命令行窗口中键入matlabroot。复制matlabroot所返回的路径，如下图所示<img src="/../../images/MachineLearning/sublime+Anaconda/matlab1.jpg" alt="Alt text">(3)<strong>安装：</strong><br>在Windows系统中:</p>
<pre><code>cd &quot;matlabroot\extern\engines\python&quot;
python setup.py install
</code></pre><p>在Mac或Linux系统中:</p>
<pre><code>cd &quot;matlabroot/extern/engines/python&quot;
python setup.py install
</code></pre><h2 id="5-参考"><a href="#5-参考" class="headerlink" title="5.参考"></a>5.参考</h2><p>1.<a href="https://www.cnblogs.com/jxldjsn/p/6034158.html0" target="_blank" rel="noopener">sublimetext3+python配置，完整搭建及常用插件安装</a><br>2.<a href="https://www.zybuluo.com/king/note/47271" target="_blank" rel="noopener">SublimeText3配置和使用方法</a></p>
]]></content>
      <categories>
        <category>MachineLearning</category>
      </categories>
      <tags>
        <tag>python编辑插件配置</tag>
      </tags>
  </entry>
  <entry>
    <title>The impact of data imbalance and solution</title>
    <url>/2018/06/30/MachineLearning/The%20impact%20of%20data%20imbalance%20and%20solutions/</url>
    <content><![CDATA[<h2 id="1-数据不平衡的影响"><a href="#1-数据不平衡的影响" class="headerlink" title="1.数据不平衡的影响"></a>1.数据不平衡的影响</h2><p><strong>机器学习(Machine learning)的训练(train)和预测(predict)受到数据集不平衡问题的影响。</strong><br>&nbsp;&nbsp;&nbsp;&nbsp;举个例子，当我们进行判断题考试训练的时候，对的打勾错的打叉。试卷过来发现1000题的判断题，真实答案中有990道题是打叉的，只有10道题是打勾的。那在此时我们会想要偷个懒，到时候真正考试的时候，我全打叉，那起码也有99分左右的水平了，哈哈大笑。我们人能学到这个规律，那么机器当然也能学到这个规律了。<br>&nbsp;&nbsp;&nbsp;&nbsp;除此之外，特征选择是机器学习中不可或缺的一步伐，但是当样本不平衡(data unbalanced)的情况下，<strong>部分特征(feature)对于少量样本(sample)的描述比较少,导致算法认为贡献度低</strong>。因为算法的标准是找出分类准确度高的特征，也就是上个例子中表述的让答案都选错的特征，因此那些偏向让我们答案选对的特征显的就没那么重要了。<br><a id="more"></a></p>
<h2 id="2-数据不平衡的解决方案"><a href="#2-数据不平衡的解决方案" class="headerlink" title="2.数据不平衡的解决方案"></a>2.数据不平衡的解决方案</h2><h3 id="2-1-代价损失函数角度——非均等代价-unequal-cost"><a href="#2-1-代价损失函数角度——非均等代价-unequal-cost" class="headerlink" title="2.1 代价损失函数角度——非均等代价(unequal cost)"></a>2.1 代价损失函数角度——非均等代价(unequal cost)</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;在现实生活中会遇到一些情况：不同类型的错误造成的后果是不同的。例如在医疗癌症诊断中，如果我们将健康的人给检测为癌症患者，那么会增加了进一步检测的麻烦。但是如果将癌症患者诊断为健康人，那么后果可能就是丧失了拯救生命的最佳时机。这两类错误是不对等的，为了权衡不同类型的错误造成的损失不同，我们可为错误赋予非均等代价(unequal cost)。举个例子，在我们的考试中，取个极端情况，错的判为对的-1分，对的判为错的-100分。那么我们在考试中就会极其关注，肯定不会出现上述都打叉的情况了，因为极有可能是0分了。同时，在做练习题的的时候，也会特别的关注打勾的答案对应的特征(重点词汇之类的)。<br>&nbsp;&nbsp;&nbsp;&nbsp;但是在利用非均等代价(unequal cost)难点就在于如何设置你的代价矩阵(cost matrix)<br>以二分类任务为例如表所示：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">真实类别\预测类别</th>
<th style="text-align:center">第0类</th>
<th style="text-align:center">第1类</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">第0类</td>
<td style="text-align:center">0</td>
<td style="text-align:center">$cost_01$</td>
</tr>
<tr>
<td style="text-align:center">第1类</td>
<td style="text-align:center">$cost_10$</td>
<td style="text-align:center">0</td>
</tr>
</tbody>
</table>
</div>
<p>总体代价(total cost)也称为“代价敏感”(cost-sensitive)：</p>
<script type="math/tex; mode=display">E(f;D;cost)=\frac{1}{m}(\sum_{x_i\in D^+}{I(f(x_i)\neq y_i) \times cost_{01}}+ \sum_{x_i\in D^-}{I(f(x_i)\neq y_i)  \times cost_{10}})</script><p>&nbsp;&nbsp;&nbsp;&nbsp;其中的最难定义的就是$cost_{01}$和$cost_{10}$,其值的确定使得我们加入了一个先验知识。但是这并不影响我们使用。最常见的是将其与类别比设置为一样，这基于的假设是训练集是真实样本总体的无偏采样(这个假设一般很难成立)。</p>
<h3 id="2-2-分类器的决策角度——再平衡-rebalance"><a href="#2-2-分类器的决策角度——再平衡-rebalance" class="headerlink" title="2.2 分类器的决策角度——再平衡(rebalance)"></a>2.2 分类器的决策角度——再平衡(rebalance)</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;事实上我们的分类决策方法是用预测出的y和一个阈值(threshold)进行比较，通常以0.5作为阈值。而几率$\frac{y}{1-y}$反应了正例可能性和反例可能性的比值。阈值设置为0.5恰表明了分类器认为正反例可能性相同，都为0.5。假设我们采集到的训练集是真实样本总体的无偏估计，正例为$m^+$,负例为$m^-$,观测几率则为<script type="math/tex">\frac{m^+}{m^-}</script>由于假设的无偏估计，那么观测几率就代表了真实的几率:<script type="math/tex">\frac{y}{1-y}>\frac{m^+}{m^-}</script>则预测为正例。<br>&nbsp;&nbsp;&nbsp;&nbsp;再平衡的思想虽然简单，但是训练样本是真实样本总体的无偏估计这个假设往往不成立，但是这也作为一种策略来对不平衡训练样本进行处理，称之为阈值移动(threshold-moving)。<br><strong>再平衡是非均等代价基础，是从不同的角度解决问题的类似方法。</strong></p>
<h3 id="2-3-训练的样本角度——采样-sampling"><a href="#2-3-训练的样本角度——采样-sampling" class="headerlink" title="2.3 训练的样本角度——采样(sampling)"></a>2.3 训练的样本角度——采样(sampling)</h3><h4 id="2-3-1-过采样-over-sampling"><a href="#2-3-1-过采样-over-sampling" class="headerlink" title="2.3.1 过采样(over-sampling)"></a>2.3.1 过采样(over-sampling)</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;过采样简单理解就是对于数据比较少的一类，我们进行数据补充，让少数类和多数类数据接近或者相当，接下来我们来了解过采样中的几种方法。</p>
<h5 id="2-3-1-1-随机过采样-RandomOverSampler"><a href="#2-3-1-1-随机过采样-RandomOverSampler" class="headerlink" title="2.3.1.1 随机过采样(RandomOverSampler)"></a>2.3.1.1 随机过采样(RandomOverSampler)</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;随机过采样方法，也称之为野蛮过采样方法。其是一种最简单的方法，通过有放回的进行重采样，不断的从少数的类中抽取，以达到扩充样本的目的。我们可以自己编写函数进行随机重采样，当然imblearn库中有这个的应用，但是其称之为(Naive random over-sampling)：</p>
<pre><code>from imblearn.over_sampling import RandomOverSampler
ros = RandomOverSampler(random_state=0)
X_resampled, y_resampled = ros.fit_sample(X, y)
</code></pre><p>RandomOverSampler()中的参数ratio可以进行设置你采样的数据。</p>
<h5 id="2-3-1-2-合成少量超采样技术-SMOTE"><a href="#2-3-1-2-合成少量超采样技术-SMOTE" class="headerlink" title="2.3.1.2 合成少量超采样技术(SMOTE)"></a>2.3.1.2 合成少量超采样技术(SMOTE)</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;SMOTE是在2002年被提出,其核心算法(Algorithm)如下,对于一些细节本人做了一些小修改：</p>
<pre><code>**Algorithm** SMOTE(S,N,k):
**Input:** S is the minority class samples;Amount of SMOTE N(ratio);Number of nearest neighbor k
**Output:**N*T synthetic mniority class samples
For each point p in S:
    1.compute the KNN in S to require the nnarray.
    2.choose randomly $r\leq k$ in the nnarray.
    3.for each point s_p in selected r,the vector is p to s_p,choose a random point along the lines to join to the new_sample.
4.add these synthetic points(new_sample) to the dataset.
</code></pre><p>计算核心的公式是，假设我们新产生的点是$x_{new}$,起源的点是$x_i$,其KNN中的其中一个点是$x_{zi}$。则计算公式是：<script type="math/tex">x_{new} = x_i + \lambda \times (x_{zi} - x_i) \lambda \in [0,1]</script><br><img src="/../../images/MachineLearning/imbalance_data/new_point.png" alt="Alt text"></p>
<h5 id="2-3-1-3-边缘性合成少量超采样技术-Borderline-SMOTE"><a href="#2-3-1-3-边缘性合成少量超采样技术-Borderline-SMOTE" class="headerlink" title="2.3.1.3 边缘性合成少量超采样技术(Borderline-SMOTE)"></a>2.3.1.3 边缘性合成少量超采样技术(Borderline-SMOTE)</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;这是对SMOTE改进的一个算法，其相对于SMOTE改进的点在于到底选取S中的哪些点我们进行扩增，因为原始的算法是对所有S中的点都进行扩增。具体的改进如下：</p>
<pre><code>For each point in S:
    (1)noise:  all nearest-neighbors are from L.
    (2)danger: more than half of nearest-neighbors are from L,add each point to the DANGER.
    (3)safe:   more than half of nearest-neighbors are from S.
For each point in DANGER，apply the SMOTE algorithm to generate synthetic mniority class samples.
</code></pre><p>&nbsp;&nbsp;&nbsp;&nbsp;有些人的理解，不应是将安全的点进行扩增么，这样证明我们扩增的点是正确的概率就越大了。但是得明确我们最终目的是想要使我们的分类正确，那么我在危险的点的中间多加一些点，使我得危险点又变为了安全点，何乐而不为。<br>在边缘性合成少量超采样技术中也存在两个变种：<br>Borderline-1 SMOTE：$x_{zi}$是属于与$x_i$(危险点)不同的类(就是数据比较多的那个类)<br>Borderline-2 SMOTE：$x_{zi}$是属于任何类都行。<br>上述的方法imblance都有实现：</p>
<pre><code>from imblearn.over_sampling import SMOTE, ADASYN
X_resampled, y_resampled = SMOTE().fit_sample(X, y)
</code></pre><p>&nbsp;&nbsp;&nbsp;&nbsp;通过kind参数可以对方法进行选择，其还提供了SVM的方式，核心思想都不变，只是最近邻的方式变成由支持向量机来进行划分。同时，在我们的方法中还提供了一种ADASYN的方法，具体可以去对应的库中进行了解。</p>
<h4 id="2-3-2-欠采样-under-sampling"><a href="#2-3-2-欠采样-under-sampling" class="headerlink" title="2.3.2 欠采样(under-sampling)"></a>2.3.2 欠采样(under-sampling)</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;欠采样，同样比较好理解，其就是对多数的那个类进行缩小，使数据达到平衡，接下来我们来了解过采样中的几种方法。</p>
<h5 id="2-3-2-1-随机欠采样-RandomUnderSampler"><a href="#2-3-2-1-随机欠采样-RandomUnderSampler" class="headerlink" title="2.3.2.1 随机欠采样(RandomUnderSampler)"></a>2.3.2.1 随机欠采样(RandomUnderSampler)</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;随机欠采样方法是一种最简单的方法，通过对多数的类进行采样，使得多数类和少数类数据相当。imblearn库中有这个的应用：</p>
<pre><code>from imblearn.under_sampling import RandomUnderSampler
rus = RandomUnderSampler(random_state=0)
</code></pre><p>RandomUnderSampler()中的参数ratio可以进行设置你采样的数据，具体不细讲。</p>
<h5 id="2-3-2-2-相近差错-NearMiss"><a href="#2-3-2-2-相近差错-NearMiss" class="headerlink" title="2.3.2.2 相近差错(NearMiss)"></a>2.3.2.2 相近差错(NearMiss)</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;刚刚的随机欠采样是无规则的进行随机采样，而相近差错法则是在随机欠采样的基础上进行有规则的采样。假设我们的正例是多数类，需要被欠采样的，负例是少数类。<br><strong>NearMiss-1：</strong>从所有的正样本中，找出其与N个负类最近邻的平均距离最短的正样本,舍弃掉那些距离远的。<br><img src="/../../images/MachineLearning/imbalance_data/NearMiss-1.png" alt="Alt text"><br>&nbsp;&nbsp;&nbsp;&nbsp;这种方法的核心思想是将距离近的样本留下，距离远的删除。当我们训练数据的时候，两者接近的数据我们都能区分，那么那些距离分离面远的数据当然没问题。<strong>但是这种方法很容易被噪声影响，因为在原始数据中可能就存在噪声(分离面的周围产生的噪声对分类影响最大)</strong><br><strong>NearMiss-2：</strong><br>从所有的正样本中，找出其与N个负类最远邻的平均距离最短的正样本。<br><img src="/../../images/MachineLearning/imbalance_data/NearMiss-2.png" alt="Alt text"><br>&nbsp;&nbsp;&nbsp;&nbsp;这种方法的聚焦在最远的样本上，这样在分离面上的噪声对其干扰相对较少，但是其对边缘离群值比较敏感，边缘离群噪声对其会有很大的干扰。<br><strong>NearMiss-3：</strong><br>&nbsp;&nbsp;&nbsp;&nbsp;这种方法分为两部分：(1)对于每一个负样本，选出其M个最近邻样本。(2)然后对于正样本，挑选出N个K近邻平均距离最大的样本。<br><img src="/../../images/MachineLearning/imbalance_data/NearMiss-3.png" alt="Alt text">这相当于是上述两者的结合，来减少两类噪声的影响。</p>
<h5 id="2-3-2-3-移除Tomek连接-Tomek-Link-Removal"><a href="#2-3-2-3-移除Tomek连接-Tomek-Link-Removal" class="headerlink" title="2.3.2.3 移除Tomek连接(Tomek Link Removal)"></a>2.3.2.3 移除Tomek连接(Tomek Link Removal)</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;Tomek连接指的是两个样本的最近邻是彼此，也就是A是B的最近邻，B也是A的最近邻。那么我们操作方式是删除多数类的那个。</p>
<h5 id="2-3-2-4-修改最近邻法-Edited-Nearest-Neighbor"><a href="#2-3-2-4-修改最近邻法-Edited-Nearest-Neighbor" class="headerlink" title="2.3.2.4 修改最近邻法(Edited Nearest Neighbor)"></a>2.3.2.4 修改最近邻法(Edited Nearest Neighbor)</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;ENN是一种利用最近邻算法进行欠采样的处理方法。对于L类，如果他的大部分K近邻样本都和他本身的类别不一样，我们就将其删除。Repeated Edited Nearest Neighbours(RENN)就是重复这个算法多次，然后对数据进行删除。当然这里面就涉及到多个参数了。比如最近邻的K是否可以进行迭代增加，重复的次数等想法都可以实现，也就出现了改进版了，这里不做详细说明。<br>&nbsp;&nbsp;&nbsp;&nbsp;另外，还有一些提出比如利用最近邻算法预测，将正例中的样本预测为负例的进行填充的负样本中、或者利用其它算法等等，这里就不说明了。</p>
<h4 id="2-3-3-组合方法-combination-of-over-and-under-sampling"><a href="#2-3-3-组合方法-combination-of-over-and-under-sampling" class="headerlink" title="2.3.3 组合方法(combination of over- and under- sampling)"></a>2.3.3 组合方法(combination of over- and under- sampling)</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;组合方法就是讲过采样和欠采样方法进行组合，以SMOTETomek为例：首先SMOTE产生噪声点，然后通过Tomek进行消噪，那么留下来的自然好的概率就大了。代表新的算法为<br>(1)合成少量超采样技术+移除Tomek连接(SMOTETomek)<br>(2)合成少量超采样技术+修改最近邻法(SMOTEENN)</p>
<h3 id="2-4-算法的角度——集成-ensemble"><a href="#2-4-算法的角度——集成-ensemble" class="headerlink" title="2.4 算法的角度——集成(ensemble)"></a>2.4 算法的角度——集成(ensemble)</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;EasyEnsemble是一个简单的方法，其思想是使得在对算法进行训练的时候，每个模型输入的都是平衡的样本，最后将这些样本集成起来：</p>
<pre><code>**Algorithm** EasyEnsemble(N):
**Input:** N is the number of model
**Output:**final model
1.For each model:
    (i)  sample randomly in L to generate Li whose data size is same as S
    (ii) use Li and S to train the model
2.Ensemble this model and get the final model
</code></pre><p>&nbsp;&nbsp;&nbsp;&nbsp;这就是EasyEasyEnsemble,跟Adaboost的整体框架非常相似。<br>&nbsp;&nbsp;&nbsp;&nbsp;而BalanceCascade算法和EasyEnsemble相似，但是其改进点在于，每次训练一个小模型结束，将L的样本进行分类，正确分类的就移除掉，然后进行重复训练。<br>还有一种变型，在最后模型集成的时候，是采用boosting的集成方式还是bagging的投票方式，这里也有一定的差别了。</p>
<h2 id="3-总结"><a href="#3-总结" class="headerlink" title="3.总结"></a>3.总结</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;讲到这里，对数据不平衡问题的处理方式也罗列的差不多了。这边没有讲具体算法的实现，主要是罗列了近几年提出的解决数据不平衡的问题的一些思路：<strong>从数据不平衡导致的影响出发，依靠损失函数、分类器决策、数据采样和算法角度等方面</strong>，总结了现有提出的一些方法以及个人的一些小见解。本篇的不足还在于没有依据具体的公有数据集进行实验来说明问题，等后续有时间再进行说明。</p>
<h2 id="4-参考"><a href="#4-参考" class="headerlink" title="4.参考"></a>4.参考</h2><p>1.<a href="https://blog.csdn.net/a358463121/article/details/52304670" target="_blank" rel="noopener">不平衡数据分类算法介绍与比较</a><br>2.<a href="http://contrib.scikit-learn.org/imbalanced-learn/stable/index.html" target="_blank" rel="noopener">imbalanced-learn</a><br>3.Chawla N V, Bowyer K W, Hall L O, et al. SMOTE: synthetic minority over-sampling technique[J]. Journal of Artificial Intelligence Research, 2011, 16(1):321-357.<br>4.More A. Survey of resampling techniques for improving classification performance in unbalanced datasets[J]. 2016.</p>
]]></content>
      <categories>
        <category>MachineLearning</category>
      </categories>
      <tags>
        <tag>data imbalance</tag>
      </tags>
  </entry>
  <entry>
    <title>Machine Learning Architecture</title>
    <url>/2018/03/20/MachineLearning/Machine-LearningArchitecture/</url>
    <content><![CDATA[<h2 id="1-机器学习定义、基本概念以及三要素"><a href="#1-机器学习定义、基本概念以及三要素" class="headerlink" title="1.机器学习定义、基本概念以及三要素"></a>1.机器学习定义、基本概念以及三要素</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;今天我们来简单的聊一聊机器学习的一个架构。我们从定义出发，机器学习,简而言之，是一门致力于利用data，通过learning algorithm来产生model的这么一门关于“学习算法”的学问。自然而言我们已经了解了机器学习的三要素:<br><strong>1.dataprocessing 2.model building 3.learning algorithm</strong><br>在接下来的学习中我也会分这三要素分别进行我的一个学习.<br><a id="more"></a></p>
<h3 id="1-1机器学习思维导图"><a href="#1-1机器学习思维导图" class="headerlink" title="1.1机器学习思维导图"></a>1.1机器学习思维导图</h3><p><img src="../../images/MachineLearning/Machine_Architecture/Machine_learning.png" alt="Machine_learning"></p>
<h3 id="1-2基本概念"><a href="#1-2基本概念" class="headerlink" title="1.2基本概念"></a>1.2基本概念</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;要了解机器学习，还需要知道<strong>Sample? Feature? Supervised Learning? Unsuper—vised Learning? Classification?Regression? Training Data? Validation Data? Testing Data? Bias? Variance? Underfit? Overfit?</strong>接下来我会通过一个最简单的例子来解释这些，以学校的学生为例.</p>
<h4 id="1-2-1-Sample-and-Feature"><a href="#1-2-1-Sample-and-Feature" class="headerlink" title="1.2.1.Sample and Feature"></a>1.2.1.Sample and Feature</h4><p><strong>Sample:</strong> 我们的研究对象是学生，我们学校有20000个学生，这20000个学生就是我们的sample了.<br><strong>Feature:</strong> 而对于每一个Sample,我们会发现每个学生会有名字,性别,身高,体重等等。那么其中的每一项就是我们的Feature了。对于每个sample,它的feature总和称为特征维数目。</p>
<h4 id="1-2-2-Training-Data、Validation-Data-and-Testing-Data"><a href="#1-2-2-Training-Data、Validation-Data-and-Testing-Data" class="headerlink" title="1.2.2.Training Data、Validation Data and Testing Data"></a>1.2.2.Training Data、Validation Data and Testing Data</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;接下来就聊聊我们的这三者的区别。要理解这三者，我们开头介绍了，机器学习主要是利用数据通过学习算法来进行模型选择的学科。这里的数据指的就是Training Data.<br><strong>Training Data:</strong> 用来训练我们模型的数据，进行我们的参数调节。<br><strong>Validation Data:</strong> 用来测试我们模型优劣以及作为调参的依据，用来确定最终模型。<br><strong>Testing Data:</strong> 依据测试数据，利用我们的模型来给出结果。</p>
<h4 id="1-2-3-Supervised-Learning-and-Unsupervised-Learning"><a href="#1-2-3-Supervised-Learning-and-Unsupervised-Learning" class="headerlink" title="1.2.3 Supervised Learning and Unsupervised Learning"></a>1.2.3 Supervised Learning and Unsupervised Learning</h4><p><strong>Supervised Learning:</strong> 我们有训练集,而且训练集是包括特征+结果.也就是已知结果的训练.<br><strong>Unsupervised Learning:</strong> 我们有训练集,但是我们并不知到这类训练集代表什么,也就是没有结果的一个训练.</p>
<h4 id="1-2-4-Classification-and-Regression"><a href="#1-2-4-Classification-and-Regression" class="headerlink" title="1.2.4 Classification and Regression"></a>1.2.4 Classification and Regression</h4><p><strong>Classification:</strong> 这是对于监督学习而言的，假设结果是离散的，比如性别男女，成绩好坏。这些都是类别问题。根据类别的多少又分为二分类和多分类问题。<br><strong>Regression:</strong> 同样是对于监督学习而言的，假设结果是连续的，比如人体水的含量。这就是我们的回归问题了。<br>何为分类和何为回归是根据我们预测的结果而来的。</p>
<h4 id="1-2-5-Training-error-Testing-error-and-Generalization-error"><a href="#1-2-5-Training-error-Testing-error-and-Generalization-error" class="headerlink" title="1.2.5 Training error Testing error and Generalization error"></a>1.2.5 Training error Testing error and Generalization error</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;以监督学习中的分类问题为例，我们通常把我们模型预测输出和其真实结果的差别称为Error<br><strong>Training Error:</strong> 利用训练集和验证集来进行模型选择时产生的误差，这作为我们调参模型选择的依据.<br><strong>Testing Error:</strong> 根据测试集的数据，利用我们训练好的模型来输出结果，输出的结果和真实值之间的误差。<br><strong>Generalization Error:</strong> 将我们的模型应用与一切所有相关样本而产生的误差.<br>这里大家容易混淆的就是Testing Error和Generalization Error.因为我们一般不可能知道我们的Generalization Error,所以基本是用Testing Error来进行替代。</p>
<h4 id="1-2-6-Underfit-and-Overfit"><a href="#1-2-6-Underfit-and-Overfit" class="headerlink" title="1.2.6 Underfit and Overfit"></a>1.2.6 Underfit and Overfit</h4><p><strong>Overfit:</strong> 表现为Training Error很小，Testing Error很大。原因是学习器将我们的特征学得太好了，把训练集上的一些特别的东西也当作一般的特征了。<br><strong>Underfit:</strong> 与Overfit相对。原因是我们的训练数据没有完全学好，没有学到它的特性，一般情况下有时候表现为为了防止Overfit而使得Traning Error很小导致。</p>
<h4 id="1-2-7-Performance-measure"><a href="#1-2-7-Performance-measure" class="headerlink" title="1.2.7 Performance measure"></a>1.2.7 Performance measure</h4><h5 id="1-2-7-1回归类的性能度量"><a href="#1-2-7-1回归类的性能度量" class="headerlink" title="1.2.7.1回归类的性能度量"></a>1.2.7.1回归类的性能度量</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;回归类的性能度量方法最常用的就是mean square error。</p>
<script type="math/tex; mode=display">E(f;D)=\frac{1}{m}\sum_{i=1}^m{(f(x_i)-y_i)^2}</script><p>这个非常好理解，不做具体的阐述.</p>
<h5 id="1-2-7-2分类问题的性能度量"><a href="#1-2-7-2分类问题的性能度量" class="headerlink" title="1.2.7.2分类问题的性能度量"></a>1.2.7.2分类问题的性能度量</h5><h6 id="error-and-accuracy"><a href="#error-and-accuracy" class="headerlink" title="error and accuracy"></a>error and accuracy</h6><p><strong>error:</strong> 分类错误的样本数占总样本数的比例。<br><strong>accuracy:</strong> 分类正确的样本数占总样本数的比例。<br>这两个是我们最常用的性能指标，也是很多分类的任务或者比赛中大家经常能够看见的</p>
<h6 id="precision-recall-and-F1"><a href="#precision-recall-and-F1" class="headerlink" title="precision recall and F1"></a>precision recall and F1</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;在我刚开始看书的时候，我最容易混淆的就是precision和accuracy.谁叫咱当初看单词只记住一个意思呢。<br><strong>precision:</strong> 预测的正例中真实的正例所占的比例。<br><strong>recall:</strong> 真实的正例中预测的正例所占的比例。<br>我们接下来以二分类来说，假设我们预测的真值，正例称为真，反例称为反。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">真实情况</th>
<th style="text-align:center">预测结果：是</th>
<th style="text-align:center">预测结果：非</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">真</td>
<td style="text-align:center">是真</td>
<td style="text-align:center">非真</td>
</tr>
<tr>
<td style="text-align:center">假</td>
<td style="text-align:center">是假</td>
<td style="text-align:center">非假</td>
</tr>
</tbody>
</table>
</div>
<p>precision:<script type="math/tex">\frac{是真}{是真+是假}</script><br>recall:<script type="math/tex">\frac{是真}{是真+非真}</script><br>&nbsp;&nbsp;&nbsp;&nbsp;简单的理解就是，查准率：我们预测出来的是真的，但实际真的占我们的比例，这完全证明我们的预测能力。召回率：我姑且翻译为“找回率”，也就是说在我们所有真的东西中，我们的模型找回了多个真的。这样相信大家会有点了解了。或许大家和我之前一样有疑惑，为什么会有这两个率呢。这主要和我们分析的问题有关系了。比如我们的项目对象是罪犯侦查识别系统的构建，那么在这么一个项目中，我们应该最关注的是尽量将罪犯全部抓住，那么就希望recall高一些。再比如我们的癌症检测系统，我们希望能够检测出癌症的人(有癌的人没检测出来和没有癌误检测的这个代价是完全不一样的，这大家可以理解)<br>&nbsp;&nbsp;&nbsp;&nbsp;precision的话，比如我们做食物的推荐系统，我们尽可能的不想打扰用户，因此我们应该尽量提高precision。当然一个系统如果查准率和查全率都高就好了。但是查准率和查全率是一个矛盾的参数。就拿癌症系统来说，我们想要提高recall，那么我们可以将所有人都抽样出来，判别其为癌症患者，那么是不是recall为100%，但是precison就会很低。因此根据任务来说，我们尽可能的想recall和precision都高。若我们训练的某一学习器recall和precision都大于另一学习器，那么这个学习器性能肯定优.因此这边推出了一个参数$F_\beta$来验证.<script type="math/tex">\frac{1}{F_\beta} = \frac{1}{1+\beta^2}*(\frac{1}{P}+\frac{\beta^2}{R})</script>.当$\beta&gt;1$时查全率有更大的影响，当$\beta&lt;1$时，查准率有更大的影响。当$\beta=1$时，就是标准的$F_1$了。选取$F_\beta$最大的算法。</p>
<h4 id="1-2-8-bias-and-variance"><a href="#1-2-8-bias-and-variance" class="headerlink" title="1.2.8 bias and variance"></a>1.2.8 bias and variance</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;对于学习算法来说，我们不仅通过测试误差来估计其泛化误差，同时要知道其泛化误差的来源。因此我们将我们的泛化误差来源。我们此处采用“方差-偏差”分解。对于我们的测试样本X，$y_D$为X的标记，y为真实标记，f(X;D)为训练集D上学得的模型f在D上的输出。学习算法的期望预测为<script type="math/tex">f^-(x) = E_D[f(x;D)]</script>(这是我们算法的期望输出，也就是期望风险)，其variance为<script type="math/tex">var(x)=E_D[(f(x;D)-f^-(x))^2]</script>(由于不同的训练数据会导致训练输出的结果是不同的，因此存在这方差)。噪声为<script type="math/tex">\epsilon^2=E_D[(y_D-y)^2]</script>(因为我们的测试集的标记与这个事物原本的真实值有可能还存在其他的误差)其bias为<script type="math/tex">bias^2(x) = (f^-(x)-y)^2</script>.<br>&nbsp;&nbsp;&nbsp;&nbsp;假设我们的噪声为0,标记值就是真实值了(因为这个噪声一般无法测量，实际处理的时候也是假设其为0的),接下来对我们的泛化误差进行下分解:<br><img src="../../images/MachineLearning/Machine_Architecture/BiasAndVariance.jpg" alt="Alt text">&nbsp;&nbsp;&nbsp;&nbsp;根据上述推算，我们得出我们的泛化误差分解为偏差、方差和噪声这三个之和。偏差是算法的本身能力，方差是数据集的变动引起的变化(数据扰动的影响),而噪声是代表了我们利用任何算法能得到的泛化误差的下界（最好的算法也只能到这个程度了）。做泛化误差的分解不是为了简单的推导一下而已，而是通过泛化误差的推导，在我们做算法训练的时候进行分析。判断是欠拟合还是过拟合，到底应该增加训练的复杂度呢还是少训练几下等等。从泛化误差的推到来看，噪声作为一个定值我们不做处理。所以偏差+方差是一个定值。因此当偏差大的时候方差小，偏差小的时候方差大。因此在训练的时候，训练不够，偏差大，方差比较小。训练过多，偏差小，方差就比较大，就过拟合了。所以总能找到一个方差小偏差也小的平衡点最好。</p>
<h2 id="2-机器学习工程架构"><a href="#2-机器学习工程架构" class="headerlink" title="2.机器学习工程架构"></a>2.机器学习工程架构</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;对于机器学习的整体架构，我觉得用一张图来介绍最好了：<img src="../../images/MachineLearning/Machine_Architecture/machine_architecture.jpg" alt="hello"><br>EDA（Exploratory Data Analysis）:探索型数据分析，利用社会、人文以及数学等关系对特征进行探索型知识数据分析等等<br><strong>clean</strong>:数据清洗</p>
<p><strong>Feature Engineering</strong>:特征工程</p>
<p><strong>Model Selection/Parameter Tuning</strong>:进行算法模型选择以及参数调节</p>
<p><strong>Ensemble</strong>:一些聚类的方法</p>
<p><strong>cv</strong>:交叉验证</p>
<p><strong>lb</strong>:leaderboard</p>
]]></content>
      <categories>
        <category>MachineLearning</category>
      </categories>
      <tags>
        <tag>机器学习的整体架构与介绍</tag>
      </tags>
  </entry>
</search>
